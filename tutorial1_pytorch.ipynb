{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdeaf960-8b67-4ab3-a02d-515acc453eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65a067-ec91-47ea-bad7-b6f05627ad49",
   "metadata": {},
   "source": [
    "# Naive Implementation of Feed Forward Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7949f8ed-7d4a-4e6b-b533-e6dad1d5b99d",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c05c11a-d4d6-4188-8acf-3fa55d7ab3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension\n",
    "D = 3\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6261a0c-b50a-45c3-9a18-f862ac0787ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the parameters of the neural network\n",
    "## the first layer\n",
    "W1 = torch.randn((K, D), requires_grad=True)\n",
    "b1 = torch.randn((K, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eba0ee0-4ba3-4e96-ad85-258042428890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8932,  0.2623, -2.7496],\n",
      "        [ 1.1011, -0.4415, -2.2481],\n",
      "        [ 0.9644, -0.7362,  0.7305],\n",
      "        [ 0.3782, -1.0921,  0.3289],\n",
      "        [-0.0778, -1.0564, -2.0712]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16842730-d4c1-4759-a517-4ca1c9066c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1006],\n",
      "        [-0.3950],\n",
      "        [ 0.3838],\n",
      "        [-0.6522],\n",
      "        [ 2.0503]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f11de54-1739-48c6-a510-0f74fc3a2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the second layer\n",
    "W2 = torch.randn((K, K), requires_grad=True)\n",
    "b2 = torch.randn((K, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c62cd3a-dca9-4597-9c11-d96b7e735fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the third layer\n",
    "W3 = torch.randn((K, K), requires_grad=True)\n",
    "b3 = torch.randn((K, 1), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9ca883a-7ee7-4c4b-af21-b7769fd8da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the last layer\n",
    "W4 = torch.randn((1, K), requires_grad=True)\n",
    "b4 = torch.randn((1,), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447452c3-a1bb-4714-acfe-798d02c6deee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5244613-2303-4c63-a97d-7c9bc93f1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe62e43-3a31-4c1e-83b5-d4b83880f2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81e5c490-de01-4a5e-9d04-d6a2b43f1199",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0b00d38-6a85-477e-b473-7877f8ec3c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6686],\n",
      "        [ 1.3910],\n",
      "        [-2.4452]], requires_grad=True)\n",
      "tensor([-1.0027])\n"
     ]
    }
   ],
   "source": [
    "# input\n",
    "x = torch.randn((D, 1), requires_grad=True)\n",
    "\n",
    "# ground truth\n",
    "y_ground = torch.randn((1,))\n",
    "\n",
    "print(x)\n",
    "print(y_ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "524082e6-15ce-40f2-82bb-4e0d69258c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9955],\n",
      "        [0.9946],\n",
      "        [0.1441],\n",
      "        [0.0616],\n",
      "        [0.9963]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# forward\n",
    "## the first layer\n",
    "z1 = torch.matmul(W1, x) + b1\n",
    "a1 = sigmoid(z1)\n",
    "\n",
    "print(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf94a652-8e6f-4eee-befc-df50a1f31204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8472],\n",
      "        [0.0949],\n",
      "        [0.7196],\n",
      "        [0.7907],\n",
      "        [0.0670]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## the second layer\n",
    "z2 = torch.matmul(W2, a1) + b2\n",
    "a2 = sigmoid(z2)\n",
    "\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "994a463d-1059-4fd3-939d-3e563c381252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3788],\n",
      "        [0.9726],\n",
      "        [0.5198],\n",
      "        [0.7039],\n",
      "        [0.4933]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## the third layer\n",
    "z3 = torch.matmul(W3, a2) + b3\n",
    "a3 = sigmoid(z3)\n",
    "\n",
    "print(a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "881c96f0-631d-4003-8ce4-97aa391eccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5010]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## the last layer\n",
    "z4 = torch.matmul(W4, a3) + b4\n",
    "y_pred = sigmoid(z4)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee4cca-3d6c-47e0-a266-32f0b2b5b347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeb70e8c-cba1-4d5b-9d7c-ec2432b95ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2612]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate the loss\n",
    "J = (y_pred - y_ground) ** 2\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5874aff7-39e1-4dd9-8383-4b63e04fb070",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fb06098-20ca-4328-9359-dede09c96ef3",
   "metadata": {},
   "source": [
    "## Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b52ecc7b-0aa3-4ff0-81a8-cbf5d9498e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gradient of W2:\n",
      "None\n",
      "the gradient of b2:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('the gradient of W2:')\n",
    "print(W2.grad)\n",
    "print('the gradient of b2:')\n",
    "print(b2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e577d2c-402f-402c-937c-eaed1d59eefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gradients\n",
    "J.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "918dc852-570b-471e-91a1-ae9b50676ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gradient of W2:\n",
      "tensor([[ 3.5393e-02,  3.5364e-02,  5.1236e-03,  2.1918e-03,  3.5423e-02],\n",
      "        [ 1.4044e-02,  1.4032e-02,  2.0330e-03,  8.6968e-04,  1.4055e-02],\n",
      "        [-8.3885e-02, -8.3816e-02, -1.2143e-02, -5.1948e-03, -8.3955e-02],\n",
      "        [-3.2176e-02, -3.2150e-02, -4.6579e-03, -1.9926e-03, -3.2203e-02],\n",
      "        [-1.5706e-03, -1.5693e-03, -2.2736e-04, -9.7264e-05, -1.5719e-03]])\n",
      "the gradient of b2:\n",
      "tensor([[ 0.0356],\n",
      "        [ 0.0141],\n",
      "        [-0.0843],\n",
      "        [-0.0323],\n",
      "        [-0.0016]])\n"
     ]
    }
   ],
   "source": [
    "print('the gradient of W2:')\n",
    "print(W2.grad)\n",
    "print('the gradient of b2:')\n",
    "print(b2.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217ed772-b25b-4c21-8824-34b8426b9a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdc02648-414a-4665-b4df-626be5959ff3",
   "metadata": {},
   "source": [
    "## Update the Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aed013-eff9-49d5-b0c1-7637b70aea75",
   "metadata": {},
   "source": [
    "W <- W - lr * W_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b48e797-bb4b-4af4-a232-7f77cab29f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2549cc09-2296-498d-aefc-4be8ad476bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the weight of W2 before updation:\n",
      "tensor([[ 3.1237e-01, -1.8969e+00, -6.3974e-01,  7.8099e-01,  2.4940e+00],\n",
      "        [-1.2192e+00, -2.2249e-03,  1.3047e+00, -1.6586e+00, -3.0698e-01],\n",
      "        [ 1.3101e-01,  1.2648e-01,  8.8051e-01,  6.0813e-01,  1.5349e+00],\n",
      "        [ 7.2325e-01, -6.9043e-01,  8.3009e-01,  1.9712e+00,  1.5277e+00],\n",
      "        [-1.7916e+00, -8.0821e-01, -2.3108e-01,  1.0899e+00,  1.1378e-01]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('the weight of W2 before updation:')\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0949e3a1-1794-42ee-b557-9781cdb8ad06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3088, -1.9004, -0.6403,  0.7808,  2.4905],\n",
      "        [-1.2206, -0.0036,  1.3045, -1.6587, -0.3084],\n",
      "        [ 0.1394,  0.1349,  0.8817,  0.6087,  1.5433],\n",
      "        [ 0.7265, -0.6872,  0.8306,  1.9714,  1.5309],\n",
      "        [-1.7914, -0.8080, -0.2311,  1.0899,  0.1139]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "expected_new_W2 = W2 - lr * W2.grad\n",
    "print(expected_new_W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae37f78-c872-44f0-8e60-8be7f3c2e7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d60458af-7aa7-487d-a9c5-e72866d2527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify an optimizer\n",
    "optimizer = torch.optim.SGD([W1, b1, W2, b2, W3, b3, W4, b4], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "265f8898-5771-493f-ad5a-9ac70d46cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08946635-4774-449d-a39a-e037ae5c6271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the weight of W2 after updation:\n",
      "tensor([[ 0.3088, -1.9004, -0.6403,  0.7808,  2.4905],\n",
      "        [-1.2206, -0.0036,  1.3045, -1.6587, -0.3084],\n",
      "        [ 0.1394,  0.1349,  0.8817,  0.6087,  1.5433],\n",
      "        [ 0.7265, -0.6872,  0.8306,  1.9714,  1.5309],\n",
      "        [-1.7914, -0.8080, -0.2311,  1.0899,  0.1139]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('the weight of W2 after updation:')\n",
    "print(W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dd03875-3928-4b4a-8189-fe59bc11cc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if two tensors, W2 and expected_new_W2, are element-wise equal\n",
    "torch.eq(W2, expected_new_W2).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0b3e50-45d2-4c3e-b583-5a0e6bc0be8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43bcadc9-fdcd-4403-a35a-759212f80f9b",
   "metadata": {},
   "source": [
    "# Pytorch Implementation of FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe56076-49bb-4ddb-b6d2-14640e544e25",
   "metadata": {},
   "source": [
    "## FFN Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1603daa-64e4-40cc-b12c-66c418c33496",
   "metadata": {},
   "source": [
    "```\n",
    "W1 = torch.randn((K, D), requires_grad=True)\n",
    "\n",
    "b1 = torch.randn((K, 1), requires_grad=True)\n",
    "\n",
    "def sigmoid(x):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2b1d781-95f9-4749-9c0c-506c65c5092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define\n",
    "layer1 = torch.nn.Linear(in_features=D, out_features=K, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00c26f42-5f46-4540-b57d-b145b2073a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "x = torch.randn((D,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002c3c2-587d-490c-ac82-e691bf001608",
   "metadata": {},
   "source": [
    "```\n",
    "z1 = torch.matmul(W1, x) + b1\n",
    "a1 = sigmoid(z1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6d8ab9f2-a348-4741-a891-b784b9707a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "z1 = layer1(x)\n",
    "a1 = torch.nn.functional.sigmoid(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a581b87-d623-4d16-aaf0-abd1282b2740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "437c3cf5-2fd7-4ad6-bc04-71adb112cc2a",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef2ee2c6-9679-4a9b-a6bb-ecb1459aa1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_network(torch.nn.Module):\n",
    "    def __init__(self, D, K):\n",
    "        super(neural_network, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=D, out_features=K, bias=True)\n",
    "        self.layer2 = torch.nn.Linear(in_features=K, out_features=K, bias=True)\n",
    "        self.layer3 = torch.nn.Linear(in_features=K, out_features=K, bias=True)\n",
    "        self.layer4 = torch.nn.Linear(in_features=K, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the first layer\n",
    "        z1 = self.layer1(x)\n",
    "        a1 = torch.nn.functional.sigmoid(z1)\n",
    "        \n",
    "        # the second layer\n",
    "        z2 = self.layer2(a1)\n",
    "        a2 = torch.nn.functional.sigmoid(z2)\n",
    "        \n",
    "        # the third layer\n",
    "        z3 = self.layer3(a2)\n",
    "        a3 = torch.nn.functional.sigmoid(z3)\n",
    "        \n",
    "        # the fourth layer\n",
    "        z4 = self.layer4(a3)\n",
    "        a4 = torch.nn.functional.sigmoid(z4)\n",
    "        return a4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cd3931a-0fe3-4cd4-aaaf-144c933a6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the network and the optimizer\n",
    "network = neural_network(D, K)\n",
    "\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d334b48-dc28-48e1-9be7-2eebe797eb54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a44f791a-1a89-48f2-9aa0-45a8665e8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "x = torch.randn((D,))\n",
    "\n",
    "# ground truth\n",
    "y_ground = torch.randn((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fce8b18-db15-42b7-97d4-ed6b29780128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "y_pred = network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "caed4964-ebde-4cf6-b745-c11c2db71b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1314], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# calculate the loss\n",
    "J = (y_pred - y_ground) ** 2\n",
    "print(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aaffca48-9ec0-4b74-a883-ab3e08472959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward\n",
    "J.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91c29397-4a4b-413e-99a8-4065a84054f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the parameters\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7c990-f0c4-4175-a7e1-1b5e43709b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead5734-148f-4b73-86a7-4cbd1a25c89f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14dc0c4-964c-43f2-951a-174a45db2e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f46b0a23-c140-4c47-aee7-5f737292c6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 1.9532e-04,  2.5752e-05,  5.7299e-05],\n",
       "         [ 1.9530e-04,  2.5749e-05,  5.7293e-05],\n",
       "         [-4.7596e-05, -6.2753e-06, -1.3963e-05],\n",
       "         [ 1.9874e-05,  2.6204e-06,  5.8305e-06],\n",
       "         [ 1.7986e-05,  2.3714e-06,  5.2764e-06]]),\n",
       " tensor([ 1.2927e-04,  1.2925e-04, -3.1500e-05,  1.3154e-05,  1.1904e-05]),\n",
       " tensor([[-7.2583e-05, -8.7198e-05, -4.9712e-05, -7.4146e-05, -7.9270e-05],\n",
       "         [ 6.4507e-04,  7.7495e-04,  4.4181e-04,  6.5896e-04,  7.0450e-04],\n",
       "         [ 4.4140e-04,  5.3028e-04,  3.0232e-04,  4.5091e-04,  4.8207e-04],\n",
       "         [-5.1966e-04, -6.2429e-04, -3.5591e-04, -5.3085e-04, -5.6753e-04],\n",
       "         [ 2.7265e-04,  3.2755e-04,  1.8674e-04,  2.7852e-04,  2.9777e-04]]),\n",
       " tensor([-0.0002,  0.0014,  0.0010, -0.0011,  0.0006]),\n",
       " tensor([[-0.0052, -0.0090, -0.0071, -0.0079, -0.0077],\n",
       "         [ 0.0042,  0.0073,  0.0057,  0.0064,  0.0062],\n",
       "         [ 0.0002,  0.0003,  0.0002,  0.0003,  0.0003],\n",
       "         [-0.0025, -0.0044, -0.0035, -0.0039, -0.0038],\n",
       "         [-0.0024, -0.0041, -0.0032, -0.0036, -0.0035]]),\n",
       " tensor([-0.0146,  0.0118,  0.0005, -0.0071, -0.0066]),\n",
       " tensor([[0.0905, 0.0708, 0.0994, 0.0642, 0.0639]]),\n",
       " tensor([0.1807])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in network.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "23cb7574-51d9-4001-bcea-1b95373ec300",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2f215425-d060-4250-89b4-e025f06395e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in network.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f36a8-b4d5-447f-9a3c-6c3b3ba3146c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3c323ad-62a5-4e5f-ab5d-5b94e70f97a6",
   "metadata": {},
   "source": [
    "## Loading data from numpy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83ff034b-7fdf-4e01-9089-bd39df32806b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a numpy array.\n",
    "import numpy as np\n",
    "x = np.array([[1, 2], [3, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b53f1406-136b-499d-a980-d23223119ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the numpy array to a torch tensor.\n",
    "y = torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "66e58f7a-855a-4b33-af4c-826228aaf502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2df9428e-70ec-4fb3-8920-29945145674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the torch tensor to a numpy array.\n",
    "z = y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9be6d49d-d2e8-4794-bfea-39ef456eae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3066f51-041a-4758-a4be-4c2a58f5ed1b",
   "metadata": {},
   "source": [
    "## Input pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f88d7ebb-e957-4675-ab03-f891fc9cc549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Download and construct CIFAR-10 dataset.\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms.ToTensor(),\n",
    "                                             download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8b3e8ca-6903-41e9-94b9-b248a458aefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Fetch one data pair (read data from disk).\n",
    "image, label = train_dataset[0]\n",
    "print (image.size())\n",
    "print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "163ea466-1fa9-4477-91f9-287b2268b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader (this provides queues and threads in a very simple way).\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "939c0b6d-f415-41af-92ec-e83514f573d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fccb09fc-b018-4fb2-9114-8e96c9072710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a mini-batch of images and labels\n",
    "images, labels = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac3d86-c4c6-44ad-977b-7b70157f8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    # Training code should be written here.\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Reshape images to match the input size expected by the network\n",
    "    images = images.view(images.size(0), -1)[:, :D]\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = network(images)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = torch.nn.functional.mse_loss(outputs, labels.float().view(-1, 1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ee9c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[Epoch 1, Batch 100] loss: 2.013\n",
      "[Epoch 1, Batch 200] loss: 1.746\n",
      "[Epoch 1, Batch 300] loss: 1.613\n",
      "[Epoch 1, Batch 400] loss: 1.539\n",
      "[Epoch 1, Batch 500] loss: 1.477\n",
      "[Epoch 1, Batch 600] loss: 1.417\n",
      "[Epoch 1, Batch 700] loss: 1.347\n",
      "[Epoch 2, Batch 100] loss: 1.299\n",
      "[Epoch 2, Batch 200] loss: 1.265\n",
      "[Epoch 2, Batch 300] loss: 1.218\n",
      "[Epoch 2, Batch 400] loss: 1.167\n",
      "[Epoch 2, Batch 500] loss: 1.197\n",
      "[Epoch 2, Batch 600] loss: 1.166\n",
      "[Epoch 2, Batch 700] loss: 1.112\n",
      "[Epoch 3, Batch 100] loss: 1.075\n",
      "[Epoch 3, Batch 200] loss: 1.062\n",
      "[Epoch 3, Batch 300] loss: 1.045\n",
      "[Epoch 3, Batch 400] loss: 1.057\n",
      "[Epoch 3, Batch 500] loss: 1.012\n",
      "[Epoch 3, Batch 600] loss: 1.029\n",
      "[Epoch 3, Batch 700] loss: 0.999\n",
      "[Epoch 4, Batch 100] loss: 0.977\n",
      "[Epoch 4, Batch 200] loss: 0.959\n",
      "[Epoch 4, Batch 300] loss: 0.920\n",
      "[Epoch 4, Batch 400] loss: 0.938\n",
      "[Epoch 4, Batch 500] loss: 0.925\n",
      "[Epoch 4, Batch 600] loss: 0.891\n",
      "[Epoch 4, Batch 700] loss: 0.919\n",
      "[Epoch 5, Batch 100] loss: 0.882\n",
      "[Epoch 5, Batch 200] loss: 0.875\n",
      "[Epoch 5, Batch 300] loss: 0.872\n",
      "[Epoch 5, Batch 400] loss: 0.875\n",
      "[Epoch 5, Batch 500] loss: 0.872\n",
      "[Epoch 5, Batch 600] loss: 0.852\n",
      "[Epoch 5, Batch 700] loss: 0.870\n",
      "[Epoch 6, Batch 100] loss: 0.834\n",
      "[Epoch 6, Batch 200] loss: 0.838\n",
      "[Epoch 6, Batch 300] loss: 0.818\n",
      "[Epoch 6, Batch 400] loss: 0.831\n",
      "[Epoch 6, Batch 500] loss: 0.834\n",
      "[Epoch 6, Batch 600] loss: 0.823\n",
      "[Epoch 6, Batch 700] loss: 0.799\n",
      "[Epoch 7, Batch 100] loss: 0.791\n",
      "[Epoch 7, Batch 200] loss: 0.792\n",
      "[Epoch 7, Batch 300] loss: 0.784\n",
      "[Epoch 7, Batch 400] loss: 0.804\n",
      "[Epoch 7, Batch 500] loss: 0.814\n",
      "[Epoch 7, Batch 600] loss: 0.792\n",
      "[Epoch 7, Batch 700] loss: 0.777\n",
      "[Epoch 8, Batch 100] loss: 0.775\n",
      "[Epoch 8, Batch 200] loss: 0.767\n",
      "[Epoch 8, Batch 300] loss: 0.777\n",
      "[Epoch 8, Batch 400] loss: 0.765\n",
      "[Epoch 8, Batch 500] loss: 0.741\n",
      "[Epoch 8, Batch 600] loss: 0.739\n",
      "[Epoch 8, Batch 700] loss: 0.760\n",
      "[Epoch 9, Batch 100] loss: 0.721\n",
      "[Epoch 9, Batch 200] loss: 0.753\n",
      "[Epoch 9, Batch 300] loss: 0.743\n",
      "[Epoch 9, Batch 400] loss: 0.734\n",
      "[Epoch 9, Batch 500] loss: 0.734\n",
      "[Epoch 9, Batch 600] loss: 0.745\n",
      "[Epoch 9, Batch 700] loss: 0.746\n",
      "[Epoch 10, Batch 100] loss: 0.703\n",
      "[Epoch 10, Batch 200] loss: 0.734\n",
      "[Epoch 10, Batch 300] loss: 0.737\n",
      "[Epoch 10, Batch 400] loss: 0.744\n",
      "[Epoch 10, Batch 500] loss: 0.705\n",
      "[Epoch 10, Batch 600] loss: 0.719\n",
      "[Epoch 10, Batch 700] loss: 0.695\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 74.31%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the transformations for the training and test sets\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomCrop(32, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "\n",
    "# Load the CIFAR-10 dataset \n",
    "# The first time you run this, it will download the dataset (../../data/ is the directory to save the dataset)\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../data/', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../../data/', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize the network, loss function, and optimizer\n",
    "network = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # number of epochs\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = network(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:  # print every 100 mini-batches\n",
    "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(network.state_dict(), 'cnn_cifar10.pth')\n",
    "\n",
    "# Evaluate the network on the test data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        outputs = network(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "79c7a454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'conv3.weight', 'conv3.bias', 'fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias'])\n",
      "CNN(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_16700\\1314130222.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load('cnn_cifar10.pth')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model state dictionary\n",
    "state_dict = torch.load('cnn_cifar10.pth')\n",
    "print(state_dict.keys())\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = CNN()\n",
    "print(model)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0794eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the CIFAR-10 training dataset: 10.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        # Reshape images to match the input size expected by the network\n",
    "        images = images.view(images.size(0), -1)[:, :D]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = network(images)\n",
    "\n",
    "        # Convert outputs to predicted labels\n",
    "        predicted = outputs.round().view(-1).long()\n",
    "\n",
    "        # Update the total and correct counts\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the CIFAR-10 training dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "14c14079-6b50-4179-a364-2ed33dc8c1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# You can build your custom dataset as below.\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        # Download and construct CIFAR-10 dataset.\n",
    "        self.dataset = torchvision.datasets.CIFAR10(root='../../data/',\n",
    "                                                    train=True, \n",
    "                                                    transform=transforms.ToTensor(),\n",
    "                                                    download=True)\n",
    "    def __getitem__(self, index):\n",
    "        # Fetch one data pair (image and label).\n",
    "        image, label = self.dataset[index]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total size of the dataset.\n",
    "        return len(self.dataset)\n",
    "\n",
    "# You can then use the prebuilt data loader. \n",
    "custom_dataset = CustomDataset()\n",
    "print(len(custom_dataset))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=64, \n",
    "                                           shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
