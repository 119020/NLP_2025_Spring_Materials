{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/119020/NLP_2025_Spring_Materials/blob/main/25Spring_NLP_Assignment_2_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Enginnering\n",
        "### **Course Name:** Natural Language Processing **<font color=\"red\">(CSC6052/MDS5110/CSC5051/CSC4100)</font>**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9yFFKuDZB_ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hello, everyone.\n",
        "In this tutorial, we'll explore *Prompt Engineering* techniques.\n",
        "\n",
        "**<font color=\"blue\">What is prompt engineering in AI?</font>**\n",
        "\n",
        "An AI prompt is a carefully crafted instruction given to an AI model to generate a specific output. These inputs can range from text and images to videos or even music.\n",
        "\n",
        "Prompt engineering means writing precise instructions that guide AI models like **ChatGPT** to produce specific and useful responses. It involves designing inputs that an AI can easily understand and act upon, ensuring the output is relevant and accurate."
      ],
      "metadata": {
        "id": "Y_qFy6J9EgJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **The First Step: Mastering the Use of APIs**\n",
        "\n"
      ],
      "metadata": {
        "id": "iEiLphcKt6Ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### About API Keys\n",
        "#### DeepSeek API Keys\n",
        "‚ùó Note that you can get your deepseek api key from it official website. Upon registeration, you will get 14 RMB free credit, and it will account for about 20M token usage, which will be enough to cover your assignment. Alternatively, you can go to [Siliconflow](https://cloud.siliconflow.cn/) for more free credit:\n",
        "- Go to the website [https://platform.deepseek.com/api_keys](https://platform.deepseek.com/api_keys).\n",
        "- Sign up for a new account, and you will have 14 RMB free credit.\n",
        "- Change the `DEEPSEEK_API_KEY` environment variable with the key you purchased.\n",
        "- Remember to update `DEEPSEEK_BASE_URL` to https://api.siliconflow.cn/v1/chat/completions when using API from SiliconFlow.\n",
        "\n",
        "#### OpenAI API Keys\n",
        "Note that we provide a key with 100 US dollars, if it is used up you need to buy the Keys yourself (it may cost you a little bit of money), here is how to buy the keys:\n",
        "- Go to the website [https://eylink.cn/buy/7](https://eylink.cn/buy/7).\n",
        "- Purchase a 14 RMB key (10 US dollars). (10 dollars are enough.)\n",
        "- Fill in the `OPENAI_API_KEY` below with the key you purchased.\n",
        "\n",
        "(As a student, you can apply for a $100 free API credit at https://azure.microsoft.com/en-us/free/students. Remember to update `OPENAI_BASE_URL` to https://api.openai.com/v1/chat/completions when using API from Azure.)"
      ],
      "metadata": {
        "id": "lR37y8eVuG8V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîÖ To facilitate easier access to OpenAI's model APIs, we make use of a popular framework langchain."
      ],
      "metadata": {
        "id": "OOubmZd8vGl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-deepseek\n",
        "!pip install retrying"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWx2mmC-fFuP",
        "outputId": "3eb602c9-5117-4da8-a432-a3ed9fb32f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.39 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.40)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (0.3.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.39->langchain-openai) (2.10.6)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.39->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain-openai) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.39->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.39->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.39->langchain-openai) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Requirement already satisfied: langchain-deepseek in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain-deepseek) (0.3.40)\n",
            "Requirement already satisfied: langchain-openai<1.0.0,>=0.3.5 in /usr/local/lib/python3.11/dist-packages (from langchain-deepseek) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (0.3.11)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (2.10.6)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (1.61.1)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (3.10.15)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (2.27.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai<1.0.0,>=0.3.5->langchain-deepseek) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.34->langchain-deepseek) (2.3.0)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.11/dist-packages (1.3.4)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from retrying) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_deepseek import ChatDeepSeek\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import random\n",
        "import json\n",
        "from retrying import retry\n",
        "import requests\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-8bWHFZhLVSPyeXoO6f0327Ee96A34a1dB158Ad85174eE5A0\"\n",
        "# os.environ[\"OPENAI_BASE_URL\"] = \"https://apix.ai-gaochao.cn/v1\"\n",
        "# gpt_4o_mini = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n",
        "\n",
        "\n",
        "os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-\"\n",
        "os.environ[\"DEEPSEEK_BASE_URL\"] = \"https://api.deepseek.com/v1\"\n",
        "deepseek_chat = ChatDeepSeek(model=\"deepseek-chat\", temperature=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "EXIC-LVvVXit",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üòä You can now engage directly with DeepSeek-Chat using our `invoke` function."
      ],
      "metadata": {
        "id": "4ElnWZGKu9Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an AI assistant, please answer user's question.\"),\n",
        "        (\"user\", \"{input}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = ChatDeepSeek(model=\"deepseek-chat\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "response = chain.invoke({\"input\": \"Hello\"})\n",
        "print(response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzUZFHrweW2D",
        "outputId": "346bcdf2-e4c8-40c1-f746-f7c456dfc08d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hello! How can I assist you today? üòä' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 16, 'total_tokens': 27, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 16}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3a5770e1b4_prod0225', 'finish_reason': 'stop', 'logprobs': None} id='run-97ffbe55-9b6d-4d80-a501-c01009423b3c-0' usage_metadata={'input_tokens': 16, 'output_tokens': 11, 'total_tokens': 27, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the output easy to use, we can apply a output parser to the original output!"
      ],
      "metadata": {
        "id": "6Ly8ERAJgI8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "response = chain.invoke({\"input\": \"Hello\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPnmOckZgICw",
        "outputId": "4d752498-82c7-4d49-ca1c-debdda626176"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today? üòä\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **LLM Setting**\n",
        "\n",
        "**1. Model Selection**\n",
        "\n",
        "You are free to choose from several models. Below are the models DeepSeek offers:\n",
        "- deepseek-chat (currently pointing to deepseek-v3, offering a balance of capability and cost, highly recommended!)\n",
        "- deepseek-reasoner (currently pointing to deepseek-r1, extremely sophisticated and intelligent)\n",
        "For detailed information, please visit [DeepSeek's Website](https://api-docs.deepseek.com/zh-cn/news/news250120).\n",
        "\n",
        "Note that DeepSeek offers a discount when using API in the midnight, details showing [here](https://api-docs.deepseek.com/zh-cn/quick_start/pricing)\n",
        "\n",
        "Below are the models OpenAI offers:\n",
        "\n",
        "- gpt-4o-mini (the most cost-effective model, highly recommended!)\n",
        "- gpt-4o (offers a balance of capability and cost)\n",
        "- gpt-4-turbo (extremely sophisticated and intelligent)\n",
        "\n",
        "For detailed information, please visit [OpenAI's Website](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)."
      ],
      "metadata": {
        "id": "ADk-FqFZvaNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deepseek_reasoner = ChatDeepSeek(model=\"deepseek-reasoner\", temperature=1)\n",
        "chain = prompt | deepseek_reasoner | StrOutputParser()\n",
        "chain.invoke({'input': \"hello! what\\' your name?\"}) # Ê≥®ÊÑèreasonerÂæàË¥µÂì¶"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "tNyoi8HSvZop",
        "outputId": "ca654479-2c83-44da-9c3c-60132d071817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello! I'm an AI assistant here to help you. You can call me whatever you like‚ÄîI don't have a personal name, but feel free to give me one if you'd like! How can I assist you today? üòä\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Temperature**\n",
        "Controls the randomness of the model's output. Lower values make responses more deterministic and focused, while higher values allow for more creative and diverse outputs. Use low values for factual tasks and high values for creative tasks."
      ],
      "metadata": {
        "id": "x-Glb5kzzu1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatDeepSeek(model=\"deepseek-chat\", temperature=0)\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke( {\"input\": 'hello! what\\' your name?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VmnzLvyiw-aJ",
        "outputId": "46950dbc-26c6-40a7-bc2e-4b31d7e3d804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! I‚Äôm ChatGPT, an AI created by OpenAI. You can call me whatever you like‚ÄîI‚Äôm here to help! How can I assist you today? üòä'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Top P**\n",
        "This is nucleus sampling, where only tokens from the top probability mass (up to `top_p`) are considered. Lower values encourage more focused responses, while higher values increase the diversity of possible outputs."
      ],
      "metadata": {
        "id": "Cp7Tlio5w_DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatDeepSeek(model=\"deepseek-chat\", top_p=0.9)\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke( {\"input\": 'hello! what\\' your name?'})"
      ],
      "metadata": {
        "id": "HShZxbyOu7b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e4307ea2-bb22-4dac-ba58-00b408731898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! I‚Äôm an AI assistant created to help with your questions and tasks. You can call me whatever you like‚ÄîI don‚Äôt have a specific name. How can I assist you today? üòä'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Max Length** This limits the total number of tokens the model can generate, helping control response length and prevent irrelevant output."
      ],
      "metadata": {
        "id": "RW6oF6wK0L5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatDeepSeek(model=\"deepseek-chat\", max_tokens=5)\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "chain.invoke( {\"input\": 'hello! what\\' your name?'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o_cbFy0V0XmF",
        "outputId": "cdf755ba-3bd6-475a-ba6e-9374f5c95f07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! I‚Äôm'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Prompting Techniques**"
      ],
      "metadata": {
        "id": "bPOyoMZZKn9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Zero-Shot Prompting** Large language models (LLMs) today, such as GPT-3.5 Turbo, GPT-4, and Claude 3, are tuned to follow instructions and are trained on large amounts of data. Large-scale training makes these models capable of performing some tasks in a \"zero-shot\" manner. Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."
      ],
      "metadata": {
        "id": "rW7OjIeqMzC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatDeepSeek(model=\"deepseek-chat\")\n",
        "\n",
        "chain = prompt | model | StrOutputParser()\n",
        "\n",
        "your_prompt = \"\"\"Classify the text into neutral, negative or positive.\n",
        "Text: I think the vacation is okay.\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "chain.invoke({\"input\": your_prompt})"
      ],
      "metadata": {
        "id": "h77_UeVAKnOy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6bdf2eec-1250-419b-dfc3-5c466a2ac09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Neutral'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Few-Shot Prompting** While large-language models demonstrate remarkable zero-shot capabilities, they still fall short on more complex tasks when using the zero-shot setting. Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance. The demonstrations serve as conditioning for subsequent examples where we would like the model to generate a response.\n",
        "\n",
        "According to [Touvron et al. 2023](https://arxiv.org/pdf/2302.13971.pdf) few shot properties first appeared when models were scaled to a sufficient size ([Kaplan et al., 2020](https://arxiv.org/abs/2001.08361))."
      ],
      "metadata": {
        "id": "wLUojJlxNHsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = \"\"\"This is bad! // Negative\n",
        "This is awesome! // Positive\n",
        "Wow that movie was rad! // Positive\n",
        "What a horrible show! //\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})"
      ],
      "metadata": {
        "id": "aiMceIYUNISX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "786d01ee-cf73-4c34-adaf-27c5717267bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Negative'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**3. Chain-of-Thought Prompting** Introduced in [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding.\n",
        "\n",
        "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fcot.1933d9fe.png&w=1920&q=75)  "
      ],
      "metadata": {
        "id": "vah7gl7qNIlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = \"\"\"I went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?\n",
        "Let's think step by step.\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})"
      ],
      "metadata": {
        "id": "cd0llsdSNJDC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b59707b5-d662-4af0-a01c-588a57fc8832"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure, let's break it down step by step:\\n\\n1. You initially bought 10 apples.\\n2. You gave 2 apples to the neighbor, leaving you with 10 - 2 = 8 apples.\\n3. You then gave 2 apples to the repairman, leaving you with 8 - 2 = 6 apples.\\n4. You bought 5 more apples, so now you have 6 + 5 = 11 apples.\\n5. You ate 1 apple, leaving you with 11 - 1 = 10 apples.\\n\\nSo, you are left with 10 apples.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Self-Consistency** Perhaps one of the more advanced techniques out there for prompt engineering is self-consistency. Proposed by [Wang et al. (2022)](https://arxiv.org/abs/2203.11171), self-consistency aims \"to replace the naive greedy decoding used in chain-of-thought prompting\". The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning."
      ],
      "metadata": {
        "id": "KGKBvMJPOIoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = \"\"\"**Problem: Calculate the total cost if you buy 3 notebooks at $2 each and 2 pens at $1.50 each.**\n",
        "\n",
        "**Examples:**\n",
        "\n",
        "1. **Example Problem:** How many apples can you buy with $10 if each costs $2?\n",
        "   **Solution:**\n",
        "   - I have $10.\n",
        "   - Each apple costs $2.\n",
        "   - $10 / $2 = 5 apples.\n",
        "   **Answer: You can buy 5 apples.**\n",
        "\n",
        "2. **Example Problem:** If a train travels 50 miles in an hour, how far will it travel in 4 hours?\n",
        "   **Solution:**\n",
        "   - The train travels 50 miles in one hour.\n",
        "   - In 4 hours, it will travel 50 miles/hour * 4 hours = 200 miles.\n",
        "   **Answer: The train will travel 200 miles.**\n",
        "\n",
        "**Your Task:**\n",
        "\n",
        "- Use the chain of thought to break down the cost calculation.\n",
        "- Sample multiple reasoning paths.\n",
        "- Determine the most consistent calculation across different samples.\n",
        "\n",
        "**Reasoning:**\n",
        "- Start by identifying the cost of one category of items:\n",
        "  - 3 notebooks at $2 each = $6.\n",
        "- Then, calculate the cost for the other category:\n",
        "  - 2 pens at $1.50 each = $3.\n",
        "- Add both amounts to find the total cost:\n",
        "  - $6 (notebooks) + $3 (pens) = $9.\n",
        "\n",
        "**Consistency Check:**\n",
        "- Sample several reasoning paths. For example:\n",
        "  1. Calculate total cost for notebooks first, then pens, and sum.\n",
        "  2. Calculate total cost for pens first, then notebooks, and sum.\n",
        "  3. Directly multiply and add the costs of notebooks and pens.\n",
        "- Compare the answers and select the most frequently occurring result.\n",
        "\n",
        "**Final Answer:**\n",
        "- After verifying consistency across samples, conclude with the most consistent answer.\n",
        "\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})\n"
      ],
      "metadata": {
        "id": "dz3RT22jOOzC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "b348daf9-f8c2-42fa-8f9a-234a4ad435e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"To calculate the total cost of buying 3 notebooks and 2 pens, we will proceed through a series of logical steps.\\n\\n### Step-by-Step Calculation\\n\\n1. **Calculate the cost of the notebooks:**\\n   - You are buying 3 notebooks.\\n   - Each notebook costs $2.\\n   - Total cost for notebooks = Number of notebooks √ó Price per notebook\\n   - Total cost for notebooks = 3 √ó $2 = $6.\\n\\n2. **Calculate the cost of the pens:**\\n   - You are buying 2 pens.\\n   - Each pen costs $1.50.\\n   - Total cost for pens = Number of pens √ó Price per pen\\n   - Total cost for pens = 2 √ó $1.50 = $3.\\n\\n3. **Calculate the total cost:**\\n   - Add the total cost of the notebooks to the total cost of the pens.\\n   - Total cost = Cost of notebooks + Cost of pens\\n   - Total cost = $6 (notebooks) + $3 (pens) = $9.\\n\\n### Consistency Check\\n\\nLet's verify the calculation using different reasoning paths:\\n\\n- **Path 1: Calculate notebooks first, then pens:**\\n  - Total cost for notebooks: 3 √ó $2 = $6\\n  - Total cost for pens: 2 √ó $1.50 = $3\\n  - Sum: $6 + $3 = $9\\n\\n- **Path 2: Calculate pens first, then notebooks:**\\n  - Total cost for pens: 2 √ó $1.50 = $3\\n  - Total cost for notebooks: 3 √ó $2 = $6\\n  - Sum: $3 + $6 = $9\\n\\n- **Path 3: Simultaneous calculation by adding individual items:**\\n  - (3 √ó $2) + (2 √ó $1.50) = $6 + $3 = $9\\n\\nAcross all paths, the calculations consistently result in a total cost of $9.\\n\\n### Final Answer\\n\\nThe total cost if you buy 3 notebooks at $2 each and 2 pens at $1.50 each is **$9**.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**5. Tree of Thoughts (ToT)** For complex tasks that require exploration or strategic lookahead, traditional or simple prompting techniques fall short. [Yao et el. (2023)](https://arxiv.org/abs/2305.10601) and [Long (2023)](https://arxiv.org/abs/2305.08291) recently proposed Tree of Thoughts (ToT), a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts that serve as intermediate steps for general problem solving with language models.\n",
        "\n",
        "ToT maintains a tree of thoughts, where thoughts represent coherent language sequences that serve as intermediate steps toward solving a problem. This approach enables an LM to self-evaluate the progress through intermediate thoughts made towards solving a problem through a deliberate reasoning process. The LM's ability to generate and evaluate thoughts is then combined with search algorithms (e.g., breadth-first search and depth-first search) to enable systematic exploration of thoughts with lookahead and backtracking.\n",
        "\n",
        "\n",
        "\n",
        "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FTOT.3b13bc5e.png&w=3840&q=75)  "
      ],
      "metadata": {
        "id": "fdwRvqrGOYJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = \"\"\"Imagine you are solving a complex logic puzzle where you need to arrange six objects in a specific order based on a set of rules. Each object can only be placed once, and certain objects must be placed before others according to the rules provided below. You need to explore different arrangements systematically to find the correct solution. Follow these steps:_\n",
        "\n",
        "1. **Step 1: Start by listing possible first moves**: Consider each of the six objects and think through which ones could logically come first based on the rules.\n",
        "2. **Step 2: Generate multiple intermediate thoughts for the second position**: After selecting a first object, think about which objects can go next while considering the constraints. Explore at least three different possibilities.\n",
        "3. **Step 3: Evaluate each option**: After placing the first two objects, evaluate whether the current arrangement aligns with the rules. If it does, proceed to explore the third position. If not, backtrack and try another path.\n",
        "4. **Step 4: Search using Breadth-First Search (BFS)**: Expand on each potential arrangement one step at a time. Use BFS to keep track of multiple arrangements at once, evaluating their adherence to the rules as you go.\n",
        "5. **Step 5: Look ahead and refine**: After placing three objects, look ahead to the remaining positions and evaluate potential placements. If a placement leads to a conflict, backtrack and explore a different arrangement. Use depth-first search (DFS) if necessary to explore more deeply.\n",
        "6. **Step 6: Complete the arrangement and find the correct solution**: Continue exploring arrangements, self-evaluating each step, until the correct order is found. Summarize your reasoning process after completing the task.\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})"
      ],
      "metadata": {
        "id": "w32M94fOOYqE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "943421a6-a4c9-42dd-dc97-4f3c8bbe85b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"### **Understanding the Problem**\\n\\nBefore diving into solving the puzzle, it's crucial to fully grasp the problem statement. We have six distinct objects that need to be arranged in a specific order. Each object can only occupy one position in the sequence, and there are certain rules dictating which objects must come before others. Our goal is to determine the correct sequence that adheres to all these rules.\\n\\n### **Step 1: Listing Possible First Moves**\\n\\nThe first step is to identify which objects can logically be placed first in the sequence. To do this, we need to consider the rules that specify which objects must come before others. If an object has no prerequisites (i.e., no rules stating that it must come after another object), it's a candidate for the first position.\\n\\n**Assumption:** Since the specific objects and rules aren't provided, I'll assume a hypothetical set of objects and rules to illustrate the process.\\n\\nLet's define our six objects as: **A, B, C, D, E, F**.\\n\\nAnd the rules are:\\n1. **A** must come before **B**.\\n2. **C** must come before **D**.\\n3. **E** must come before **F**.\\n4. **B** must come before **D**.\\n5. **D** must come before **E**.\\n\\n**Analyzing the Rules:**\\n\\n- **A** must come before **B**.\\n- **C** must come before **D**.\\n- **E** must come before **F**.\\n- **B** must come before **D**.\\n- **D** must come before **E**.\\n\\nFrom these rules, we can infer dependencies:\\n- **A** ‚Üí **B** ‚Üí **D** ‚Üí **E** ‚Üí **F**\\n- **C** ‚Üí **D**\\n\\nThis means:\\n- **A** must come before **B**, which must come before **D**, which must come before **E**, which must come before **F**.\\n- **C** must come before **D**.\\n\\n**Identifying Potential First Objects:**\\n\\nLooking at the dependencies:\\n- **A** has no prerequisites; it can be first.\\n- **C** has no prerequisites; it can be first.\\n- **B**, **D**, **E**, and **F** all have prerequisites and cannot be first.\\n\\nTherefore, the possible first objects are **A** and **C**.\\n\\n### **Step 2: Generating Intermediate Thoughts for the Second Position**\\n\\nNow, let's explore the second position after selecting a first object. We'll consider both **A** and **C** as starting points.\\n\\n#### **Case 1: Starting with A**\\n\\nIf **A** is first, the sequence so far is: **A**, ?\\n\\n**Possible Second Objects:**\\n\\nFrom the rules:\\n- **A** must come before **B**, so **B** is a candidate.\\n- **C** has no prerequisites and hasn't been placed yet, so **C** is also a candidate.\\n- **D**, **E**, and **F** have prerequisites and cannot be placed yet.\\n\\nThus, the possible second objects after **A** are **B** and **C**.\\n\\n**Exploring Three Possibilities:**\\n\\n1. **A**, **B**, ?\\n2. **A**, **C**, ?\\n3. **A**, **B**, **C**, ?\\n\\nWait, the third option seems to be extending beyond the second position. Let's focus on the second position first.\\n\\nSo, the two possibilities for the second position after **A** are:\\n1. **A**, **B**, ?\\n2. **A**, **C**, ?\\n\\n#### **Case 2: Starting with C**\\n\\nIf **C** is first, the sequence so far is: **C**, ?\\n\\n**Possible Second Objects:**\\n\\nFrom the rules:\\n- **C** must come before **D**, so **D** is a candidate.\\n- **A** has no prerequisites and hasn't been placed yet, so **A** is also a candidate.\\n- **B**, **E**, and **F** have prerequisites and cannot be placed yet.\\n\\nThus, the possible second objects after **C** are **A** and **D**.\\n\\n**Exploring Three Possibilities:**\\n\\n1. **C**, **A**, ?\\n2. **C**, **D**, ?\\n3. **C**, **A**, **D**, ?\\n\\nAgain, the third option extends beyond the second position. So, the two possibilities for the second position after **C** are:\\n1. **C**, **A**, ?\\n2. **C**, **D**, ?\\n\\n### **Step 3: Evaluating Each Option**\\n\\nNow, let's evaluate each of these options to see if they align with the rules.\\n\\n#### **Case 1: Starting with A**\\n\\n**Option 1: A, B, ?**\\n\\n- **A** is before **B**: Valid.\\n- Next, we need to place **C**, **D**, **E**, **F**.\\n- **C** must come before **D**.\\n- **B** must come before **D**.\\n- **D** must come before **E**.\\n- **E** must come before **F**.\\n\\nPossible next steps:\\n- **C** can be placed next since it has no prerequisites beyond being before **D**.\\n- After **C**, **D** must come before **E** and **F**.\\n- Then **E** before **F**.\\n\\nSo, one possible sequence: **A**, **B**, **C**, **D**, **E**, **F**.\\n\\n**Option 2: A, C, ?**\\n\\n- **A** is before **B**: Valid.\\n- **C** is before **D**: Valid.\\n- Next, we need to place **B**, **D**, **E**, **F**.\\n- **B** must come before **D**.\\n- **D** must come before **E**.\\n- **E** must come before **F**.\\n\\nPossible next steps:\\n- **B** can be placed next since it must come before **D**.\\n- After **B**, **D** must come before **E** and **F**.\\n- Then **E** before **F**.\\n\\nSo, one possible sequence: **A**, **C**, **B**, **D**, **E**, **F**.\\n\\n**Evaluation:**\\n\\nBoth sequences **A, B, C, D, E, F** and **A, C, B, D, E, F** seem to satisfy all the rules.\\n\\n#### **Case 2: Starting with C**\\n\\n**Option 1: C, A, ?**\\n\\n- **C** is before **D**: Valid.\\n- **A** is before **B**: Valid.\\n- Next, we need to place **B**, **D**, **E**, **F**.\\n- **B** must come before **D**.\\n- **D** must come before **E**.\\n- **E** must come before **F**.\\n\\nPossible next steps:\\n- **B** can be placed next since it must come before **D**.\\n- After **B**, **D** must come before **E** and **F**.\\n- Then **E** before **F**.\\n\\nSo, one possible sequence: **C**, **A**, **B**, **D**, **E**, **F**.\\n\\n**Option 2: C, D, ?**\\n\\n- **C** is before **D**: Valid.\\n- Next, we need to place **A**, **B**, **E**, **F**.\\n- **A** must come before **B**.\\n- **B** must come before **D** (but **D** is already placed after **C**).\\n- **D** must come before **E**.\\n- **E** must come before **F**.\\n\\nPossible next steps:\\n- **A** can be placed next since it has no prerequisites beyond being before **B**.\\n- After **A**, **B** must come before **D** (but **D** is already placed).\\n- Wait, **B** must come before **D**, but **D** is already placed after **C**. This creates a conflict because **B** must come before **D**, but **D** is already in position 2.\\n\\nThis means that placing **D** immediately after **C** violates the rule that **B** must come before **D**. Therefore, **C, D, ?** is not a valid sequence.\\n\\n**Evaluation:**\\n\\nOnly the sequence **C, A, B, D, E, F** is valid when starting with **C**.\\n\\n### **Step 4: Searching Using Breadth-First Search (BFS)**\\n\\nTo ensure we explore all possible valid sequences, we'll use the Breadth-First Search (BFS) approach. This means we'll explore all possible sequences level by level, ensuring that at each step, we adhere to the rules.\\n\\n**Starting Point:**\\n\\n- **A** or **C** as the first object.\\n\\n**Level 1: First Object**\\n\\n1. **A**\\n2. **C**\\n\\n**Level 2: Second Object**\\n\\nFor **A**:\\n1. **A**, **B**\\n2. **A**, **C**\\n\\nFor **C**:\\n1. **C**, **A**\\n2. **C**, **D** (invalid as discussed)\\n\\n**Level 3: Third Object**\\n\\nFor **A, B**:\\n1. **A, B, C**\\n2. **A, B, D** (invalid, since **C** must come before **D**)\\n\\nFor **A, C**:\\n1. **A, C, B**\\n2. **A, C, D** (invalid, since **B** must come before **D**)\\n\\nFor **C, A**:\\n1. **C, A, B**\\n2. **C, A, D** (invalid, since **B** must come before **D**)\\n\\n**Level 4: Fourth Object**\\n\\nFor **A, B, C**:\\n1. **A, B, C, D**\\n2. **A, B, C, E** (invalid, since **D** must come before **E**)\\n\\nFor **A, C, B**:\\n1. **A, C, B, D**\\n2. **A, C, B, E** (invalid, since **D** must come before **E**)\\n\\nFor **C, A, B**:\\n1. **C, A, B, D**\\n2. **C, A, B, E** (invalid, since **D** must come before **E**)\\n\\n**Level 5: Fifth Object**\\n\\nFor **A, B, C, D**:\\n1. **A, B, C, D, E**\\n2. **A, B, C, D, F** (invalid, since **E** must come before **F**)\\n\\nFor **A, C, B, D**:\\n1. **A, C, B, D, E**\\n2. **A, C, B, D, F** (invalid, since **E** must come before **F**)\\n\\nFor **C, A, B, D**:\\n1. **C, A, B, D, E**\\n2. **C, A, B, D, F** (invalid, since **E** must come before **F**)\\n\\n**Level 6: Sixth Object**\\n\\nFor **A, B, C, D, E**:\\n1. **A, B, C, D, E, F**\\n\\nFor **A, C, B, D, E**:\\n1. **A, C, B, D, E, F**\\n\\nFor **C, A, B, D, E**:\\n1. **C, A, B, D, E, F**\\n\\n**Valid Sequences Found:**\\n\\n1. **A, B, C, D, E, F**\\n2. **A, C, B, D, E, F**\\n3. **C, A, B, D, E, F**\\n\\n### **Step 5: Looking Ahead and Refining**\\n\\nNow, let's verify if all these sequences adhere to all the rules.\\n\\n**Sequence 1: A, B, C, D, E, F**\\n\\n- **A** before **B**: Yes.\\n- **C** before **D**: Yes.\\n- **E** before **F**: Yes.\\n- **B** before **D**: Yes.\\n- **D** before **E**: Yes.\\n\\nAll rules are satisfied.\\n\\n**Sequence 2: A, C, B, D, E, F**\\n\\n- **A** before **B**: Yes.\\n- **C** before **D**: Yes.\\n- **E** before **F**: Yes.\\n- **B** before **D**: Yes.\\n- **D** before **E**: Yes.\\n\\nAll rules are satisfied.\\n\\n**Sequence 3: C, A, B, D, E, F**\\n\\n- **A** before **B**: Yes.\\n- **C** before **D**: Yes.\\n- **E** before **F**: Yes.\\n- **B** before **D**: Yes.\\n- **D** before **E**: Yes.\\n\\nAll rules are satisfied.\\n\\n### **Step 6: Completing the Arrangement and Finding the Correct Solution**\\n\\nAfter systematically exploring all possible sequences using BFS and evaluating each against the rules, we've identified three valid sequences:\\n\\n1. **A, B, C, D, E, F**\\n2. **A, C, B, D, E, F**\\n3. **C, A, B, D, E, F**\\n\\nAll these sequences satisfy all the given rules. Therefore, any of these could be the correct arrangement depending on whether there are additional constraints or preferences not specified in the problem.\\n\\n### **Final Answer**\\n\\nBased on the provided rules and the systematic exploration of possible sequences, the valid arrangements of the six objects are:\\n\\n1. **A, B, C, D, E, F**\\n2. **A, C, B, D, E, F**\\n3. **C, A, B, D, E, F**\\n\\nEach of these sequences adheres to all the specified rules, ensuring that each object is placed in the correct order relative to the others.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Retrieval Augmented Generation (RAG)** General-purpose language models can be fine-tuned to achieve several common tasks such as sentiment analysis and named entity recognition. These tasks generally don't require additional background knowledge.\n",
        "\n",
        "For more complex and knowledge-intensive tasks, it's possible to build a language model-based system that accesses external knowledge sources to complete tasks. This enables more factual consistency, improves reliability of the generated responses, and helps to mitigate the problem of \"hallucination\".\n",
        "\n",
        "Meta AI researchers introduced a method called [Retrieval Augmented Generation (RAG)](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to address such knowledge-intensive tasks. RAG combines an information retrieval component with a text generator model. RAG can be fine-tuned and its internal knowledge can be modified in an efficient manner and without needing retraining of the entire model.\n",
        "\n",
        "\n",
        "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Frag.c6528d99.png&w=1920&q=75)  \n",
        "\n",
        "A recommended repository of implementations: (langchain) [https://github.com/langchain-ai/langchain]."
      ],
      "metadata": {
        "id": "wpmoWsqnP_Eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = \"\"\"## Instruction: Use the provided retrieval content to answer the question.\n",
        "### Retrieval Content:\n",
        "1. The retrieved document discusses the impact of climate change on polar bear populations in the Arctic, detailing factors like ice melting, loss of habitat, and changes in prey availability.\n",
        "\n",
        "### Question:\n",
        "What are the primary reasons for the decline in polar bear populations as discussed in the retrieval content, and what measures can be implemented to mitigate this issue?\n",
        "\"\"\"\n",
        "chain.invoke({\"input\": your_prompt})"
      ],
      "metadata": {
        "id": "qEGZnErERUKd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "a363f015-74cd-41f8-a8a1-ec5ce0de7d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The primary reasons for the decline in polar bear populations, as discussed in the retrieval content, include:\\n\\n1. **Ice Melting**: Climate change is causing the Arctic ice to melt at an accelerated rate, which reduces the polar bears' habitat. Polar bears rely on sea ice for hunting seals, their primary prey, and for traveling across their range.\\n\\n2. **Loss of Habitat**: As the ice melts, polar bears are losing their natural habitat, forcing them to spend more time on land where food sources are scarce. This leads to increased starvation and reduced reproductive success.\\n\\n3. **Changes in Prey Availability**: The melting ice and changing ecosystems are affecting the availability of seals, the main food source for polar bears. This makes it harder for polar bears to find sufficient food to survive and reproduce.\\n\\n### Measures to Mitigate the Issue:\\n\\n1. **Reducing Greenhouse Gas Emissions**: Implementing global policies to reduce carbon emissions and combat climate change is crucial. This can help slow down the rate of ice melting and stabilize polar bear habitats.\\n\\n2. **Protecting Critical Habitats**: Establishing and enforcing protected areas in the Arctic where human activities are limited can help preserve the remaining ice and the ecosystems polar bears depend on.\\n\\n3. **Research and Monitoring**: Increasing funding for research to monitor polar bear populations and their habitats can provide valuable data to inform conservation strategies.\\n\\n4. **Community Engagement**: Working with local communities to promote sustainable practices and reduce human-wildlife conflicts can help protect polar bears while supporting the livelihoods of Arctic residents.\\n\\n5. **International Cooperation**: Collaborating with international organizations and governments to create and enforce policies that protect polar bears and their habitats on a global scale.\\n\\nBy addressing these factors and implementing these measures, it may be possible to mitigate the decline in polar bear populations and ensure their survival in the face of climate change.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Automatic Reasoning and Tool-use (ART)** Combining CoT prompting and tools in an interleaved manner has shown to be a strong and robust approach to address many tasks with LLMs. These approaches typically require hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. Paranjape et al., (2023) propose a new framework that uses a frozen LLM to automatically generate intermediate reasoning steps as a program.\n",
        "\n",
        "ART works as follows:\n",
        "\n",
        "- given a new task, it select demonstrations of multi-step reasoning and tool use from a task library\n",
        "- at test time, it pauses generation whenever external tools are called, and integrate their output before resuming generation\n",
        "\n",
        "ART encourages the model to generalize from demonstrations to decompose a new task and use tools in appropriate places, in a zero-shot fashion. In addition, ART is extensible as it also enables humans to fix mistakes in the reasoning steps or add new tools by simply updating the task and tool libraries. The process is demonstrated below:\n",
        "\n",
        "![](https://www.promptingguide.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2FART.3b30f615.png&w=1200&q=75)  "
      ],
      "metadata": {
        "id": "USvPTrsGST8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 1: LLMs as a knowledgeable doctor**\n"
      ],
      "metadata": {
        "id": "SEgnOeWYj43i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The pharmacist licensure exam  is a cornerstone in the pharmacy profession, ensuring that candidates possess the requisite knowledge and skills for safe and effective practice. Its significance lies not only in validating credentials but also in safeguarding public health, enabling professional recognition, and ensuring adherence to legal and regulatory standards.\n",
        "\n",
        "Advanced models like ChatGPT have significant potential in exam preparation, boasting an extensive knowledge base and the capability to provide in-depth explanations and clarify complex concepts. However, despite the prowess of such large models, if prompts are not designed appropriately, the information retrieved might be inaccurate or incomplete, potentially hindering success in the pharmacist exam."
      ],
      "metadata": {
        "id": "A2M-7NsrLe7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://NLP-course-cuhksz.github.io/Assignments/Assignment1/task1/data/1.exam.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeJKZ08wljui",
        "outputId": "3149391f-0a6e-4629-9e28-a7cf4f7796dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-06 22:47:21--  https://nlp-course-cuhksz.github.io/Assignments/Assignment1/task1/data/1.exam.json\n",
            "Resolving nlp-course-cuhksz.github.io (nlp-course-cuhksz.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to nlp-course-cuhksz.github.io (nlp-course-cuhksz.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 86227 (84K) [application/json]\n",
            "Saving to: ‚Äò1.exam.json‚Äô\n",
            "\n",
            "\r1.exam.json           0%[                    ]       0  --.-KB/s               \r1.exam.json         100%[===================>]  84.21K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-03-06 22:47:21 (4.70 MB/s) - ‚Äò1.exam.json‚Äô saved [86227/86227]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open('1.exam.json') as f:\n",
        "  data = json.load(f)"
      ],
      "metadata": {
        "id": "FbVxQUfkmZg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpbU3bOdop67",
        "outputId": "1346cd6a-2eb8-46b5-93c6-a0f298dc26c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': '27. Ê†πÊçÆÂõΩÂÆ∂ËçØÂìÅÁõëÁù£ÁÆ°ÁêÜÂ±ÄÔºåÂÖ¨ÂÆâÈÉ®ÔºåÂõΩÂÆ∂Âç´‚Ω£ÂÅ•Â∫∑ÂßîÂëò‰ºöÁöÑÊúâÂÖ≥ËßÑÂÆöÔºå‚ºúÊúçÂõ∫‰ΩìÂà∂ÂâÇÊØèÂâÇÈáèÂçï‰ΩçÂê´ÁæüËÄÉÈÖÆÁ¢±‰∏çË∂ÖËøá5ÊØ´ÂÖãÔºå‰∏î‰∏çÂê´ÂÖ∂‰ªñ‚øáÈÜâËçØÂìÅÔºåÁ≤æÁ•ûËçØÂìÅÊàñËÄÖËçØÂìÅÁ±ªÊòìÂà∂ÊØíÂåñÂ≠¶ÂìÅÁöÑÂ§ç‚ΩÖÂà∂ÂâÇÂàó‚ºäÔºàÔºâ„ÄÇ',\n",
              " 'option': {'A': 'Âê´‚øáÈÜâËçØÂìÅÂ§ç‚ΩÖÂà∂ÂâÇÁöÑÁÆ°ÁêÜ',\n",
              "  'B': 'Á¨¨‚ºÜÁ±ªÁ≤æÁ•ûËçØÂìÅÁÆ°ÁêÜ',\n",
              "  'C': 'Á¨¨‚ºÄÁ±ªÁ≤æÁ•ûËçØÂìÅÁÆ°ÁêÜ',\n",
              "  'D': 'ÂåªÁñó‚Ω§ÊØíÊÄßËçØÂìÅÁÆ°ÁêÜ',\n",
              "  'E': ''},\n",
              " 'analysis': '‚ºúÊúçÂõ∫‰ΩìÂà∂ÂâÇÊØèÂâÇÈáèÂçï‰ΩçÂê´ÁæüËÄÉÈÖÆÁ¢±‰∏çË∂ÖËøá5ÊØ´ÂÖãÔºå‰∏î‰∏çÂê´ÂÖ∂‰ªñ‚øáÈÜâËçØÂìÅ„ÄÅÁ≤æÁ•ûËçØÂìÅÊàñËçØÂìÅÁ±ªÊòìÂà∂ÊØíÂåñÂ≠¶ÂìÅÁöÑÂ§ç‚ΩÖÂà∂ÂâÇÂàó‚ºäÁ¨¨‚ºÜÁ±ªÁ≤æÁ•ûËçØÂìÅÁÆ°ÁêÜ„ÄÇ',\n",
              " 'answer': 'B',\n",
              " 'question_type': 'ÊúÄ‰Ω≥ÈÄâÊã©È¢ò',\n",
              " 'source': '2021Âπ¥Êâß‰∏öËçØÂ∏àËÅå‰∏öËµÑÊ†ºËÄÉËØï„ÄäËçØ‰∫ãÁÆ°ÁêÜ‰∏éÊ≥ïËßÑ„Äã'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = \"\"\"ËØ∑ÂõûÁ≠î‰∏ãÈù¢ÁöÑÂ§öÈÄâÈ¢òÔºåËØ∑Áõ¥Êé•Ê≠£Á°ÆÁ≠îÊ°àÈÄâÈ°πÔºå‰∏çË¶ÅËæìÂá∫ÂÖ∂‰ªñÂÜÖÂÆπ„ÄÇ\n",
        "{question}\n",
        "{options}\"\"\"\n",
        "\n",
        "def get_query(da):\n",
        "  da['options'] = '\\n'.join([f\"{k}:{v}\" for k,v in da['option'].items()])\n",
        "  return your_prompt.format_map(da)\n",
        "\n",
        "get_query(data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "gtmMDV5Hnaun",
        "outputId": "a8cfdeba-352f-4d2a-84b2-1df2c9522917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ËØ∑ÂõûÁ≠î‰∏ãÈù¢ÁöÑÂ§öÈÄâÈ¢òÔºåËØ∑Áõ¥Êé•Ê≠£Á°ÆÁ≠îÊ°àÈÄâÈ°πÔºå‰∏çË¶ÅËæìÂá∫ÂÖ∂‰ªñÂÜÖÂÆπ„ÄÇ\\n27. Ê†πÊçÆÂõΩÂÆ∂ËçØÂìÅÁõëÁù£ÁÆ°ÁêÜÂ±ÄÔºåÂÖ¨ÂÆâÈÉ®ÔºåÂõΩÂÆ∂Âç´‚Ω£ÂÅ•Â∫∑ÂßîÂëò‰ºöÁöÑÊúâÂÖ≥ËßÑÂÆöÔºå‚ºúÊúçÂõ∫‰ΩìÂà∂ÂâÇÊØèÂâÇÈáèÂçï‰ΩçÂê´ÁæüËÄÉÈÖÆÁ¢±‰∏çË∂ÖËøá5ÊØ´ÂÖãÔºå‰∏î‰∏çÂê´ÂÖ∂‰ªñ‚øáÈÜâËçØÂìÅÔºåÁ≤æÁ•ûËçØÂìÅÊàñËÄÖËçØÂìÅÁ±ªÊòìÂà∂ÊØíÂåñÂ≠¶ÂìÅÁöÑÂ§ç‚ΩÖÂà∂ÂâÇÂàó‚ºäÔºàÔºâ„ÄÇ\\nA:Âê´‚øáÈÜâËçØÂìÅÂ§ç‚ΩÖÂà∂ÂâÇÁöÑÁÆ°ÁêÜ\\nB:Á¨¨‚ºÜÁ±ªÁ≤æÁ•ûËçØÂìÅÁÆ°ÁêÜ\\nC:Á¨¨‚ºÄÁ±ªÁ≤æÁ•ûËçØÂìÅÁÆ°ÁêÜ\\nD:ÂåªÁñó‚Ω§ÊØíÊÄßËçØÂìÅÁÆ°ÁêÜ\\nE:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(get_query(data[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j5t7lU5kpO3Z",
        "outputId": "a5ce6cef-af1e-4d80-ea0f-b13848b9c0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ËÆ°ÁÆóÂÅöÈ¢òÂáÜÁ°ÆÁéá\n",
        "your_prompt = \"\"\"Please answer the multiple choice questions below, please direct the correct answer option and do not output anything else.\n",
        "{question}\n",
        "{options}\"\"\"\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_ans(ans):\n",
        "    match = re.findall(r'.*?([A-E]+(?:[„ÄÅ, ]+[A-E]+)*)', ans)\n",
        "    if match:\n",
        "        last_match = match[-1]\n",
        "        return ''.join(re.split(r'[„ÄÅ, Ôºå]+', last_match))\n",
        "    return ''\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "for da in tqdm(data[:10]):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if get_ans(da['deepseek_ans']) == da['answer']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Ê®°ÂûãÂáÜÁ°ÆÁéá: {correct_num/total_num:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRw_RUoUpfAH",
        "outputId": "d51633b2-df21-4ae2-e05f-d115b03bcd5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:44<00:00, 10.46s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ê®°ÂûãÂáÜÁ°ÆÁéá: 70.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">You need to optimize the prompt to improve the performance (accuracy) of large language models (LLMs).</font>"
      ],
      "metadata": {
        "id": "SwPb1z2rFJzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Task 2: LLMS for AI Feedback**"
      ],
      "metadata": {
        "id": "fCGdiS8SqtJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final stage of large language model training involves reinforcement learning through feedback. Such feedback can come from either  human experts or   AI. This feedback is used to learn a reward model, with data defined in a triplet form. This triplet comes from a question, two answers, and a choice by a human or AI on which answer is better.\n",
        "\n",
        "The triplet consists of three elements: a  *question*, the  *chosen* answer, and the *rejected* answer. You are asked to use ChatGPT to provide the feedback, namely, choose the preferred one. Note that the feedback is highly biased by the order of placed answers, please shuffle the order of answers when using chatGPT for preference feedback."
      ],
      "metadata": {
        "id": "GTLaP9TULpNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget  https://NLP-course-cuhksz.github.io/Assignments/Assignment1/task2/data/1.rlhf.jsonl\n",
        "\n",
        "import json\n",
        "with open('1.rlhf.jsonl') as f:\n",
        "  data = [json.loads(l) for l in f]\n",
        "data[0] # sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chUVTLQLrFcF",
        "outputId": "564a8976-bafd-43ca-94d9-dd718c9cf552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-06 22:50:09--  https://nlp-course-cuhksz.github.io/Assignments/Assignment1/task2/data/1.rlhf.jsonl\n",
            "Resolving nlp-course-cuhksz.github.io (nlp-course-cuhksz.github.io)... 185.199.109.153, 185.199.111.153, 185.199.110.153, ...\n",
            "Connecting to nlp-course-cuhksz.github.io (nlp-course-cuhksz.github.io)|185.199.109.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70333 (69K) [application/octet-stream]\n",
            "Saving to: ‚Äò1.rlhf.jsonl‚Äô\n",
            "\n",
            "1.rlhf.jsonl        100%[===================>]  68.68K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-03-06 22:50:09 (5.36 MB/s) - ‚Äò1.rlhf.jsonl‚Äô saved [70333/70333]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Question': 'Human: What are some aspects of religious Islam that are compatible with western civilization?',\n",
              " 'Answer1': \"Assistant: I'm not sure what 'western civilization' means in this context, but if we assume it means the values of modern Western culture, it seems there are many facets of Islamic religious practice that are compatible with those values.  For example, a key principle of Islam is that there is one God who should be worshiped, which is consistent with the principle of religious tolerance, as well as the value of equality.  There are different branches of Islam, but many of them require\",\n",
              " 'Answer2': 'Assistant: Well in a basic sense it is possible for Muslims to live in the Western world and take part in Western civilization. In fact, many Muslims have been at the center of important cultural developments of the last centuries.',\n",
              " 'Preference': 'Answer2'}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTPm1-ngWQ38",
        "outputId": "dfa0ed7a-54f3-452f-e252-ef0b47caea9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Question': 'Human: What are some aspects of religious Islam that are compatible with western civilization?',\n",
              " 'Answer1': \"Assistant: I'm not sure what 'western civilization' means in this context, but if we assume it means the values of modern Western culture, it seems there are many facets of Islamic religious practice that are compatible with those values.  For example, a key principle of Islam is that there is one God who should be worshiped, which is consistent with the principle of religious tolerance, as well as the value of equality.  There are different branches of Islam, but many of them require\",\n",
              " 'Answer2': 'Assistant: Well in a basic sense it is possible for Muslims to live in the Western world and take part in Western civilization. In fact, many Muslims have been at the center of important cultural developments of the last centuries.',\n",
              " 'Preference': 'Answer2'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = '''\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "A good response should be relevant, accurate and helpful. Which is better, Answer1 or Answer2?\n",
        "Do not explain your answer, just output 'Answer1' or 'Answer2'.\n",
        "'''\n",
        "\n",
        "def get_query(da):\n",
        "  return your_prompt.format_map(da)\n",
        "\n",
        "testdata = data[0]\n",
        "\n",
        "print(get_query(testdata))\n",
        "\n",
        "print(f'---- DeepSeek-Chat response ----')\n",
        "\n",
        "print(chain.invoke(get_query(testdata)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nci7lKr6rdT8",
        "outputId": "a7d4d7f6-443c-42b2-8b4a-b42bb0b6f5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Question]:\n",
            "Human: What are some aspects of religious Islam that are compatible with western civilization?\n",
            "\n",
            "[Answer1]:\n",
            "Assistant: I'm not sure what 'western civilization' means in this context, but if we assume it means the values of modern Western culture, it seems there are many facets of Islamic religious practice that are compatible with those values.  For example, a key principle of Islam is that there is one God who should be worshiped, which is consistent with the principle of religious tolerance, as well as the value of equality.  There are different branches of Islam, but many of them require\n",
            "\n",
            "[Answer2]:\n",
            "Assistant: Well in a basic sense it is possible for Muslims to live in the Western world and take part in Western civilization. In fact, many Muslims have been at the center of important cultural developments of the last centuries.\n",
            "\n",
            "A good response should be relevant, accurate and helpful. Which is better, Answer1 or Answer2?\n",
            "Do not explain your answer, just output 'Answer1' or 'Answer2'.\n",
            "\n",
            "---- DeepSeek-Chat response ----\n",
            "Answer2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "your_prompt = '''\n",
        "[Question]:\n",
        "{Question}\n",
        "\n",
        "[Answer1]:\n",
        "{Answer1}\n",
        "\n",
        "[Answer2]:\n",
        "{Answer2}\n",
        "\n",
        "A good response should be relevant, accurate and helpful. Which is better, Answer1 or Answer2?\n",
        "Do not explain your answer, just output 'Answer1' or 'Answer2'.\n",
        "'''\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "correct_num = 0\n",
        "total_num = 0\n",
        "for da in tqdm(data[:10]):\n",
        "  da['deepseek_ans'] =  chain.invoke(get_query(da))\n",
        "  if da['deepseek_ans'] == da['Preference']:\n",
        "    correct_num += 1\n",
        "  total_num += 1\n",
        "print(f\"Model consistency rate with human: {correct_num/total_num:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeHYMM_rr7aV",
        "outputId": "78e13e31-0e54-4187-ed0e-92cae1a12421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:35<00:00,  9.56s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model consistency rate with human: 50.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">You need to optimize the prompt to improve the performance (consistency rate) of large language models (LLMs).</font>"
      ],
      "metadata": {
        "id": "c_nVCyGLFn6U"
      }
    }
  ]
}