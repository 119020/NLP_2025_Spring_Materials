{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/119020/NLP_Assignment_1_Exploring-Word-Embeddings/blob/main/25Spring_NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Assignment 1: Exploring Word Embeddings\n",
        "**Course Name:** Natural Language Processing (CSC6052/5051/4100/DDA6307/MDS5110)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9yFFKuDZB_ow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Please enter your personal information (make sure you have copied this colab)*\n",
        "\n",
        "**Name**: 匡博文 KUANG, Bowen\n",
        "\n",
        "**Student ID**: 119020237\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y_qFy6J9EgJe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AbC_Md4Gicaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment Requirements\n",
        "\n",
        "This Colab file includes all contents for Assignment 1.\n",
        "\n",
        "#### You are required to:\n",
        "\n",
        "1. **Make a copy of the provided Google Colab file.**  \n",
        "   First, you need to make a copy of the provided file into your own Google Drive. To accomplish this, open the Colab file link, navigate to `File` → `Save a copy in Drive`.\n",
        "\n",
        "2. **Execute the notebook to generate results.**  \n",
        "   You can click on \"Connect to GPU\" to apply for a free T4 GPU. Then, you can press the large play button to run a code cell.\n",
        "\n",
        "3. **Complete the Necessary Parts.**  \n",
        "   Some sections of the code are incomplete and require your input, especially pay attention to the parts marked with **<font color=\"red\">[Task]</font>**. These sections are critical for scoring the assignment.\n",
        "\n",
        "For more detailed instructions, refer to [Working with Google Colab](https://docs.google.com/document/d/1vMe8kC-oSyP3w7rIurDbG3NqfyQw7sZJ2C_S2ngtQnk/edit?usp=sharing).\n",
        "\n",
        "## Submission Guidelines\n",
        "\n",
        "Follow these steps to submit your assignment:\n",
        "\n",
        "1. **Export the Notebook:** Navigate to `File` → `Download .ipynb` to download your notebook.\n",
        "\n",
        "2. **Upload Your File:** Access the [Blackboard system](https://bb.cuhk.edu.cn/) and upload your `.ipynb` file."
      ],
      "metadata": {
        "id": "nQDMk5Uu1niv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Overview\n",
        "\n",
        "*Assignment 1* consists of two tasks:\n",
        "- Task 1: Train Word Embeddings with Word2Vec (5 points)\n",
        "- Task 2: Explore word embeddings (3 ponits)\n",
        "- Task 3: Utilize word embeddings (2 points)\n",
        "\n",
        "Your task is to **run all the code in this script** and complete the parts marked with <font color=\"red\">[task]</font>.\n",
        "\n",
        "## Prerequisite\n",
        "If you're new to Python, Numpy, or PyTorch, consider these tutorials for a quick start:\n",
        "- [Python-Numpy-Tutorial](https://cs231n.github.io/python-numpy-tutorial/)\n",
        "- [Introduction to PyTorch](https://colab.research.google.com/drive/1obAmmGHsMizB38aiZJ_-L1bVMT5KOLMd?usp=sharing)"
      ],
      "metadata": {
        "id": "H0gO12GJE06O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Train Word Embeddings with Word2Vec\n",
        "\n",
        "**In this task, you will implement and train your own Word2Vec model.**\n",
        "\n",
        "Before diving in, let's clarify what Word2Vec is.\n",
        "\n",
        "Its core concept is straightforward: you can infer the meaning of a word from its neighbors - the words that frequently appear in the same context. Consider this illustration:\n",
        "![Contexts](https://image.ibb.co/mnQ2uz/2018_09_17_21_07_08.png)\n",
        "\n",
        "A basic approach is to use the context word counts as meaningful word vectors. Take this simple corpus for example:\n",
        "\n",
        "```\n",
        "The red fox jumped\n",
        "The brown fox jumped\n",
        "```\n",
        "\n",
        "The count vectors would look like this:\n",
        "```\n",
        "        the fox jumped red brown\n",
        "red   = (1   1    1     0    0)\n",
        "brown = (1   1    1     0    0)\n",
        "```\n",
        "\n",
        "Notice how `red` and `brown` have similar vectors! We're close to solving the problem, but the goal is to obtain more compact embedding vectors.\n",
        "\n",
        "This is where Word2Vec algorithms come into play. They construct embedding vectors based on the word's neighbors in the corpus.\n",
        "\n",
        "For a more detailed introduction, check out this post: [king - man + woman = queen; but why?](http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html).\n",
        "\n",
        "Let's do some preparation work before moving to the interesting stuff.\n",
        "\n"
      ],
      "metadata": {
        "id": "SwAJjpp-K8RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Preparation**"
      ],
      "metadata": {
        "id": "8rK5IxppLRHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment installation and data download"
      ],
      "metadata": {
        "id": "blHR2OjyO5mg"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vacV4BIFI8l",
        "outputId": "b4bcfc50-3b7e-4a6d-8652-b7721cef6951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# !pip3 -qq install torch==1.1\n",
        "!pip -qq install nltk==3.8\n",
        "!pip -qq install gensim\n",
        "!pip -qq install bokeh==3.2.0\n",
        "\n",
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip -o quora.zip\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "np.random.seed(42)\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tqdm import tqdm\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.5 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 1.6.1 requires bokeh<3.7.0,>=3.5.0, but you have bokeh 3.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mArchive:  quora.zip\n",
            "  inflating: train.csv               \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenize and lower-case texts."
      ],
      "metadata": {
        "id": "wgSYdy30O_UZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "texts = texts[:50000] # Accelerated operation\n",
        "print(len(texts))\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in tqdm(texts)]\n",
        "\n",
        "assert len(tokenized_texts) == len(texts)\n",
        "assert isinstance(tokenized_texts[0], list)\n",
        "assert isinstance(tokenized_texts[0][0], str)"
      ],
      "metadata": {
        "id": "t27TcJKJO-ry",
        "outputId": "27bb5fb3-44d8-44c2-ff14-21cbab8156bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:15<00:00, 3221.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_texts[0]\n",
        "#texts[0:10]"
      ],
      "metadata": {
        "id": "C9K7y4mD2UK2",
        "outputId": "f7879948-fc35-4a86-bf55-5a801680b573",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['what',\n",
              " 'is',\n",
              " 'the',\n",
              " 'step',\n",
              " 'by',\n",
              " 'step',\n",
              " 'guide',\n",
              " 'to',\n",
              " 'invest',\n",
              " 'in',\n",
              " 'share',\n",
              " 'market',\n",
              " 'in',\n",
              " 'india',\n",
              " '?']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYoj91iDDDfT"
      },
      "source": [
        "2. Collect the indices of the words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PL471pGjuVN",
        "outputId": "eed5e76c-1f85-4eb9-d078-19793c4ef37b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "MIN_COUNT = 5\n",
        "\n",
        "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
        "word2index = {\n",
        "    '<unk>': 0\n",
        "}\n",
        "\n",
        "print(words_counter.most_common())\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < MIN_COUNT:# 只统计出现次数>=5的word\n",
        "        break\n",
        "\n",
        "    word2index[word] = len(word2index)\n",
        "\n",
        "print(word2index)\n",
        "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
        "\n",
        "print('Vocabulary size:', len(word2index))\n",
        "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
        "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
        "print('Most freq words:', index2word[1:21])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<unk>': 0, '?': 1, 'the': 2, 'what': 3, 'is': 4, 'how': 5, 'i': 6, 'a': 7, 'to': 8, 'in': 9, 'do': 10, 'of': 11, 'are': 12, 'and': 13, 'can': 14, 'for': 15, ',': 16, 'you': 17, 'why': 18, 'it': 19, 'best': 20, 'my': 21, 'does': 22, 'on': 23, 'or': 24, 'which': 25, 'if': 26, 'be': 27, 'have': 28, 'with': 29, 'some': 30, \"'s\": 31, 'that': 32, '.': 33, 'should': 34, 'get': 35, 'an': 36, 'from': 37, 'your': 38, 'india': 39, ')': 40, '(': 41, 'who': 42, 'when': 43, 'like': 44, 'at': 45, 'people': 46, 'will': 47, 'good': 48, 'would': 49, 'there': 50, 'between': 51, 'as': 52, 'about': 53, '``': 54, \"''\": 55, 'not': 56, \"n't\": 57, 'one': 58, 'did': 59, 'most': 60, 'we': 61, 'where': 62, 'was': 63, 'any': 64, 'by': 65, 'make': 66, 'way': 67, 'so': 68, 'after': 69, 'they': 70, 'quora': 71, 'life': 72, ':': 73, 'difference': 74, 'me': 75, 'this': 76, 'has': 77, 'know': 78, 'learn': 79, 'time': 80, 'their': 81, 'use': 82, 'many': 83, 'much': 84, 'someone': 85, 'money': 86, 'am': 87, 'all': 88, 'new': 89, 'think': 90, 'find': 91, 'work': 92, 'without': 93, 'become': 94, 'indian': 95, 'ever': 96, 'than': 97, 'start': 98, 'more': 99, 'better': 100, 'other': 101, 'trump': 102, 'mean': 103, 'but': 104, 'first': 105, 'world': 106, 'job': 107, 'want': 108, 'out': 109, 'online': 110, 'into': 111, 'year': 112, 'up': 113, 'feel': 114, 'us': 115, 'take': 116, 'love': 117, 'go': 118, 'day': 119, 'used': 120, \"'\": 121, 'engineering': 122, '2016': 123, 'could': 124, 'person': 125, 'account': 126, 'being': 127, 'really': 128, 'possible': 129, 'he': 130, 'things': 131, 'number': 132, 'english': 133, 'were': 134, 'buy': 135, 'phone': 136, 'google': 137, 'them': 138, 'long': 139, 'using': 140, 'need': 141, 'girl': 142, 'sex': 143, 'books': 144, 'language': 145, 'business': 146, 'questions': 147, 'his': 148, 'movie': 149, 'different': 150, 'stop': 151, \"'m\": 152, 'free': 153, 'its': 154, 'thing': 155, 'book': 156, 'facebook': 157, 'ways': 158, 'company': 159, 'no': 160, 'old': 161, 'donald': 162, 'weight': 163, 'examples': 164, 'been': 165, 'ca': 166, 'just': 167, 'still': 168, '&': 169, 'now': 170, 'years': 171, 'before': 172, 'see': 173, 'going': 174, 'had': 175, 'real': 176, '2': 177, 'improve': 178, 'college': 179, 'only': 180, 'president': 181, 'prepare': 182, 'her': 183, 'programming': 184, 'under': 185, 'black': 186, 'change': 187, 'war': 188, 'over': 189, 'happen': 190, 'computer': 191, 'data': 192, 'women': 193, 'live': 194, 'country': 195, 'question': 196, 'help': 197, 'app': 198, 'school': 199, 'while': 200, 'instagram': 201, 'back': 202, 'compare': 203, 'lose': 204, 'iphone': 205, 'during': 206, 'important': 207, 'say': 208, 'website': 209, 'movies': 210, 'learning': 211, 'made': 212, 'android': 213, 'name': 214, 'university': 215, 'software': 216, 'science': 217, 'clinton': 218, 'through': 219, 'notes': 220, '’': 221, 'same': 222, 'our': 223, 'system': 224, 'study': 225, 'card': 226, 'exam': 227, 'word': 228, 'read': 229, '1': 230, 'government': 231, 'give': 232, 'bad': 233, 'anyone': 234, 'hillary': 235, 'two': 236, 'increase': 237, 'water': 238, 'she': 239, '3': 240, 'car': 241, 'mobile': 242, 'true': 243, 'top': 244, 'men': 245, ']': 246, '[': 247, 'getting': 248, 'high': 249, 'student': 250, 'win': 251, '10': 252, 'earn': 253, '1000': 254, '500': 255, '5': 256, 'happens': 257, 'girls': 258, 'career': 259, 'cost': 260, 'companies': 261, 'meaning': 262, 'then': 263, 'tell': 264, 'watch': 265, 'differences': 266, 'write': 267, 'students': 268, 'interview': 269, 'home': 270, 'friends': 271, 'web': 272, 'experience': 273, 'energy': 274, 'china': 275, 'hair': 276, 's': 277, 'right': 278, 'usa': 279, 'earth': 280, 'having': 281, 'ask': 282, 'come': 283, 'laptop': 284, 'doing': 285, 'social': 286, 'even': 287, 'answer': 288, 'look': 289, 'working': 290, 'game': 291, 'bank': 292, 'guy': 293, 'white': 294, 'process': 295, 'play': 296, 'tv': 297, 'believe': 298, 'place': 299, 'hard': 300, 'relationship': 301, 'travel': 302, 'days': 303, 'big': 304, 'video': 305, 'youtube': 306, 'own': 307, 'whatsapp': 308, 'employees': 309, 'average': 310, \"'ve\": 311, 'test': 312, 'man': 313, 'done': 314, 'password': 315, 'pakistan': 316, 'body': 317, 'engineer': 318, 'human': 319, 'very': 320, 'chinese': 321, 'safe': 322, 'next': 323, 'age': 324, 'future': 325, 'skills': 326, 'math': 327, 'history': 328, 'last': 329, 'eat': 330, 'windows': 331, 'countries': 332, 'friend': 333, 'salary': 334, 'every': 335, 'off': 336, 'food': 337, 'c': 338, 'service': 339, 'god': 340, 'united': 341, 'american': 342, '%': 343, 'class': 344, 'state': 345, 'myself': 346, 'rs': 347, 'tips': 348, 'email': 349, '2017': 350, 'him': 351, 'interesting': 352, 'available': 353, 'police': 354, 'makes': 355, 'java': 356, 'month': 357, 'favorite': 358, 'states': 359, 'against': 360, 'end': 361, 'keep': 362, 'music': 363, '$': 364, 'run': 365, 'such': 366, 'marketing': 367, 'development': 368, 'rid': 369, 'internet': 370, 'download': 371, 'exist': 372, 'got': 373, 'create': 374, 'woman': 375, 'actually': 376, 'making': 377, 'worth': 378, 'major': 379, 'affect': 380, 'common': 381, 'song': 382, 'never': 383, 'delhi': 384, 'great': 385, 'happened': 386, 'girlfriend': 387, 'each': 388, 'jobs': 389, '4': 390, 'code': 391, 'mechanical': 392, 'today': 393, 'behind': 394, 'something': 395, 'light': 396, 'power': 397, 'kind': 398, 'market': 399, 'too': 400, 'bangalore': 401, 'america': 402, 'looking': 403, 'around': 404, '-': 405, 'management': 406, 'universe': 407, 'both': 408, 'universities': 409, '6': 410, 'differ': 411, 'parents': 412, 'site': 413, 'services': 414, 'always': 415, 'gmail': 416, 'support': 417, '7': 418, 'another': 419, 'months': 420, 'effects': 421, 'places': 422, 'culture': 423, 'writing': 424, 'open': 425, 'pay': 426, 'series': 427, 'modi': 428, 'show': 429, 'per': 430, 'able': 431, 'visit': 432, 'considered': 433, 'mba': 434, 'type': 435, 'delete': 436, 'score': 437, 'law': 438, 'majors': 439, 'die': 440, 'education': 441, 'project': 442, 'deal': 443, 'biggest': 444, 'fall': 445, 'international': 446, 'family': 447, 'cause': 448, 'songs': 449, 'facts': 450, 'popular': 451, 'these': 452, 'course': 453, 'games': 454, 'canada': 455, 'hate': 456, 'death': 457, 'answers': 458, 'private': 459, 'living': 460, 'cat': 461, 'apply': 462, 'review': 463, 'apps': 464, 'control': 465, 'terms': 466, 'speed': 467, 'machine': 468, 'given': 469, 'degree': 470, 'choose': 471, 'program': 472, \"'re\": 473, 'sleep': 474, 'worst': 475, 'civil': 476, 'reason': 477, 'face': 478, 'sentence': 479, 'benefits': 480, 'note': 481, 'call': 482, 'fat': 483, 'move': 484, 'space': 485, 'sites': 486, 'products': 487, 'websites': 488, 'house': 489, 'compared': 490, 'problem': 491, 'main': 492, 'hack': 493, 'etc': 494, 'apple': 495, 'idea': 496, 'plan': 497, 'theory': 498, 'date': 499, 'legal': 500, 'hotel': 501, 'digital': 502, 'preparation': 503, 'drive': 504, 'election': 505, 'similar': 506, 'indians': 507, 'down': 508, 'technology': 509, 'design': 510, 'called': 511, 'form': 512, 'medical': 513, 'health': 514, 'boyfriend': 515, 'gain': 516, 'city': 517, 'options': 518, 'air': 519, 'product': 520, 'talk': 521, 'level': 522, 'amazon': 523, 'value': 524, 'center': 525, 'near': 526, 'u.s.': 527, 'causes': 528, 'videos': 529, 'point': 530, 'instead': 531, 'cell': 532, 'stay': 533, 'seen': 534, '”': 535, 'public': 536, 'story': 537, 'guys': 538, 'wrong': 539, 'mumbai': 540, 'remove': 541, 'child': 542, 'order': 543, 'economy': 544, 'jee': 545, 'purpose': 546, 'humans': 547, 'small': 548, 'australia': 549, 'required': 550, 'currency': 551, 'application': 552, 'because': 553, 'height': 554, 'join': 555, 'successful': 556, 'media': 557, 'side': 558, 'kill': 559, 'startup': 560, 'effect': 561, 'post': 562, 'iit': 563, 'mind': 564, 'daily': 565, 'vote': 566, 'recover': 567, 'institute': 568, '/math': 569, 'rupee': 570, 'california': 571, 'famous': 572, 'alcohol': 573, 'python': 574, 'rate': 575, 'normal': 576, 'courses': 577, 'yourself': 578, 'x': 579, 'build': 580, 'sell': 581, 'reduce': 582, 'snapchat': 583, 'ideas': 584, 'well': 585, 'correct': 586, 'ms': 587, 'based': 588, 'foreign': 589, 'lost': 590, 'faster': 591, 'hours': 592, 'period': 593, 'uk': 594, '}': 595, 'coaching': 596, 'night': 597, 'types': 598, 'part': 599, 'stock': 600, 'send': 601, 'again': 602, 'credit': 603, 'children': 604, 'industry': 605, 'russia': 606, 'current': 607, 'also': 608, 'list': 609, 'dream': 610, '{': 611, 'overcome': 612, '“': 613, 'wear': 614, 'times': 615, 'research': 616, 'porn': 617, 'function': 618, 'less': 619, 'asked': 620, 'view': 621, 'grow': 622, 'visa': 623, 'others': 624, 'traffic': 625, 'japanese': 626, 'drug': 627, 'often': 628, 'invest': 629, 'search': 630, 'matter': 631, 'follow': 632, 'avoid': 633, 'pros': 634, 'model': 635, 'physics': 636, 'star': 637, 'dark': 638, 'marks': 639, 'quality': 640, 'graduate': 641, 'tax': 642, 'advice': 643, 'presidential': 644, 'gate': 645, 'information': 646, 'advantages': 647, 'says': 648, '8': 649, 'fast': 650, '20': 651, 'fake': 652, 'group': 653, 'oil': 654, 'put': 655, 'deleted': 656, 'security': 657, 'cons': 658, 'dog': 659, 'easy': 660, 'green': 661, 'national': 662, 'general': 663, 'training': 664, 'solar': 665, 'found': 666, 'solve': 667, 'message': 668, 'said': 669, 'ias': 670, 'must': 671, 'add': 672, 'sydney': 673, 'messages': 674, 'team': 675, 'wife': 676, 'created': 677, 'everything': 678, 'those': 679, 'three': 680, 'changed': 681, 'pregnant': 682, 'happy': 683, 'bollywood': 684, 'contact': 685, 'problems': 686, 'leave': 687, 'blog': 688, 'full': 689, 'short': 690, 'network': 691, 'brain': 692, 'single': 693, 'lot': 694, 'marriage': 695, 'share': 696, 'few': 697, 'understand': 698, 'languages': 699, 'left': 700, 'price': 701, 'area': 702, 'uber': 703, 'obama': 704, 'germany': 705, 'profile': 706, 'tech': 707, 'check': 708, 'exactly': 709, 'british': 710, 'related': 711, 'married': 712, 'anything': 713, '2000': 714, 'away': 715, 'mass': 716, 'japan': 717, 'option': 718, 'determine': 719, 'loss': 720, 'ones': 721, 'south': 722, 'size': 723, 'americans': 724, 'field': 725, 'chances': 726, 'source': 727, 'across': 728, 'chemical': 729, 'words': 730, 'develop': 731, 'easily': 732, 'investment': 733, 'past': 734, 'laws': 735, 'numbers': 736, 'taking': 737, 'electrical': 738, 'charge': 739, 'personal': 740, 'studying': 741, 'scope': 742, 'boy': 743, 'address': 744, 'male': 745, 'offer': 746, 'colleges': 747, 'chance': 748, 'week': 749, 'blood': 750, 'provider': 751, 'pune': 752, 'party': 753, 'hyderabad': 754, 'force': 755, 'distance': 756, 'names': 757, 'effective': 758, 'balance': 759, 'sun': 760, 'special': 761, 'paper': 762, 'meet': 763, 'dogs': 764, 'file': 765, 'pro': 766, 'gay': 767, 'second': 768, 'suicide': 769, 'suitable': 770, 'developer': 771, 'religion': 772, 'known': 773, 'self': 774, 'moon': 775, 'master': 776, 'female': 777, 'b': 778, 'office': 779, 'pictures': 780, 'twitter': 781, 'eating': 782, 'explain': 783, 'army': 784, 'picture': 785, 'moment': 786, 'skin': 787, 'text': 788, 'basic': 789, 'penis': 790, 'samsung': 791, 'transfer': 792, 'enough': 793, 'bill': 794, 'decision': 795, 'iq': 796, 'started': 797, 'camera': 798, 'convert': 799, 'cold': 800, 'views': 801, 'jio': 802, 'drink': 803, 'healthy': 804, 'blue': 805, 'seo': 806, 'negative': 807, 'taken': 808, 'financial': 809, 'speak': 810, 'access': 811, 'term': 812, 'playing': 813, 'impact': 814, 'sim': 815, 'pass': 816, 'beautiful': 817, 'pc': 818, 'photos': 819, 'role': 820, 'calculate': 821, '100': 822, 'wo': 823, 'fix': 824, 'engine': 825, 'recruit': 826, 'disadvantages': 827, 'letter': 828, '12': 829, 'milk': 830, 'once': 831, 'please': 832, 'already': 833, 'professional': 834, 'paid': 835, 'structure': 836, 'least': 837, 'shows': 838, 'depression': 839, 'train': 840, 'gold': 841, 'coming': 842, 'low': 843, 'turn': 844, 'hindi': 845, 'crush': 846, 'improvement': 847, 'reasons': 848, 'exams': 849, 'reading': 850, '!': 851, 'fight': 852, 'desert': 853, 'political': 854, 'animals': 855, 'mix': 856, 'admission': 857, 't': 858, 'phd': 859, 'masturbation': 860, 'ban': 861, 'county': 862, 'page': 863, 'military': 864, 'topics': 865, 'resources': 866, 'else': 867, 'income': 868, 'pain': 869, 'late': 870, 'hollywood': 871, 'starting': 872, 'care': 873, '15': 874, 'feeling': 875, 'version': 876, 'set': 877, 'board': 878, 'won': 879, 'capital': 880, 'allowed': 881, 'store': 882, 'muslims': 883, 'heard': 884, 'greatest': 885, 'block': 886, 'battle': 887, 'easiest': 888, 'break': 889, 'rich': 890, 'systems': 891, 'studies': 892, 'thinking': 893, 'likes': 894, 'sound': 895, 'diet': 896, 'within': 897, 'officer': 898, 'news': 899, 'internship': 900, 'galaxy': 901, 'users': 902, 'red': 903, 'eyes': 904, 'knowledge': 905, 'treat': 906, 'necessary': 907, 'coding': 908, 'doctor': 909, 'stories': 910, 'microsoft': 911, 'cards': 912, 'europe': 913, 'though': 914, 'memory': 915, 'formula': 916, 'line': 917, 'percentage': 918, 'grads': 919, 'difficult': 920, 'fear': 921, 'screen': 922, 'banning': 923, 'classes': 924, 'character': 925, 'everyone': 926, 'husband': 927, 'ex': 928, 'father': 929, 'season': 930, 'crack': 931, 'far': 932, 'watching': 933, 'illegal': 934, 'chemistry': 935, 'dating': 936, '2015': 937, 'rehab': 938, 'prime': 939, 'eye': 940, 'prevent': 941, 'early': 942, 'significance': 943, 'smart': 944, 'written': 945, 'gift': 946, 'minimum': 947, 'opinion': 948, 'north': 949, 'user': 950, 'practice': 951, 'applications': 952, 'nuclear': 953, '30': 954, 'french': 955, 'wants': 956, 'young': 957, 'deep': 958, 'sent': 959, 'ios': 960, 'natural': 961, 'anime': 962, 'islam': 963, 'film': 964, 'neet': 965, 'c++': 966, 'football': 967, '/': 968, 'installation': 969, 'case': 970, 'positions': 971, 'method': 972, 'likely': 973, 'phones': 974, 'install': 975, 'positive': 976, 'narendra': 977, 'macbook': 978, 'track': 979, 'evidence': 980, 'simple': 981, 'smartphone': 982, 'economics': 983, 'bring': 984, '*': 985, 'heart': 986, 'alone': 987, 'try': 988, 'expect': 989, 'wars': 990, 'competitive': 991, 'mac': 992, 'smoking': 993, 'useful': 994, 'net': 995, 'schools': 996, 'modern': 997, 'presidency': 998, 'society': 999, 'upsc': 1000, 'inr': 1001, 'rupees': 1002, 'color': 1003, 'position': 1004, 'blocked': 1005, 'secret': 1006, 'amount': 1007, 'yet': 1008, 'towards': 1009, 'ssc': 1010, 'projects': 1011, 'straight': 1012, 'due': 1013, 'yes': 1014, 'panel': 1015, 'pokémon': 1016, 'little': 1017, 'details': 1018, 'prefer': 1019, 'inside': 1020, 'favourite': 1021, 'example': 1022, 'prove': 1023, 'since': 1024, 'million': 1025, 'mother': 1026, 'cut': 1027, 'couples': 1028, 'battery': 1029, 'policy': 1030, 'communication': 1031, 'gets': 1032, 'attack': 1033, 'highest': 1034, 'estate': 1035, 'visiting': 1036, 'western': 1037, 'according': 1038, 'cheap': 1039, 'court': 1040, 'clear': 1041, 'attractive': 1042, 'inpatient': 1043, 'cbse': 1044, 'brand': 1045, 'san': 1046, 'currently': 1047, 'engineers': 1048, 'tea': 1049, 'latest': 1050, 'connect': 1051, 'solution': 1052, 'cars': 1053, 'following': 1054, 'muslim': 1055, 'moral': 1056, 'buying': 1057, 'let': 1058, 'property': 1059, 'save': 1060, 'enjoy': 1061, 'reset': 1062, 'database': 1063, 'cancer': 1064, 'head': 1065, 'may': 1066, 'whole': 1067, 'politics': 1068, 'quickly': 1069, 'elections': 1070, 'teeth': 1071, 'spend': 1072, 'electronics': 1073, 'korean': 1074, 'return': 1075, 'corporate': 1076, 'files': 1077, 'speech': 1078, 'higher': 1079, 'hole': 1080, 'linux': 1081, 'ball': 1082, 'core': 1083, 'cities': 1084, 'kids': 1085, 'vs': 1086, 'photo': 1087, 'teacher': 1088, 'teach': 1089, 'rest': 1090, 'prospects': 1091, 'land': 1092, 'strategy': 1093, 'passport': 1094, 'works': 1095, 'departments': 1096, 'belly': 1097, 'accounts': 1098, 'content': 1099, 'gaming': 1100, 'tools': 1101, 'wifi': 1102, 'drinking': 1103, 'gas': 1104, 'present': 1105, '50': 1106, 'intelligence': 1107, 'allow': 1108, 'dollars': 1109, 'commit': 1110, 'opportunities': 1111, 'sbi': 1112, 'dangerous': 1113, 'technical': 1114, 'interest': 1115, 'limit': 1116, 'chennai': 1117, 'final': 1118, 'followers': 1119, 'harry': 1120, 'personality': 1121, 'strongest': 1122, 'german': 1123, 'running': 1124, 'bang': 1125, 'issue': 1126, 'preparing': 1127, 'advanced': 1128, 'singapore': 1129, 'ancient': 1130, 'b.tech': 1131, 'minister': 1132, 'birthday': 1133, 'procedure': 1134, 'league': 1135, 'rank': 1136, 'birth': 1137, 'dreams': 1138, 'feelings': 1139, 'hand': 1140, 'together': 1141, 'unusual': 1142, 'insurance': 1143, 'fastest': 1144, 'non': 1145, 'rather': 1146, 'middle': 1147, 'pressure': 1148, 'losing': 1149, 'zero': 1150, 'front': 1151, 'survive': 1152, 'cure': 1153, 'provide': 1154, 'hire': 1155, 'hindu': 1156, 'infinite': 1157, 'strong': 1158, 'device': 1159, 'anxiety': 1160, 'protein': 1161, '=': 1162, 'mark': 1163, '0': 1164, 'treatment': 1165, 'animal': 1166, 'planet': 1167, 'staff': 1168, 'adult': 1169, 'giving': 1170, 'clean': 1171, 'interested': 1172, 'baby': 1173, 'partner': 1174, 'sexual': 1175, 'status': 1176, 'remember': 1177, 'potter': 1178, 'programs': 1179, 'creative': 1180, 'growth': 1181, 'cse': 1182, 'demonetization': 1183, 'hacks': 1184, 'developed': 1185, '2014': 1186, 'cash': 1187, 'gre': 1188, 'heat': 1189, 'unmarried': 1190, 'key': 1191, 'might': 1192, 'consider': 1193, 'comes': 1194, 'poor': 1195, 'okay': 1196, '+': 1197, 'clothes': 1198, 'saying': 1199, 'psychology': 1200, 'weed': 1201, 'recommend': 1202, 'steps': 1203, 'elected': 1204, 'super': 1205, 'trying': 1206, 'mathematics': 1207, 'safety': 1208, 'completing': 1209, 'art': 1210, 'specific': 1211, 'everyday': 1212, 'server': 1213, 'told': 1214, 'complete': 1215, 'olympics': 1216, 'php': 1217, 'lower': 1218, 'yahoo': 1219, 'suggest': 1220, 'cheating': 1221, 'beginner': 1222, 'perfect': 1223, 'asking': 1224, 'importance': 1225, '9': 1226, 'manager': 1227, 'harvard': 1228, 'location': 1229, 'racist': 1230, 'image': 1231, 'exercise': 1232, 'needs': 1233, 'describe': 1234, 'via': 1235, 'proof': 1236, 'coffee': 1237, 'uses': 1238, '#': 1239, 'bible': 1240, 'morning': 1241, 'speaking': 1242, 'act': 1243, 'determined': 1244, 'approach': 1245, 'weeks': 1246, 'work-life': 1247, 'javascript': 1248, 'startups': 1249, 'voice': 1250, 'expensive': 1251, 'almost': 1252, 'whether': 1253, 'grade': 1254, 'needing': 1255, 'placements': 1256, 'episode': 1257, 'below': 1258, 'plus': 1259, 'gravity': 1260, 'manage': 1261, 'themselves': 1262, 'third': 1263, 'temperature': 1264, 'driving': 1265, 'close': 1266, 'acne': 1267, 'building': 1268, 'intelligent': 1269, 'pokemon': 1270, 'success': 1271, 'characteristics': 1272, 'range': 1273, 'means': 1274, 'ice': 1275, 'listen': 1276, 'large': 1277, 'trading': 1278, 'commercial': 1279, 'forgot': 1280, 'applying': 1281, 'switch': 1282, 'stars': 1283, 'dead': 1284, 'whom': 1285, 'among': 1286, 'mit': 1287, 'solutions': 1288, 'abroad': 1289, 'benefit': 1290, 'multiple': 1291, 'tamil': 1292, 'bike': 1293, 'hold': 1294, 'thoughts': 1295, 'messenger': 1296, 'license': 1297, 'moving': 1298, 'root': 1299, 'spanish': 1300, 'meant': 1301, 'm': 1302, 'invented': 1303, 'sea': 1304, 'equation': 1305, 'global': 1306, 'hands': 1307, 'debate': 1308, 'round': 1309, 'step': 1310, 'longer': 1311, 'disorder': 1312, 'older': 1313, 'kerala': 1314, 'cs': 1315, 'masters': 1316, 'cricket': 1317, 'israel': 1318, '12th': 1319, 'ram': 1320, 'beginners': 1321, 'update': 1322, 'dc': 1323, 'goes': 1324, 'topic': 1325, 'pan': 1326, 'total': 1327, 'lead': 1328, 'marry': 1329, 'hot': 1330, 'potential': 1331, 'viewed': 1332, 'boys': 1333, 'designer': 1334, 'citizen': 1335, 'mbbs': 1336, 'usb': 1337, 'gst': 1338, 'fire': 1339, ';': 1340, 'material': 1341, 'medicine': 1342, 'surgical': 1343, 'tcs': 1344, 'khan': 1345, 'hit': 1346, 'id': 1347, 'pounds': 1348, 'plant': 1349, 'cool': 1350, 'here': 1351, 'completely': 1352, 'harassment': 1353, 'mains': 1354, 'king': 1355, 'fit': 1356, 'stand': 1357, 'resume': 1358, 'philosophy': 1359, 'sales': 1360, 'permanent': 1361, 'writer': 1362, 'acid': 1363, 'piece': 1364, '16': 1365, 'expected': 1366, 'corruption': 1367, 'cats': 1368, 'analytics': 1369, 'sometimes': 1370, 'outside': 1371, 'thrones': 1372, 'earthquake': 1373, 'attracted': 1374, 'talking': 1375, 'minutes': 1376, 'wish': 1377, 'shopping': 1378, 'chicken': 1379, 'sleeping': 1380, 'needed': 1381, 'decide': 1382, 'aliens': 1383, 'wordpress': 1384, 'cultural': 1385, 'crime': 1386, 'error': 1387, 'accept': 1388, 'york': 1389, 'n': 1390, 'certificate': 1391, 'subject': 1392, 'meth': 1393, 'francisco': 1394, 'four': 1395, 'recovery': 1396, 'european': 1397, 'performance': 1398, 'powerful': 1399, 'pm': 1400, 'fund': 1401, 'cover': 1402, 'pursue': 1403, 'continue': 1404, 'five': 1405, 'phrase': 1406, 'climate': 1407, 'planning': 1408, 'graduation': 1409, 'gym': 1410, 'gadgets': 1411, 'maximum': 1412, 'turkey': 1413, 'proposed': 1414, 'experiences': 1415, 'symptoms': 1416, 'replace': 1417, 'parts': 1418, 'wall': 1419, 'basis': 1420, 'thought': 1421, 'species': 1422, 'selling': 1423, 'becomes': 1424, 'original': 1425, 'east': 1426, 'branch': 1427, 'and/or': 1428, 'comments': 1429, 'sports': 1430, 'mail': 1431, 'budget': 1432, 'active': 1433, 'room': 1434, 'equal': 1435, 'table': 1436, 'nothing': 1437, 'recently': 1438, 'gb': 1439, 'asian': 1440, 'mexico': 1441, 'later': 1442, 'drop': 1443, 'economic': 1444, '13': 1445, 'ok': 1446, 'count': 1447, 'events': 1448, 'appear': 1449, 'korea': 1450, 'chat': 1451, 'reliable': 1452, 'generally': 1453, 'features': 1454, 'hitler': 1455, 'local': 1456, 'typical': 1457, 'driver': 1458, 'led': 1459, 'finance': 1460, 'actor': 1461, 'definition': 1462, 'romantic': 1463, '3d': 1464, 'glass': 1465, 'paypal': 1466, 'essay': 1467, 'devices': 1468, 'regret': 1469, 'tourist': 1470, 'requirements': 1471, 'factors': 1472, 'pilot': 1473, 'fact': 1474, 'africa': 1475, 'reservation': 1476, 'reality': 1477, 'campus': 1478, 'sign': 1479, 'population': 1480, 'unknown': 1481, 'profit': 1482, 'colors': 1483, 'fun': 1484, 'methods': 1485, 'package': 1486, 'feed': 1487, 'france': 1488, 'nose': 1489, 'player': 1490, 'testing': 1491, 'reach': 1492, 'mind-blowing': 1493, 'dry': 1494, 'afraid': 1495, 'candidate': 1496, 'object': 1497, 'signs': 1498, 'legally': 1499, 'gps': 1500, '4g': 1501, '25': 1502, 'physical': 1503, 'forget': 1504, 'guitar': 1505, 'rates': 1506, 'summer': 1507, 'voltage': 1508, 'weakest': 1509, 'native': 1510, 'api': 1511, 'flat': 1512, 'burn': 1513, 'flight': 1514, 'receive': 1515, 'accepted': 1516, 'businesses': 1517, 'issues': 1518, 'release': 1519, 'chrome': 1520, 'customer': 1521, 'marijuana': 1522, 'regular': 1523, 'radio': 1524, 'hacking': 1525, 'marked': 1526, 'domain': 1527, 'funniest': 1528, 'spotify': 1529, 'along': 1530, 'trade': 1531, 'lives': 1532, 'trust': 1533, 'freedom': 1534, 'dna': 1535, 'calls': 1536, 'programmer': 1537, 'scientific': 1538, 'maths': 1539, 'weather': 1540, 'certification': 1541, 'embarrassing': 1542, 'son': 1543, 'ibps': 1544, 'navy': 1545, 'direct': 1546, 'drivers': 1547, 'visitors': 1548, 'restaurants': 1549, 'half': 1550, 'becoming': 1551, 'cgl': 1552, 'institutes': 1553, 'origin': 1554, 'temperatures': 1555, 'suggestions': 1556, 'planets': 1557, 'os': 1558, 'rbi': 1559, 'hiring': 1560, 'ideal': 1561, 'london': 1562, 'contrast': 1563, 'christian': 1564, 'stream': 1565, 'analysis': 1566, 'rights': 1567, 'effectively': 1568, 'convince': 1569, 'various': 1570, 'came': 1571, 'produce': 1572, 'aspects': 1573, 'somebody': 1574, 'production': 1575, 'west': 1576, 'jesus': 1577, 'flipkart': 1578, 'platform': 1579, 'quantum': 1580, 'electric': 1581, 'slow': 1582, 'notice': 1583, 'hurt': 1584, 'netflix': 1585, 'touch': 1586, '40': 1587, 'characters': 1588, 'born': 1589, 'forces': 1590, 'tool': 1591, 'construction': 1592, 'caused': 1593, 'ipad': 1594, 'entire': 1595, 'smell': 1596, 'easier': 1597, 'immigration': 1598, 'alive': 1599, 'results': 1600, 'upload': 1601, 'campaign': 1602, 'mental': 1603, 'miss': 1604, 'funds': 1605, 'quit': 1606, 'architecture': 1607, 'literature': 1608, '..': 1609, 'activities': 1610, 'kashmir': 1611, 'goa': 1612, 'anger': 1613, 'amazing': 1614, 'dubai': 1615, 'resolution': 1616, 'overrated': 1617, 'precautions': 1618, 'handling': 1619, 'points': 1620, 'shares': 1621, 'changing': 1622, 'brands': 1623, 'band': 1624, 'blogs': 1625, 'brazil': 1626, 'surgery': 1627, 'existence': 1628, 'po': 1629, 'meat': 1630, 'stupid': 1631, 'restaurant': 1632, 'cheapest': 1633, 'electricity': 1634, 'banned': 1635, 'demand': 1636, 'exists': 1637, '11': 1638, 'showing': 1639, 'christmas': 1640, 'jews': 1641, 'isis': 1642, 'external': 1643, 'torrent': 1644, 'attend': 1645, 'recent': 1646, 'six': 1647, 'evil': 1648, 'road': 1649, 'operating': 1650, 'liquid': 1651, 'pregnancy': 1652, 'community': 1653, 'russian': 1654, 'steel': 1655, 'sources': 1656, 'tricks': 1657, 'historical': 1658, 'went': 1659, 'bit': 1660, 'religious': 1661, 'bored': 1662, 'shoes': 1663, 'comment': 1664, 'received': 1665, 'experienced': 1666, 'double': 1667, 'cutoff': 1668, 'tata': 1669, 'bitcoin': 1670, 'lesser': 1671, 'reaction': 1672, '18': 1673, 'sure': 1674, 'cream': 1675, 'equivalent': 1676, 'weird': 1677, 'metal': 1678, 'tinder': 1679, 'scientist': 1680, 'employee': 1681, 'connection': 1682, 'auto': 1683, 'log': 1684, 'leader': 1685, 'v': 1686, 'relations': 1687, 'banking': 1688, 'tried': 1689, 'activity': 1690, 'cloud': 1691, 'nra': 1692, 'wake': 1693, 'situation': 1694, 'vacuum': 1695, 'purchase': 1696, 'structures': 1697, 'algorithms': 1698, 'cells': 1699, 'excel': 1700, 'analyst': 1701, 'stack': 1702, 'democracy': 1703, 'above': 1704, 'ethical': 1705, 'largest': 1706, 'style': 1707, 'investors': 1708, 'nit': 1709, 'built': 1710, 'nice': 1711, 'liberal': 1712, 'ticket': 1713, 'strike': 1714, 'philippines': 1715, '14': 1716, 'conversation': 1717, 'addiction': 1718, 'introvert': 1719, 'naturally': 1720, 'permanently': 1721, 'answered': 1722, 'suddenly': 1723, 'ring': 1724, 'union': 1725, 'novel': 1726, 'forward': 1727, 'films': 1728, 'raise': 1729, '2nd': 1730, 'functions': 1731, 'gandhi': 1732, 'developers': 1733, 'celebrity': 1734, 'supreme': 1735, 'pursuing': 1736, 'seat': 1737, 'shotguns': 1738, 'nasa': 1739, 'hardest': 1740, 'stopped': 1741, 'angry': 1742, 'kejriwal': 1743, 'shall': 1744, '10th': 1745, 'event': 1746, 'sap': 1747, 'hacked': 1748, 'background': 1749, '60': 1750, 'channel': 1751, 'shop': 1752, 'logic': 1753, 'anybody': 1754, 'sister': 1755, 'concept': 1756, 'designing': 1757, 'carry': 1758, 'century': 1759, 'handle': 1760, 'malaysia': 1761, 'iim': 1762, 'taste': 1763, 'cycle': 1764, 'dragon': 1765, 'previous': 1766, 'phase': 1767, 'd': 1768, 'fresher': 1769, 'tall': 1770, 'html': 1771, 'stanford': 1772, 'report': 1773, 'salt': 1774, '...': 1775, 'hp': 1776, 'lord': 1777, 'gpa': 1778, 'bay': 1779, 'cancel': 1780, 'dying': 1781, 'foods': 1782, '1st': 1783, 'edit': 1784, 'entry': 1785, 'exchange': 1786, 'anymore': 1787, 'nature': 1788, 'urine': 1789, 'placement': 1790, 'respond': 1791, 'downloading': 1792, 'actual': 1793, 'hacker': 1794, 'feet': 1795, 'fail': 1796, 'serve': 1797, 'networks': 1798, 'ielts': 1799, 'upgrade': 1800, 'asia': 1801, 'itself': 1802, 'define': 1803, 'identify': 1804, 'occur': 1805, 'biology': 1806, 'actors': 1807, 'length': 1808, 'unique': 1809, 'fish': 1810, 'rock': 1811, 'seem': 1812, 'gifts': 1813, 'ip': 1814, 'muscle': 1815, 'industrial': 1816, 'novels': 1817, 'funding': 1818, 'copy': 1819, 'drawing': 1820, 'sort': 1821, 'laptops': 1822, 'standard': 1823, 'velocity': 1824, 'link': 1825, 'river': 1826, 'banks': 1827, 'skill': 1828, 'rent': 1829, 'undergraduate': 1830, 'levels': 1831, 'sense': 1832, 'fashion': 1833, 'bond': 1834, 'loan': 1835, 'ago': 1836, 'reviews': 1837, 'sugar': 1838, '21': 1839, 'members': 1840, 'calories': 1841, 'ad': 1842, 'scratch': 1843, 'coast': 1844, 'request': 1845, 'canadian': 1846, 'title': 1847, 'imported': 1848, 'growing': 1849, 'especially': 1850, 'presence': 1851, 'street': 1852, '3rd': 1853, 'washington': 1854, 'ground': 1855, 'images': 1856, 'weirdest': 1857, 'trip': 1858, 'dhoni': 1859, '<': 1860, 'independent': 1861, 'pick': 1862, 'became': 1863, 'failed': 1864, 'tree': 1865, 'hour': 1866, 'chess': 1867, 'serious': 1868, 'repair': 1869, 'rule': 1870, 'chip': 1871, '3g': 1872, 'engines': 1873, 'mistake': 1874, 'vehicle': 1875, 'load': 1876, 'rice': 1877, 'obsessed': 1878, 'strikes': 1879, 'dual': 1880, 'tired': 1881, 'taxes': 1882, 'kvpy': 1883, 'usually': 1884, 'density': 1885, 'waves': 1886, 'met': 1887, 'virtual': 1888, '24': 1889, 'weapons': 1890, 'learned': 1891, 'reply': 1892, 'sad': 1893, 'advantage': 1894, 'cry': 1895, 'alternative': 1896, 'scale': 1897, 'kiss': 1898, 'channels': 1899, 'sentences': 1900, 'developing': 1901, 'require': 1902, 'peace': 1903, 'relative': 1904, 'graphic': 1905, 'print': 1906, 'virus': 1907, 'display': 1908, 'ratio': 1909, 'protect': 1910, 'brown': 1911, 'horror': 1912, 'caste': 1913, 'involved': 1914, 'sing': 1915, '6s': 1916, 'vagina': 1917, 'workout': 1918, 'format': 1919, 'affordable': 1920, 'wins': 1921, 'took': 1922, 'ceo': 1923, 'achieve': 1924, 'legs': 1925, 'scored': 1926, 'wine': 1927, 'eu': 1928, 'vision': 1929, 'race': 1930, 'pure': 1931, 'box': 1932, 'plastic': 1933, 'tesla': 1934, 'emotions': 1935, 'valley': 1936, 'gives': 1937, 'arts': 1938, 'base': 1939, 'stomach': 1940, 'l': 1941, 'stage': 1942, 'silicon': 1943, 'automatically': 1944, 'soon': 1945, 'directly': 1946, 'e': 1947, 'killed': 1948, 'entrance': 1949, 'generation': 1950, 'editing': 1951, 'stress': 1952, 'fee': 1953, 'gmat': 1954, 'central': 1955, 'ubuntu': 1956, 'puppy': 1957, 'located': 1958, 'relationships': 1959, 'brother': 1960, 'pdf': 1961, 'paying': 1962, 'square': 1963, 'hydrogen': 1964, 'cook': 1965, 'figure': 1966, 'entrepreneur': 1967, 'breaking': 1968, 'executive': 1969, 'periods': 1970, 'respect': 1971, 'profitable': 1972, 'walking': 1973, 'wait': 1974, 'surface': 1975, 'linkedin': 1976, 'masturbate': 1977, 'cgpa': 1978, 'wedding': 1979, 'candy': 1980, 'damage': 1981, 'scientifically': 1982, 'chocolate': 1983, 'boss': 1984, 'theories': 1985, 'sold': 1986, 'hear': 1987, 'anal': 1988, 'celebrate': 1989, 'individual': 1990, 'investing': 1991, 'transaction': 1992, 'taller': 1993, '17': 1994, 'result': 1995, 'similarities': 1996, 'physically': 1997, 'certain': 1998, 'circle': 1999, 'browser': 2000, 'bigger': 2001, 'truth': 2002, 'wearing': 2003, 'measure': 2004, 'valid': 2005, 'massage': 2006, 'texas': 2007, 'quick': 2008, 'creating': 2009, 'knowing': 2010, 'react': 2011, 'funny': 2012, 'focus': 2013, 'falling': 2014, 'zone': 2015, 'limited': 2016, 'clothing': 2017, 'commerce': 2018, 'joining': 2019, 'frequency': 2020, 'empire': 2021, 'logo': 2022, 'begin': 2023, 'expert': 2024, 'reliance': 2025, 'choice': 2026, 'mom': 2027, 'hotels': 2028, 'royal': 2029, 'materials': 2030, 'knew': 2031, '‘': 2032, 'cope': 2033, 'bacteria': 2034, 'symbol': 2035, 'bar': 2036, 'christianity': 2037, 'habits': 2038, 'failure': 2039, 'carbon': 2040, 'spain': 2041, 'asks': 2042, 'aircraft': 2043, 'bjp': 2044, 'confidence': 2045, 'sector': 2046, 'democratic': 2047, 'technologies': 2048, 'supporters': 2049, 'tickets': 2050, 'affected': 2051, 'scientists': 2052, 'firm': 2053, 'pizza': 2054, 'equity': 2055, 'academic': 2056, 'manufacturing': 2057, 'loves': 2058, 'raw': 2059, 'pi': 2060, 'letters': 2061, 'e-commerce': 2062, 'evolution': 2063, '32': 2064, 'disease': 2065, 'sport': 2066, 'substitute': 2067, 'moto': 2068, 'removed': 2069, 'offline': 2070, 'probability': 2071, 'constitution': 2072, 'promote': 2073, 'supposed': 2074, 'computers': 2075, 'african': 2076, 'alternatives': 2077, 'pakistani': 2078, 'eggs': 2079, 'iron': 2080, 'lie': 2081, 'artificial': 2082, 'played': 2083, 'goods': 2084, 'harmful': 2085, 'airlines': 2086, 'button': 2087, 'r': 2088, 'ece': 2089, 'fly': 2090, 'motion': 2091, 'networking': 2092, 'offers': 2093, 'mars': 2094, 'selected': 2095, 'regarding': 2096, 'smoke': 2097, 'powder': 2098, 'loose': 2099, 'prison': 2100, 'oven': 2101, 'catch': 2102, 'host': 2103, 'particle': 2104, 'guide': 2105, 'couple': 2106, 'emotionally': 2107, 'biotechnology': 2108, 'diesel': 2109, 'batman': 2110, 'apart': 2111, 'dollar': 2112, 'broken': 2113, 'feature': 2114, 'formed': 2115, 'mr.': 2116, 'taffy': 2117, 'enter': 2118, 'lessons': 2119, 'xbox': 2120, 'baking': 2121, 'techniques': 2122, 'changes': 2123, 'intel': 2124, 'statement': 2125, 'spoken': 2126, 'behavior': 2127, 'babies': 2128, 'humanity': 2129, 'prices': 2130, 'cases': 2131, 'neural': 2132, 'december': 2133, 'stocks': 2134, 'truly': 2135, 'dress': 2136, 'arvind': 2137, 'twice': 2138, 'processor': 2139, 'revenue': 2140, 'pattern': 2141, 'properties': 2142, 'objects': 2143, 'decrease': 2144, 'academy': 2145, 'motivated': 2146, 'unable': 2147, 'broke': 2148, 'efficient': 2149, 'kolkata': 2150, 'ship': 2151, 'whose': 2152, 'republican': 2153, 'route': 2154, 'syria': 2155, 'rio': 2156, 'gun': 2157, 'contacts': 2158, 'queen': 2159, 'fees': 2160, 'explanation': 2161, 'faux': 2162, 'vs.': 2163, 'hide': 2164, 'sql': 2165, 'colour': 2166, 'worse': 2167, 'conflict': 2168, 'embedded': 2169, 'habit': 2170, 'nation': 2171, 'owner': 2172, 'sat': 2173, 'mentally': 2174, 'citizens': 2175, 'officers': 2176, 'depressed': 2177, 'un': 2178, 'shape': 2179, 'greek': 2180, 'conduct': 2181, 'documents': 2182, 'binary': 2183, 'billion': 2184, 'flash': 2185, 'publish': 2186, 'travelling': 2187, 'beer': 2188, 'motor': 2189, 'rules': 2190, 'gap': 2191, 'maintain': 2192, 'players': 2193, 'divorce': 2194, 'teaching': 2195, 'syllabus': 2196, 'infosys': 2197, 'professors': 2198, 'payment': 2199, 'winning': 2200, 'oral': 2201, 'martial': 2202, 'blind': 2203, 'remedies': 2204, 'debit': 2205, 'attract': 2206, 'initial': 2207, 'ten': 2208, 'theme': 2209, 'floor': 2210, 'died': 2211, 'flow': 2212, 'earning': 2213, 'dad': 2214, 'visitor': 2215, 'seeing': 2216, 'passed': 2217, 'criteria': 2218, 'bernie': 2219, 'sanders': 2220, 'wi-fi': 2221, 'plane': 2222, 'indonesia': 2223, 'influence': 2224, 'path': 2225, 'hr': 2226, 'doctors': 2227, 'tier': 2228, 'organic': 2229, 'boost': 2230, 'associated': 2231, 'completed': 2232, 'port': 2233, 'pas': 2234, 'episodes': 2235, 'conspiracy': 2236, 'storage': 2237, 'antivirus': 2238, 'daughter': 2239, 'fighting': 2240, 'furniture': 2241, 'express': 2242, 'incident': 2243, 'false': 2244, 'wash': 2245, 'advertising': 2246, 'chicago': 2247, 'debt': 2248, 'terrorist': 2249, 'launch': 2250, 'fine': 2251, 'either': 2252, 'statistics': 2253, '>': 2254, 'temple': 2255, 'category': 2256, 'rise': 2257, 'until': 2258, 'club': 2259, 'released': 2260, 'watched': 2261, 'fuel': 2262, 'responsible': 2263, 'jealous': 2264, 'citizenship': 2265, 'station': 2266, 'painting': 2267, 'printing': 2268, 'republic': 2269, 'chief': 2270, 'register': 2271, 'britain': 2272, 'fields': 2273, 'ads': 2274, 'looks': 2275, 'spouse': 2276, 'described': 2277, 'attractions': 2278, 'iran': 2279, 'piano': 2280, 'considering': 2281, 'younger': 2282, 'treated': 2283, 'particles': 2284, 'mutual': 2285, 'barack': 2286, 'opposite': 2287, 'shot': 2288, 'tattoo': 2289, 'models': 2290, 'records': 2291, 'consequences': 2292, 'elements': 2293, 'goals': 2294, 'exercises': 2295, 'takes': 2296, '2013': 2297, 'assembly': 2298, 'invited': 2299, 'constant': 2300, 'vietnam': 2301, 'airport': 2302, 'named': 2303, 'library': 2304, 'areas': 2305, 'himself': 2306, 'bangladesh': 2307, 'redmi': 2308, 'john': 2309, 'australian': 2310, 'delivery': 2311, 'record': 2312, 'posts': 2313, 'professor': 2314, 'ear': 2315, \"'ll\": 2316, 'michael': 2317, 'leaving': 2318, 'despite': 2319, 'eve': 2320, 'visual': 2321, 'extra': 2322, 'classic': 2323, 'unlimited': 2324, 'output': 2325, 'vegetarian': 2326, 'percentile': 2327, 'curb': 2328, 'u.s': 2329, 'deposit': 2330, 'match': 2331, 'plants': 2332, 'plans': 2333, 'seats': 2334, 'govt': 2335, 'george': 2336, 'throat': 2337, 'masturbating': 2338, 'holes': 2339, 'bsc': 2340, 'heavy': 2341, 'items': 2342, 'saved': 2343, 'digits': 2344, 'groups': 2345, 'successfully': 2346, 'artist': 2347, 'btech': 2348, 'desktop': 2349, 'bought': 2350, 'vector': 2351, 'defeat': 2352, 'conservative': 2353, 'goal': 2354, 'oxygen': 2355, 'member': 2356, 'celebrities': 2357, 'astrology': 2358, 'fiction': 2359, 'subtitles': 2360, 'copyright': 2361, '@': 2362, 'monthly': 2363, 'relation': 2364, 'psychological': 2365, 'egg': 2366, 'calculus': 2367, 'cheated': 2368, 'he/she': 2369, 'prank': 2370, 'saltwater': 2371, 'iii': 2372, 'happening': 2373, 'nepal': 2374, 'tests': 2375, 'unlock': 2376, 'agree': 2377, 'saudi': 2378, 'leaders': 2379, 'nadu': 2380, 'algorithm': 2381, 'extremely': 2382, 'gravitational': 2383, 'speakers': 2384, 'knows': 2385, 'hill-station': 2386, 'traits': 2387, 'alien': 2388, 'jokes': 2389, 'existing': 2390, 'teenager': 2391, 'quotes': 2392, 'drugs': 2393, 'answering': 2394, 'icloud': 2395, 'hindus': 2396, 'advance': 2397, 'italy': 2398, 'shown': 2399, 'tracking': 2400, 'painful': 2401, 'november': 2402, 'widely': 2403, 'bus': 2404, 'grammar': 2405, 'refund': 2406, 'scholarship': 2407, 'huge': 2408, 'firms': 2409, 'accenture': 2410, 'consultant': 2411, 'oracle': 2412, 'distribution': 2413, 'rejected': 2414, 'paint': 2415, 'components': 2416, 'steve': 2417, 'ii': 2418, 'beat': 2419, 'universal': 2420, 'yellow': 2421, 'vice': 2422, 'i.e': 2423, 'measured': 2424, 'section': 2425, 'electron': 2426, 'tank': 2427, 'primary': 2428, 'internal': 2429, 'upcoming': 2430, 'msc': 2431, 'fresh': 2432, 'blowing': 2433, 'claim': 2434, 'james': 2435, 'divided': 2436, 'la': 2437, 'dance': 2438, 'associate': 2439, 'eligible': 2440, 'hinduism': 2441, 'lack': 2442, 'understanding': 2443, 'industries': 2444, 'speaker': 2445, 'candidates': 2446, 'lee': 2447, 'operation': 2448, 'foreigners': 2449, 'machines': 2450, 'dishes': 2451, 'spots': 2452, 'processing': 2453, 'cameras': 2454, 'magic': 2455, 'hong': 2456, 'boring': 2457, 'accidentally': 2458, 'genuine': 2459, 'capacity': 2460, 'introduced': 2461, 'jackson': 2462, 'gender': 2463, 'stores': 2464, 'expanding': 2465, 'florida': 2466, '^': 2467, '200': 2468, 'paris': 2469, 'articles': 2470, 'psychopath': 2471, 'border': 2472, 'fair': 2473, 'identity': 2474, 'childhood': 2475, 'medals': 2476, 'honda': 2477, 'introduce': 2478, 'practical': 2479, 'returns': 2480, 'muscles': 2481, 'streaming': 2482, 'penalty': 2483, 'revolution': 2484, 'client': 2485, 'bomb': 2486, 'sri': 2487, 'caught': 2488, 'ac': 2489, 'fitness': 2490, 'ps4': 2491, 'newton': 2492, 'orgasm': 2493, 'sum': 2494, 'bits': 2495, '300': 2496, 'till': 2497, 'savings': 2498, 'degrees': 2499, 'hearing': 2500, 'realize': 2501, 'atheists': 2502, 'applied': 2503, 'sahara': 2504, 'walk': 2505, 'placed': 2506, 'naruto': 2507, 'disabled': 2508, 'faith': 2509, 'roman': 2510, 'random': 2511, 'accurate': 2512, 'eventually': 2513, 'article': 2514, 'pet': 2515, 'former': 2516, 'separate': 2517, 'figures': 2518, 'graphics': 2519, 'poker': 2520, 'assistant': 2521, 'wikipedia': 2522, 'shy': 2523, 'keeps': 2524, 'longest': 2525, 'nfl': 2526, 'lies': 2527, 'crazy': 2528, 'bullet': 2529, 'cambodia': 2530, 'reference': 2531, 'passive': 2532, 'virginity': 2533, 'nobody': 2534, 'joke': 2535, 'robert': 2536, 'admissions': 2537, 'waiting': 2538, 'innovations': 2539, '5s': 2540, 'actress': 2541, 'interviews': 2542, 'foot': 2543, 'lines': 2544, 'atm': 2545, 'medium': 2546, 'cup': 2547, 'flying': 2548, 'agent': 2549, 'inches': 2550, 'values': 2551, 'fantasy': 2552, 'shift': 2553, 'framework': 2554, 'custom': 2555, 'kong': 2556, 'lock': 2557, 'jet': 2558, 'boards': 2559, 'fill': 2560, 'pr': 2561, 'sweden': 2562, 'disable': 2563, 'impress': 2564, 'latin': 2565, 'dont': 2566, 'glasses': 2567, 'h1b': 2568, 'vocabulary': 2569, 'itunes': 2570, 'republicans': 2571, 'mini': 2572, 'investor': 2573, 'chest': 2574, 'translation': 2575, 'september': 2576, 'turned': 2577, 'centre': 2578, 'hope': 2579, 'volume': 2580, 'breakfast': 2581, 'lesbian': 2582, 'accounting': 2583, 'england': 2584, 'k': 2585, 'karnataka': 2586, 'extrovert': 2587, 'editor': 2588, 'adobe': 2589, 'metals': 2590, 'liberals': 2591, 'arizona': 2592, 'unit': 2593, 'script': 2594, 'comfort': 2595, 'proper': 2596, 'y': 2597, 'refer': 2598, 'besides': 2599, 'minute': 2600, 'router': 2601, 'sa': 2602, 'withdraw': 2603, 'lift': 2604, 'ocean': 2605, 'angle': 2606, 'sexually': 2607, 'font': 2608, 'possibility': 2609, 'cheat': 2610, 'disney': 2611, 'legit': 2612, 'johnson': 2613, 'papers': 2614, 'jaipur': 2615, 'thank': 2616, 'infinity': 2617, '90': 2618, 'batch': 2619, 'hardware': 2620, 'particular': 2621, 'killing': 2622, 'administration': 2623, 'muhammad': 2624, 'facial': 2625, 'signal': 2626, 'saw': 2627, 'registered': 2628, 'particularly': 2629, 'edge': 2630, 'constantly': 2631, 'valuable': 2632, 'programmers': 2633, 'electoral': 2634, 'pic': 2635, 'challenges': 2636, 'gone': 2637, 'credits': 2638, 'concepts': 2639, 'microwave': 2640, 'immediately': 2641, 'turning': 2642, 'mistakes': 2643, 'legends': 2644, 'registration': 2645, 'fluently': 2646, 'quickbooks': 2647, 'richest': 2648, 'demonetisation': 2649, 'gdp': 2650, 'gods': 2651, 'fixed': 2652, 'de': 2653, 'billionaire': 2654, 'willing': 2655, 'headphones': 2656, 'qualities': 2657, 'suffering': 2658, 'suck': 2659, 'upon': 2660, 'concentration': 2661, 'snow': 2662, 'mexican': 2663, 'motivation': 2664, 'beneficial': 2665, 'island': 2666, 'escape': 2667, 'fighter': 2668, 'tested': 2669, 'presidents': 2670, 'soda': 2671, 'underrated': 2672, 'putin': 2673, 'egypt': 2674, 'friendly': 2675, 'wireless': 2676, 'arnab': 2677, 'mathematical': 2678, 'implement': 2679, 'kg': 2680, 'bed': 2681, 'computing': 2682, 'graph': 2683, 'scripting': 2684, 'supply': 2685, 'deactivate': 2686, 'scam': 2687, 'january': 2688, 'proven': 2689, 'sociopath': 2690, 'freshman': 2691, 'angel': 2692, 'meditation': 2693, 'audio': 2694, 'traditional': 2695, 'wanted': 2696, 'tooth': 2697, 'pull': 2698, 'quote': 2699, 'import': 2700, 'export': 2701, 'voting': 2702, 'spot': 2703, 'trouble': 2704, 'lyrics': 2705, 'fever': 2706, 'unexpected': 2707, 'first-time': 2708, 'clients': 2709, 'mode': 2710, 'earphones': 2711, 'listening': 2712, 'appropriate': 2713, 'circuit': 2714, 'complex': 2715, \"'d\": 2716, 'translate': 2717, 'ui': 2718, 'routine': 2719, 'ipod': 2720, 'teachers': 2721, 'maps': 2722, 'diploma': 2723, 'na': 2724, 'hero': 2725, 'contain': 2726, 'secrets': 2727, 'motivate': 2728, 'sherlock': 2729, 'exact': 2730, 'nations': 2731, 'sd': 2732, 'sky': 2733, 'forms': 2734, 'secure': 2735, 'lakh': 2736, 'logical': 2737, 'monitor': 2738, 'mirror': 2739, 'opening': 2740, 'lewis': 2741, 'empty': 2742, 'arabia': 2743, 'mixed': 2744, 'ignore': 2745, 'iits': 2746, 'poem': 2747, 'lazy': 2748, 'washing': 2749, 'ies': 2750, 'venture': 2751, 'z': 2752, 'islamic': 2753, 'leather': 2754, 'pretty': 2755, 'differently': 2756, 'include': 2757, 'bachelor': 2758, 'soldiers': 2759, 'provides': 2760, 'strength': 2761, 'quorans': 2762, 'powers': 2763, 'consumer': 2764, 'operations': 2765, 'calculated': 2766, 'investments': 2767, 'craziest': 2768, 'bonds': 2769, 'loved': 2770, 'except': 2771, 'copper': 2772, 'posting': 2773, 'privacy': 2774, 'helpful': 2775, 'christians': 2776, '11th': 2777, 'strategies': 2778, 'petrol': 2779, 'canon': 2780, 'saving': 2781, 'fluent': 2782, 'automation': 2783, 'shell': 2784, 'worried': 2785, 'opinions': 2786, 'cotton': 2787, 'chart': 2788, '15k': 2789, 'triangle': 2790, 'manipal': 2791, 'condition': 2792, 'scenes': 2793, 'charged': 2794, 'attempt': 2795, 'crop': 2796, 'circles': 2797, 'pen': 2798, 'goswami': 2799, 'anthem': 2800, 'ability': 2801, 'psychopaths': 2802, 'provided': 2803, 'movement': 2804, 'soul': 2805, 'principle': 2806, 'contract': 2807, 'chain': 2808, 'gave': 2809, 'retrieve': 2810, 'reasonable': 2811, 'delivered': 2812, 'draw': 2813, 'ga': 2814, 'scared': 2815, 'brexit': 2816, 'mountain': 2817, 'meal': 2818, 'magnetic': 2819, 'integrate': 2820, 'soft': 2821, 'decent': 2822, 'response': 2823, 'shower': 2824, 'nyc': 2825, 'cuisine': 2826, 'coolest': 2827, 'ivy': 2828, 'communist': 2829, 'schedule': 2830, 'typically': 2831, 'cm': 2832, 'tablet': 2833, 'dinner': 2834, 'cast': 2835, 'recipes': 2836, 'added': 2837, 'visited': 2838, 'tells': 2839, 'offered': 2840, 'quota': 2841, 'his/her': 2842, 'forever': 2843, 'keyboard': 2844, 'inventions': 2845, 'liked': 2846, 'teams': 2847, 'console': 2848, 'anonymous': 2849, 'punjab': 2850, 'lenovo': 2851, 'represent': 2852, 'satellite': 2853, 'domestic': 2854, 'photoshop': 2855, 'beliefs': 2856, 'bread': 2857, 'nowadays': 2858, 'comfortable': 2859, 'ronaldo': 2860, 'mechanics': 2861, 'boot': 2862, 'significant': 2863, 'foundation': 2864, 'independence': 2865, 'narcissist': 2866, 'abuse': 2867, '80': 2868, 'paytm': 2869, 'sore': 2870, 'held': 2871, 'selection': 2872, 'terrorists': 2873, 'simply': 2874, 'designed': 2875, 'fruits': 2876, 'skinny': 2877, 'e.g': 2878, 'justice': 2879, 'donate': 2880, 'yoga': 2881, 'expression': 2882, 'mining': 2883, 'steam': 2884, 'cable': 2885, 'educational': 2886, 'generator': 2887, 'jail': 2888, 'spending': 2889, 'boobs': 2890, 'juice': 2891, 'principles': 2892, 'darth': 2893, 'produced': 2894, 'wet': 2895, 'grand': 2896, 'kingdom': 2897, 'expansion': 2898, 'franchise': 2899, 'journal': 2900, 'resolutions': 2901, 'aiims': 2902, 'clash': 2903, 'predict': 2904, 'allows': 2905, '2012': 2906, 'il': 2907, 'marvel': 2908, 'recipe': 2909, 'mahabharata': 2910, 'congress': 2911, 'photography': 2912, 'supports': 2913, 'conditions': 2914, 'percent': 2915, 'smartphones': 2916, 'removal': 2917, 'interface': 2918, 'fbi': 2919, '26': 2920, 'whenever': 2921, 'winter': 2922, '8.1': 2923, 'g': 2924, 'finding': 2925, 'venezuela': 2926, 'radiation': 2927, 'document': 2928, 'pride': 2929, 'tummy': 2930, 'happiness': 2931, 'rings': 2932, 'pronunciation': 2933, 'scariest': 2934, '\\\\frac': 2935, 'emails': 2936, 'angeles': 2937, 'destination': 2938, 'lakhs': 2939, 'butter': 2940, 'sons': 2941, 'ola': 2942, 'bush': 2943, 'discover': 2944, 'maharashtra': 2945, 'warming': 2946, 'brahmins': 2947, 'stronger': 2948, 'lady': 2949, 'nano': 2950, 'bands': 2951, 'irctc': 2952, 'factory': 2953, 'originate': 2954, 'supporting': 2955, 'usage': 2956, 'raised': 2957, 'risk': 2958, 'ready': 2959, 'cant': 2960, 'nikon': 2961, 'attacks': 2962, 'ebay': 2963, 'resource': 2964, 'singer': 2965, 'park': 2966, 'disk': 2967, 'jewish': 2968, 'organization': 2969, 'extent': 2970, 'arrive': 2971, 'split': 2972, 'cousin': 2973, 'direction': 2974, 'shock': 2975, '15000': 2976, 'connected': 2977, 'discovered': 2978, 'prize': 2979, 'diabetes': 2980, 'golden': 2981, 'automobile': 2982, 'efficiently': 2983, 'cigarettes': 2984, 'm.tech': 2985, 'proud': 2986, 'rape': 2987, 'session': 2988, 'sending': 2989, 'bite': 2990, 'spring': 2991, 'xiaomi': 2992, 'username': 2993, '₹500': 2994, 'recruitment': 2995, 'scheme': 2996, 'crisis': 2997, 'examination': 2998, 'setting': 2999, 'illuminati': 3000, 'highly': 3001, 'plot': 3002, 'expands': 3003, 'dish': 3004, 'atheist': 3005, 'mouth': 3006, 'replacement': 3007, 'remain': 3008, 'beauty': 3009, 'submit': 3010, 'cfa': 3011, 'luxury': 3012, 'internships': 3013, \"'the\": 3014, 'agency': 3015, 'milky': 3016, 'declared': 3017, 'virgin': 3018, 'simultaneously': 3019, 'fans': 3020, 'gobi': 3021, 'hedge': 3022, 'atomic': 3023, 'bones': 3024, 'ww2': 3025, 'destroyed': 3026, 'radius': 3027, 'suits': 3028, 'inspirational': 3029, 'gel': 3030, 'federal': 3031, 'target': 3032, 'contributions': 3033, 'silver': 3034, 'fully': 3035, 'eaten': 3036, 'obtain': 3037, 'ending': 3038, 'somme': 3039, 'recruiter': 3040, 'unhealthy': 3041, 'dumb': 3042, 'swami': 3043, 'vivekananda': 3044, 'lens': 3045, 'upvotes': 3046, 'region': 3047, 'customers': 3048, 'emotional': 3049, 'bluetooth': 3050, 'lived': 3051, 'whey': 3052, 'im': 3053, 'existed': 3054, 'ugc': 3055, 'designers': 3056, 'wrote': 3057, 'criminal': 3058, 'rahul': 3059, 'beyond': 3060, 'neck': 3061, 'activate': 3062, 'qualified': 3063, 'classical': 3064, 'fired': 3065, 'bengaluru': 3066, 'posted': 3067, 'senior': 3068, 'qualifications': 3069, 'teen': 3070, 'balaji': 3071, 'weak': 3072, 'rain': 3073, 'layman': 3074, 'zealand': 3075, 'sperm': 3076, 'anywhere': 3077, 'keywords': 3078, 'autism': 3079, 'awkward': 3080, 'paranormal': 3081, 'spectrum': 3082, 'interstellar': 3083, 'department': 3084, 'including': 3085, 'guinea': 3086, 'lawyer': 3087, 'laser': 3088, 'solid': 3089, 'array': 3090, 'captain': 3091, 'los': 3092, 'select': 3093, 'tour': 3094, 'greater': 3095, 'backup': 3096, 'thesis': 3097, 'comics': 3098, 'dr.': 3099, 'richard': 3100, 'orange': 3101, 'traveling': 3102, 'parties': 3103, 'transmission': 3104, 'critical': 3105, 'pool': 3106, 'interior': 3107, 'filter': 3108, 'action': 3109, 'alabama': 3110, 'restore': 3111, 'dell': 3112, 'uniform': 3113, 'marine': 3114, 'u': 3115, 'switzerland': 3116, 'stuff': 3117, 'concentrate': 3118, 'item': 3119, 'rental': 3120, 'trends': 3121, 'sequence': 3122, 'jayalalitha': 3123, 'urdu': 3124, 'adam': 3125, 'underwear': 3126, 'fan': 3127, 'freelance': 3128, 'arabic': 3129, 'photon': 3130, 'entrepreneurs': 3131, 'director': 3132, 'pronounce': 3133, 'parent': 3134, 'dj': 3135, 'bird': 3136, 'confused': 3137, 'biased': 3138, 'torrents': 3139, 'islands': 3140, 'aptitude': 3141, 'desire': 3142, 'iims': 3143, 'throughout': 3144, 'electrons': 3145, 'pimples': 3146, 'grades': 3147, 'worked': 3148, 'memories': 3149, 'consulting': 3150, 'einstein': 3151, 'ugly': 3152, 'justify': 3153, 'nail': 3154, 'verification': 3155, 'deaf': 3156, 'deserve': 3157, 'album': 3158, 'birds': 3159, 'enfield': 3160, 'beard': 3161, 'protection': 3162, 'molecule': 3163, 'forgotten': 3164, 'confident': 3165, 'lying': 3166, 'messi': 3167, 'door': 3168, 'dreaming': 3169, 'acting': 3170, 'tap': 3171, 'sms': 3172, 'belong': 3173, 'spread': 3174, 'geography': 3175, 'facing': 3176, 'telugu': 3177, 'employer': 3178, 'scene': 3179, 'pants': 3180, 'freelancing': 3181, 'beach': 3182, 'subjects': 3183, '350': 3184, 'codes': 3185, 'purchased': 3186, 'cheaper': 3187, 'cross': 3188, 'basics': 3189, 'i5': 3190, 'purple': 3191, 'airbnb': 3192, 'arrow': 3193, 'suffer': 3194, 'coins': 3195, 'friendship': 3196, 'peter': 3197, 'arms': 3198, 'mine': 3199, 'kinds': 3200, 'badly': 3201, 'orbit': 3202, 'extract': 3203, 'palestine': 3204, 'shirt': 3205, 'perform': 3206, 'lonely': 3207, 'quickest': 3208, 'vader': 3209, 'dvd': 3210, 'asians': 3211, 'abstract': 3212, 'helps': 3213, 'instant': 3214, 'mystery': 3215, 'illness': 3216, 'texts': 3217, 'ted': 3218, 'lyric': 3219, 'comedy': 3220, 'affiliate': 3221, 'pradesh': 3222, 'volte': 3223, 'tennis': 3224, 'increased': 3225, 'lesson': 3226, 'destroy': 3227, 'ghz': 3228, 'roles': 3229, 'junior': 3230, 'hospital': 3231, 'scenario': 3232, 'ultimate': 3233, 'martin': 3234, 'communism': 3235, '4th': 3236, '64': 3237, 'anonymously': 3238, 'shoot': 3239, 'cyrus': 3240, 'bihar': 3241, 'olympic': 3242, 'mentioned': 3243, 'bruce': 3244, 'upper': 3245, 'individuals': 3246, 'wisdom': 3247, 'sweet': 3248, 'snapdragon': 3249, 'millionaire': 3250, 'regularly': 3251, 'keeping': 3252, 'enterprise': 3253, 'cab': 3254, 'pink': 3255, 'further': 3256, 'innovative': 3257, 'bicycle': 3258, 'syndrome': 3259, 'payments': 3260, 'official': 3261, 'dental': 3262, 'chosen': 3263, 'resistance': 3264, 'comic': 3265, 'arguments': 3266, 'overall': 3267, 'annoying': 3268, 'asus': 3269, 'communicate': 3270, 'lunch': 3271, 'conditioner': 3272, 'settings': 3273, 'innovation': 3274, '9/11': 3275, 'hosting': 3276, 'friday': 3277, 'meeting': 3278, 'chhattisgarh': 3279, 'arranged': 3280, 'corrupt': 3281, '28': 3282, 'dslr': 3283, 'telling': 3284, 'singh': 3285, 'puppies': 3286, 'push': 3287, 'diwali': 3288, 'insert': 3289, 'fluid': 3290, 'legitimate': 3291, 'crew': 3292, 'sodium': 3293, 'concrete': 3294, 'plays': 3295, 'launched': 3296, 'bull': 3297, 'experiment': 3298, 'carrier': 3299, 'inflation': 3300, 'accident': 3301, '21st': 3302, 'generate': 3303, '22': 3304, 'leaves': 3305, 'sharing': 3306, 'obc': 3307, 'dislike': 3308, 'parallel': 3309, 'contribute': 3310, 'accepting': 3311, 'grey': 3312, '19': 3313, 'visible': 3314, 'magazine': 3315, 'offering': 3316, '2008': 3317, 'broadband': 3318, 'prophet': 3319, 'matlab': 3320, 'dates': 3321, 'persons': 3322, 'elizabeth': 3323, 'mostly': 3324, 'ups': 3325, 'costs': 3326, 'pack': 3327, 'majority': 3328, 'pf': 3329, 'waste': 3330, 'neutrality': 3331, 'sudden': 3332, 'github': 3333, 'missing': 3334, 'seems': 3335, 'murder': 3336, 'don': 3337, 'watches': 3338, 'reasoning': 3339, 'manaphy': 3340, 'trick': 3341, 'calling': 3342, 'landing': 3343, 'sociology': 3344, 'feels': 3345, 'immigrants': 3346, 'frequently': 3347, 'jeans': 3348, 'talent': 3349, 'agreement': 3350, 'careers': 3351, 'impossible': 3352, 'wealth': 3353, 'consume': 3354, 'blocking': 3355, '2018': 3356, 'environment': 3357, 'vit': 3358, 'ap': 3359, 'cake': 3360, 'painless': 3361, 'digit': 3362, 'fruit': 3363, 'reject': 3364, 'causing': 3365, 'cognizant': 3366, 'breast': 3367, 'neutral': 3368, 'vitamin': 3369, 'transport': 3370, 'retirement': 3371, 'families': 3372, 'lectures': 3373, 'jack': 3374, 'lakes': 3375, 'lake': 3376, 'jump': 3377, 'filling': 3378, 'biological': 3379, 'designs': 3380, 'f': 3381, 'holy': 3382, 'booking': 3383, 'nba': 3384, 'daniel': 3385, 'setup': 3386, 'algebra': 3387, 'appearing': 3388, 'pleasure': 3389, 'gta': 3390, '2016.': 3391, 'providers': 3392, 'swimming': 3393, 'brothers': 3394, 'kannada': 3395, 'sufficient': 3396, 'column': 3397, 'functional': 3398, 'artists': 3399, 'joined': 3400, 'agriculture': 3401, 'worry': 3402, 'halloween': 3403, 'raising': 3404, 'mistry': 3405, 'realistic': 3406, 'drama': 3407, 'fraud': 3408, 'politician': 3409, 'stem': 3410, 'jon': 3411, 'slim': 3412, 'input': 3413, 'acids': 3414, 'bags': 3415, 'sharma': 3416, 'honeymoon': 3417, 'boil': 3418, 'stark': 3419, 'norway': 3420, 'sides': 3421, 'equations': 3422, 'reputation': 3423, 'counter': 3424, 'scrapping': 3425, 'vehicles': 3426, 'kitchen': 3427, 'icici': 3428, 'introduction': 3429, 'followed': 3430, 'stuck': 3431, 'awesome': 3432, 'chewing': 3433, 'unblock': 3434, 'claims': 3435, 'progress': 3436, 'voted': 3437, 'ux': 3438, 'oldest': 3439, 'gtx': 3440, 'drinks': 3441, 'crimes': 3442, 'relevant': 3443, 'proteins': 3444, 'railways': 3445, 'safest': 3446, 'tomorrow': 3447, 'upset': 3448, 'ghosts': 3449, 'trace': 3450, 'commission': 3451, 'delta': 3452, 'communications': 3453, 'wise': 3454, 'singing': 3455, 'rotate': 3456, 'pages': 3457, 'bob': 3458, 'remote': 3459, 'absolute': 3460, '27': 3461, 'fictional': 3462, 'pork': 3463, 'managers': 3464, 'molecular': 3465, 'mandarin': 3466, 'holder': 3467, 'lifestyle': 3468, 'manual': 3469, 'perspective': 3470, 'sample': 3471, 'gurgaon': 3472, 'ruby': 3473, 'pilani': 3474, 'ssb': 3475, 'edward': 3476, 'starts': 3477, 'reddit': 3478, 'regulations': 3479, 'standards': 3480, 'ww1': 3481, 'essential': 3482, 'cooking': 3483, 'teenage': 3484, 'objective': 3485, 'architect': 3486, 'ecommerce': 3487, 'strange': 3488, 'matrix': 3489, 'formal': 3490, 'profession': 3491, 'holmes': 3492, 'nobel': 3493, 'ipl': 3494, 'aws': 3495, 'bba': 3496, 'inspired': 3497, 'resident': 3498, 'mythology': 3499, 'perfectly': 3500, 'arrested': 3501, 'electronic': 3502, 'walmart': 3503, 'contemporary': 3504, 'gallery': 3505, 'upvote': 3506, 'element': 3507, 'evolved': 3508, 'rude': 3509, 'retire': 3510, 'notifications': 3511, 'david': 3512, 'finish': 3513, 'shorter': 3514, 'bra': 3515, 'attraction': 3516, 'council': 3517, 'max': 3518, 'aid': 3519, 'residential': 3520, 'pills': 3521, '₹1000': 3522, 'programme': 3523, 'wave': 3524, 'refuse': 3525, 'ghost': 3526, 'moments': 3527, 'honest': 3528, 'plate': 3529, 'wonder': 3530, 'creation': 3531, 'earlier': 3532, 'pte': 3533, 'replaced': 3534, 'abs': 3535, 'bulk': 3536, 'conversations': 3537, 'felt': 3538, 'seconds': 3539, 'fabric': 3540, 'urban': 3541, 'taiwan': 3542, 'consciousness': 3543, 'lollipop': 3544, 'electromagnetic': 3545, 'august': 3546, 'tackle': 3547, 'sleepy': 3548, 'charger': 3549, 'nigeria': 3550, 'rated': 3551, 'karma': 3552, 'barcelona': 3553, 'tim': 3554, 'semester': 3555, 'july': 3556, 'stephen': 3557, 'h': 3558, 'pee': 3559, 'naked': 3560, 'everybody': 3561, 'gates': 3562, 'breasts': 3563, 'frozen': 3564, 'throwing': 3565, 'actresses': 3566, 'ncr': 3567, 'css': 3568, 'scan': 3569, 'pharmacy': 3570, 'login': 3571, 'animation': 3572, 'grown': 3573, 'grammatically': 3574, 'differentiate': 3575, 'integration': 3576, 'vinegar': 3577, 'cured': 3578, 'regards': 3579, 'vpn': 3580, 'clock': 3581, 'guns': 3582, '50,000': 3583, 'kid': 3584, 'lightning': 3585, '20s': 3586, 'skype': 3587, 'uri': 3588, 'talks': 3589, 'racism': 3590, 'monetize': 3591, 'moderation': 3592, 'gross': 3593, 'al': 3594, 'addicted': 3595, 'krishna': 3596, 'enhance': 3597, 'robotics': 3598, 'perception': 3599, 'cupcakes': 3600, 'acquire': 3601, 'thriller': 3602, 'mtech': 3603, 'ma': 3604, 'cruz': 3605, 'properly': 3606, 'finger': 3607, 'incest': 3608, 'kapil': 3609, 'spirit': 3610, 'membrane': 3611, 'angularjs': 3612, '1.5': 3613, 'asleep': 3614, 'governments': 3615, 'label': 3616, 'hat': 3617, 'ai': 3618, 'lights': 3619, 'generated': 3620, 'biting': 3621, 'wholesale': 3622, 'filled': 3623, 'slowly': 3624, 'hike': 3625, 'commonly': 3626, 'jimmy': 3627, 'wales': 3628, 'context': 3629, 'oh': 3630, 'newly': 3631, 'pollution': 3632, 'graduates': 3633, 'micro': 3634, 'cheese': 3635, 'gen': 3636, 'actions': 3637, 'evolve': 3638, 'relate': 3639, 'sale': 3640, 'trees': 3641, '23': 3642, 'hip': 3643, 'crystal': 3644, 'transgender': 3645, 'dropped': 3646, 'metro': 3647, 'minecraft': 3648, 'relieve': 3649, 'gujarat': 3650, 'mission': 3651, 'increases': 3652, 'soccer': 3653, 'bitsat': 3654, '9th': 3655, 'portugal': 3656, 'usps': 3657, 'axis': 3658, 'node.js': 3659, 'decisions': 3660, 'petroleum': 3661, 'dandruff': 3662, 'ethics': 3663, 'description': 3664, 'flights': 3665, 'owned': 3666, 'seven': 3667, 'referred': 3668, 'w.': 3669, 'shave': 3670, 'wood': 3671, 'weekends': 3672, 'icse': 3673, 'duty': 3674, 'flag': 3675, 'berlin': 3676, 'challenge': 3677, 'ahmedabad': 3678, 'roots': 3679, 'cement': 3680, 'tough': 3681, 'downloaded': 3682, 'cleaning': 3683, 'press': 3684, 'skip': 3685, 'recommended': 3686, 'soap': 3687, 'youth': 3688, 'taught': 3689, 'leading': 3690, 'hoax': 3691, 'october': 3692, 'foreigner': 3693, 'loses': 3694, 'louis': 3695, 'transactions': 3696, 'condom': 3697, 'eclipse': 3698, 'rejection': 3699, 'symbolize': 3700, 'collection': 3701, 'regions': 3702, 'url': 3703, 'wheel': 3704, 'collapse': 3705, 'summary': 3706, 'inner': 3707, 'bag': 3708, 'weekend': 3709, 'reverse': 3710, 'vacation': 3711, 'pornstar': 3712, 'p': 3713, 'wwe': 3714, 'agencies': 3715, 'luke': 3716, 'specifically': 3717, 'ml': 3718, 'variable': 3719, 'nursing': 3720, 'amd': 3721, 'ipcc': 3722, 'ireland': 3723, 'nazi': 3724, 'eastern': 3725, 'remedy': 3726, 'angular': 3727, 'environmental': 3728, 'hang': 3729, 'safely': 3730, 'premier': 3731, 'gary': 3732, 'portal': 3733, 'limitations': 3734, 'mature': 3735, 'trainee': 3736, 'dirty': 3737, 'millions': 3738, '2020': 3739, 'fb': 3740, 'whereas': 3741, 'stone': 3742, 'requirement': 3743, 'larger': 3744, 'decides': 3745, 'psu': 3746, 'town': 3747, 'georgia': 3748, 'versus': 3749, 'dimension': 3750, 'airplane': 3751, 'guilty': 3752, 'bark': 3753, 'faced': 3754, 'tower': 3755, 'homosexuality': 3756, 'colorado': 3757, 'bengal': 3758, 'van': 3759, 'dropbox': 3760, 'inspiring': 3761, 'sheet': 3762, 'viral': 3763, 'performed': 3764, 'bleach': 3765, 'granted': 3766, 'violence': 3767, 'curve': 3768, 'notification': 3769, 'downloads': 3770, 'audience': 3771, 'punjabi': 3772, 'shaped': 3773, 'railway': 3774, 'smaller': 3775, 'ranger': 3776, 'layer': 3777, 'beam': 3778, 'interaction': 3779, 'religions': 3780, 'excited': 3781, 'passionate': 3782, 'string': 3783, 'minor': 3784, 'wild': 3785, 'solo': 3786, 'certified': 3787, 'propose': 3788, 'units': 3789, 'instrumentation': 3790, 'contains': 3791, 'passion': 3792, 'lawyers': 3793, 'nucleus': 3794, 'acceleration': 3795, 'fifa': 3796, 'adults': 3797, 'noida': 3798, 'dota': 3799, 'genetically': 3800, 'measures': 3801, 'ensure': 3802, 'annual': 3803, 'smile': 3804, '|': 3805, 'thermal': 3806, 'versa': 3807, 'owners': 3808, 'solved': 3809, 'basketball': 3810, 'compound': 3811, 'ford': 3812, 'poetry': 3813, 'originally': 3814, 'quantitative': 3815, 'prelims': 3816, 'affects': 3817, '60k': 3818, 'ranking': 3819, 'excessive': 3820, 'turns': 3821, 'terminal': 3822, 'wildlife': 3823, 'balls': 3824, 'published': 3825, 'relatively': 3826, '2019': 3827, 'opportunity': 3828, 'sigma': 3829, 'however': 3830, 'bisexual': 3831, 'gf': 3832, '70': 3833, 'airtel': 3834, 'europeans': 3835, 'ek': 3836, 'cute': 3837, 'spiritual': 3838, 'difficulty': 3839, 'index': 3840, 'judaism': 3841, 'monitoring': 3842, 'calendar': 3843, 'playstation': 3844, 'whats': 3845, 'aluminum': 3846, 'finally': 3847, 'philippine': 3848, 'absent': 3849, 'hackers': 3850, 'church': 3851, 'acoustic': 3852, 'suggested': 3853, 'charges': 3854, 'nudity': 3855, 'capacitor': 3856, 'secretary': 3857, 'drone': 3858, 'buffalo': 3859, 'tasks': 3860, 'atmosphere': 3861, 'mentor': 3862, 'ncert': 3863, 'looked': 3864, 'neuroscience': 3865, 'kohli': 3866, 'freshers': 3867, 'tracks': 3868, 'holiday': 3869, 'printer': 3870, 'genetic': 3871, 'mount': 3872, 'tiles': 3873, 'labor': 3874, 'stranger': 3875, 'probably': 3876, 'premium': 3877, 'mnc': 3878, 'southern': 3879, 'none': 3880, 'increasing': 3881, 'nato': 3882, 'dies': 3883, 'uan': 3884, 'muller': 3885, 'sets': 3886, '10,000': 3887, 'politicians': 3888, 'hadoop': 3889, 'sc': 3890, 'implications': 3891, 'blame': 3892, 'chairman': 3893, 'menstrual': 3894, 'schizophrenia': 3895, 'temporarily': 3896, 'entropy': 3897, 'sync': 3898, 'conversion': 3899, 'deals': 3900, 'poems': 3901, 'cpu': 3902, 'settle': 3903, 'listed': 3904, 'prescription': 3905, 'strengths': 3906, 'stored': 3907, 'wechat': 3908, 'bcom': 3909, 'distinguish': 3910, 'attitude': 3911, 'amendment': 3912, 'judge': 3913, 'auckland': 3914, 'guard': 3915, 'invention': 3916, 'compatible': 3917, 'cleared': 3918, 'dick': 3919, '50k': 3920, 'perceive': 3921, 'mysql': 3922, 'flavor': 3923, 'shut': 3924, 'thermodynamics': 3925, 'cabinet': 3926, 'succeed': 3927, 'superpower': 3928, 'illinois': 3929, 'author': 3930, 'optional': 3931, 'rss': 3932, 'galaxies': 3933, 'organizations': 3934, 'reserve': 3935, 'wipro': 3936, 'lean': 3937, 'bottle': 3938, 'vegan': 3939, 'defined': 3940, 'bhopal': 3941, 'shiva': 3942, 'aap': 3943, 'betting': 3944, 'acquisition': 3945, 'failing': 3946, 'hits': 3947, 'television': 3948, 'herself': 3949, 'retail': 3950, 'default': 3951, 'penny': 3952, 'arm': 3953, 'protected': 3954, 'caring': 3955, 'refugees': 3956, 'pharmaceuticals': 3957, 'testosterone': 3958, 'differential': 3959, 'sensitive': 3960, 'stare': 3961, 'triple': 3962, 'pinterest': 3963, 'nude': 3964, 'pin': 3965, 'activation': 3966, 'cinema': 3967, 'knight': 3968, 'nor': 3969, 'strangest': 3970, 'borderline': 3971, 'bikes': 3972, 'hundred': 3973, 'honey': 3974, 'fiber': 3975, 'stable': 3976, 'manga': 3977, 'several': 3978, 'loneliness': 3979, '35': 3980, 'headphone': 3981, 'warm': 3982, 'viruses': 3983, 'sue': 3984, 'moves': 3985, 'heal': 3986, 'makeup': 3987, 'louisiana': 3988, 'abortion': 3989, 'lover': 3990, 'award': 3991, 'italian': 3992, 'lg': 3993, 'anyway': 3994, 'nexus': 3995, 'seattle': 3996, 'transplant': 3997, 'hell': 3998, '30k': 3999, 'installed': 4000, 'risks': 4001, 'april': 4002, '360': 4003, 'involve': 4004, 'serial': 4005, 'ufo': 4006, 'burning': 4007, 'appeared': 4008, 'worldwide': 4009, 'textbooks': 4010, 'argentina': 4011, 'productive': 4012, 'camp': 4013, 'feedback': 4014, 'bmw': 4015, 'beef': 4016, 'discount': 4017, 'du': 4018, 'lips': 4019, 'weapon': 4020, 'democrats': 4021, 'spy': 4022, 'simplify': 4023, 'partners': 4024, 'insane': 4025, 'punishment': 4026, 'attached': 4027, 'remotely': 4028, 'bubble': 4029, 'extreme': 4030, 'integrated': 4031, 'committed': 4032, 'armed': 4033, 'santa': 4034, 'retailers': 4035, 'seriously': 4036, 'subscribers': 4037, 'remainder': 4038, 'authors': 4039, 'robot': 4040, 'evening': 4041, 'crash': 4042, 'tongue': 4043, 'protons': 4044, 'notable': 4045, 'stains': 4046, 'volunteer': 4047, 'forex': 4048, 'hidden': 4049, 'bowl': 4050, 'fundamental': 4051, 'evolutionary': 4052, 'therapist': 4053, 'ibm': 4054, 'map': 4055, 'toefl': 4056, 'outcome': 4057, 'structural': 4058, 'mp3': 4059, 'gum': 4060, 'convictions': 4061, 'standing': 4062, 'invade': 4063, 'intended': 4064, '2010': 4065, 'texting': 4066, 'cows': 4067, 'starbucks': 4068, 'writers': 4069, 'genius': 4070, 'secondary': 4071, 'chartered': 4072, 'healthcare': 4073, 'additional': 4074, 'preferred': 4075, 'circumstances': 4076, 'blogging': 4077, 'hi': 4078, 'era': 4079, 'separation': 4080, 'medicines': 4081, 'cursed': 4082, 'swiss': 4083, 'bass': 4084, 'combination': 4085, 'screenshot': 4086, 'creativity': 4087, 'weigh': 4088, 'lighter': 4089, 'implementation': 4090, 'bone': 4091, 'cracked': 4092, 'markets': 4093, 'journalism': 4094, 'pending': 4095, 'isro': 4096, 'homes': 4097, 'bermuda': 4098, 'hostel': 4099, 'forum': 4100, 'highways': 4101, 'proceed': 4102, 'cultures': 4103, 'empathy': 4104, 'providing': 4105, 'saturn': 4106, 'trek': 4107, 'portuguese': 4108, 'managed': 4109, 'jailbreak': 4110, 'compensation': 4111, 'relativity': 4112, 'investigation': 4113, 'rational': 4114, 'coimbatore': 4115, 'socially': 4116, 'achievements': 4117, 'organs': 4118, 'mis': 4119, 'frame': 4120, 'practices': 4121, 'captaincy': 4122, 'everest': 4123, 'permission': 4124, 'ntse': 4125, 'charging': 4126, 'overnight': 4127, 'tag': 4128, 'shield': 4129, 'cookies': 4130, 'outlook': 4131, 'shipping': 4132, 'combine': 4133, 'scars': 4134, 'substance': 4135, 'gear': 4136, 'factor': 4137, '20k': 4138, 'rifle': 4139, 'brakes': 4140, 'possibly': 4141, 'firebase': 4142, 'ride': 4143, 'permit': 4144, 'expired': 4145, '75': 4146, 'luck': 4147, 'automatic': 4148, 'choosing': 4149, 'paralysis': 4150, 'civilization': 4151, 'resign': 4152, 'admitted': 4153, 'insecure': 4154, 'mi': 4155, 'swift': 4156, 'eee': 4157, 'documentary': 4158, 'proposal': 4159, 'technique': 4160, 'recommendation': 4161, 'i3': 4162, 'yale': 4163, 'aadhaar': 4164, 'epic': 4165, 'struggle': 4166, 'harm': 4167, 'scores': 4168, 'onsite': 4169, 'improving': 4170, 'cooper': 4171, 'rome': 4172, 'disadvantage': 4173, 'smoked': 4174, 'sit': 4175, 'extension': 4176, 'safari': 4177, 'gotten': 4178, 'abusive': 4179, 'mechanism': 4180, 'receiving': 4181, 'wealthy': 4182, 'diagram': 4183, 'eagle': 4184, 'equally': 4185, 'edmonton': 4186, 'afghanistan': 4187, 'festival': 4188, 'tattoos': 4189, 'surname': 4190, 'vietnamese': 4191, 'salman': 4192, 'boiling': 4193, 'simplest': 4194, 'ips': 4195, 'recommendations': 4196, 'full-time': 4197, 'astronomy': 4198, 'checking': 4199, 'elon': 4200, 'musk': 4201, 'motivational': 4202, 'cpec': 4203, 'hottest': 4204, 'intercourse': 4205, 'cyber': 4206, 'complaint': 4207, 'linear': 4208, 'gsoc': 4209, 'deeply': 4210, 'justified': 4211, 'gita': 4212, 'documentaries': 4213, 'offices': 4214, 'htc': 4215, 'membership': 4216, 'calcium': 4217, 'procrastination': 4218, 'flirt': 4219, 'stops': 4220, 'heaven': 4221, 'pair': 4222, 'voter': 4223, 'spin': 4224, 'fiitjee': 4225, 'gaining': 4226, 'etc.': 4227, 'doesn': 4228, 'cognitive': 4229, 'therapy': 4230, 'ends': 4231, 'cristiano': 4232, 'satellites': 4233, 'verizon': 4234, 'atoms': 4235, 'searching': 4236, 'plugin': 4237, 'jan': 4238, 'victim': 4239, 'iraq': 4240, 'o': 4241, 'esteem': 4242, 'reduced': 4243, 'drives': 4244, 'accent': 4245, 'disappeared': 4246, 'disappear': 4247, 'entering': 4248, 'seeds': 4249, 'batteries': 4250, 'musical': 4251, 'dot': 4252, 'installing': 4253, 'worrying': 4254, 'ignorant': 4255, 'conventional': 4256, 'refrigerator': 4257, 'cv': 4258, 'nitrogen': 4259, 'teenagers': 4260, 'competitors': 4261, 'profiles': 4262, 'torque': 4263, 'crying': 4264, 'kik': 4265, 'sir': 4266, 'scalp': 4267, 'tumblr': 4268, '10000': 4269, 'stations': 4270, 'marshmallow': 4271, 'g4': 4272, 'airplanes': 4273, 'grad': 4274, 'hobby': 4275, 'syrian': 4276, 'worship': 4277, 'ji': 4278, 'denomination': 4279, 'rural': 4280, 'heating': 4281, 'warren': 4282, 'aspirant': 4283, 'kgs': 4284, 'austin': 4285, 'marathi': 4286, 'roll': 4287, 'unfair': 4288, 'motivates': 4289, 'advisable': 4290, 'packages': 4291, 're': 4292, 'fault': 4293, 'integer': 4294, 'hdfc': 4295, 'tutorials': 4296, 'yrs': 4297, 'merge': 4298, 'detect': 4299, 'marriages': 4300, 'configuration': 4301, 'founder': 4302, 'signals': 4303, 'verify': 4304, 'circuits': 4305, 'farming': 4306, 'policies': 4307, 'excellent': 4308, 'evaluate': 4309, 'recognition': 4310, 'vegas': 4311, 'houses': 4312, 'w': 4313, 'adhd': 4314, 'links': 4315, 'holocaust': 4316, 'deliver': 4317, 'funded': 4318, 'converted': 4319, 'rare': 4320, 'brought': 4321, 'whiskey': 4322, 'passing': 4323, 'salesforce': 4324, 'addictive': 4325, 'toronto': 4326, 'avatar': 4327, 'drunk': 4328, 'husbands': 4329, 'thai': 4330, 'hello': 4331, 'bias': 4332, 'chatting': 4333, 'stamina': 4334, 'hall': 4335, 'speaks': 4336, 'fridge': 4337, 'hurting': 4338, 'presentation': 4339, 'folder': 4340, '45': 4341, 'snake': 4342, 'covered': 4343, 'siblings': 4344, 'tense': 4345, 'snowden': 4346, 'pump': 4347, 'mainland': 4348, 'sitting': 4349, 'wire': 4350, 'prostitution': 4351, 'transformer': 4352, 'playlist': 4353, 'libraries': 4354, 'possess': 4355, 'inch': 4356, 'sanskrit': 4357, 'beginning': 4358, 'wives': 4359, 'bedroom': 4360, 'zuckerberg': 4361, 'ph': 4362, 'maintenance': 4363, 'diamond': 4364, 'cap': 4365, 'rising': 4366, 'f1': 4367, 'wings': 4368, 'spam': 4369, 'escorts': 4370, 'polo': 4371, 'stereotypes': 4372, 'suit': 4373, 'towns': 4374, 'ordered': 4375, 'patch': 4376, 'lottery': 4377, 'pitbull': 4378, 'knee': 4379, 'disagree': 4380, 'tie': 4381, 'clans': 4382, 'float': 4383, 'appointment': 4384, 'ejaculate': 4385, 'friction': 4386, 'assessment': 4387, 'discharge': 4388, 'mad': 4389, 'taxis': 4390, 'molecules': 4391, 'included': 4392, 'trend': 4393, 'owns': 4394, '150': 4395, 'command': 4396, 'controlled': 4397, '2-3': 4398, 'voters': 4399, 'lots': 4400, 'interests': 4401, 'stamp': 4402, 'confirmed': 4403, 'satisfied': 4404, 'declare': 4405, 'riding': 4406, 'diarrhea': 4407, 'capable': 4408, 'ejaculation': 4409, 'ukraine': 4410, 'cpt': 4411, 'hypothesis': 4412, 'holding': 4413, 'snapdeal': 4414, 'cuba': 4415, 'virat': 4416, 'fried': 4417, '2.5': 4418, 'denmark': 4419, 'intern': 4420, 'firewall': 4421, 'creepiest': 4422, 'conductor': 4423, 'influential': 4424, 'throw': 4425, 'minds': 4426, 'mitochondria': 4427, 'trial': 4428, 'miles': 4429, 'sony': 4430, 'onto': 4431, 'ionic': 4432, 'mosquitoes': 4433, 'acceptance': 4434, 'assistance': 4435, 'purchasing': 4436, 'domino': 4437, 'menu': 4438, 'sustainable': 4439, 'glue': 4440, 'agents': 4441, 'discuss': 4442, 'nda': 4443, 'soluble': 4444, 'goldman': 4445, 'sachs': 4446, 'updates': 4447, 'imessage': 4448, 'opened': 4449, 'meaningful': 4450, 'june': 4451, 'branded': 4452, 'updated': 4453, 'transit': 4454, 'wolf': 4455, 'rating': 4456, 'expand': 4457, 'attending': 4458, 'consultancy': 4459, 'scandinavian': 4460, 'breaks': 4461, 'typing': 4462, 'statistical': 4463, 'attention': 4464, 'practicing': 4465, 'camping': 4466, 'fighters': 4467, 'lowest': 4468, 'decided': 4469, 'sphere': 4470, 'classroom': 4471, 'beings': 4472, 'motors': 4473, 'overlay': 4474, 'j7': 4475, 'tenure': 4476, 'kindle': 4477, 'organisms': 4478, 'kept': 4479, 'fireworks': 4480, 'mention': 4481, 'extend': 4482, 'downgrade': 4483, 'recovered': 4484, 'lte': 4485, 'rom': 4486, 'wan': 4487, 'paul': 4488, 'jaw': 4489, 'theft': 4490, 'bamboo': 4491, 'retina': 4492, 'instantly': 4493, 'correlation': 4494, 'gained': 4495, 'grid': 4496, 'sikhs': 4497, 'centers': 4498, 'transition': 4499, 'horses': 4500, 'tradition': 4501, 'midnight': 4502, 'switched': 4503, 'smarter': 4504, 'unity': 4505, 'minority': 4506, 'introverts': 4507, 'clubs': 4508, 'memorize': 4509, 'agricultural': 4510, 'leonardo': 4511, 'snakes': 4512, 'snap': 4513, 'ndtv': 4514, 'lucky': 4515, 'forgive': 4516, 'messaging': 4517, 'window': 4518, 'institutions': 4519, 'issued': 4520, 'bleeding': 4521, 'studio': 4522, 'conditioning': 4523, 'gears': 4524, 'bringing': 4525, 'appearance': 4526, 'patent': 4527, 'dimensions': 4528, 'serving': 4529, 'thick': 4530, 'astronauts': 4531, 'fairy': 4532, 'tail': 4533, 'loving': 4534, 'offensive': 4535, 'exit': 4536, 'titanium': 4537, 'joker': 4538, 'multiverse': 4539, 'upvoted': 4540, 'marrying': 4541, 'aadhar': 4542, 'strategic': 4543, 'x^2': 4544, 'acceptable': 4545, 'dresses': 4546, 'sin': 4547, 'tester': 4548, 'introducing': 4549, 'lemon': 4550, 'drawn': 4551, 'ridiculous': 4552, 'sick': 4553, 'aggregate': 4554, 'trans': 4555, 'afford': 4556, 'breed': 4557, 'assuming': 4558, 'surge': 4559, 'pricing': 4560, 'wwii': 4561, 'approved': 4562, 'perks': 4563, 'trichy': 4564, 'interpret': 4565, 'blonde': 4566, 'apartment': 4567, 'receipt': 4568, 'kuwait': 4569, 'fox': 4570, 'sensor': 4571, 'aloe': 4572, 'vera': 4573, 'smartest': 4574, 'capitalism': 4575, 'finished': 4576, 'studied': 4577, 'seal': 4578, 'communities': 4579, 'survey': 4580, 'graduating': 4581, 'weaknesses': 4582, 'bootstrap': 4583, 'atom': 4584, 'mood': 4585, 'wallet': 4586, 'ended': 4587, 'feminism': 4588, 'cellphone': 4589, 'astronaut': 4590, 'allies': 4591, '100k': 4592, 'sub': 4593, 'beats': 4594, 'bumps': 4595, 'vc': 4596, 'tamilnadu': 4597, 'llc': 4598, 'cancelled': 4599, 'mock': 4600, '^2': 4601, 'oxford': 4602, 'shortest': 4603, 't-shirt': 4604, 'removing': 4605, 'ohio': 4606, 'dramas': 4607, 'b.': 4608, 'improved': 4609, 'inequality': 4610, 'boeing': 4611, 'hz': 4612, 'fingers': 4613, 'prepared': 4614, 'newspapers': 4615, 'somewhere': 4616, 'composite': 4617, 'veins': 4618, 'fuck': 4619, 'rose': 4620, 'pilots': 4621, 'whatever': 4622, 'attempts': 4623, 'kissing': 4624, 'mm': 4625, 'nagpur': 4626, 'consumption': 4627, 'brush': 4628, 'narcissists': 4629, 'extinct': 4630, 'corn': 4631, 'entrepreneurship': 4632, 'superman': 4633, 'superhero': 4634, 'pre': 4635, 'vancouver': 4636, 'palm': 4637, 'competition': 4638, 'forced': 4639, 'jew': 4640, 'accountant': 4641, 'william': 4642, 'thiel': 4643, 'cube': 4644, 's6': 4645, 'slab': 4646, 'tend': 4647, 'enable': 4648, 'counselling': 4649, 'tomato': 4650, 'sauce': 4651, 'launching': 4652, 'row': 4653, 'sexiest': 4654, 'awareness': 4655, 'arduino': 4656, 'chase': 4657, 'co': 4658, 'passengers': 4659, 'attain': 4660, 'duration': 4661, 'loser': 4662, 'ace': 4663, 'bombay': 4664, 'tiger': 4665, 'bridge': 4666, 'specialist': 4667, 'phenomenon': 4668, 'totally': 4669, 'educated': 4670, '800': 4671, 'prince': 4672, 'browsing': 4673, 'aware': 4674, 'helium': 4675, 'qatar': 4676, 'significantly': 4677, 'closer': 4678, 'induction': 4679, 'extensions': 4680, 'affair': 4681, 'destiny': 4682, 'servers': 4683, 'classification': 4684, 'roller': 4685, 'apache': 4686, 'wide': 4687, 'covers': 4688, 'classified': 4689, 'spent': 4690, 'implemented': 4691, 'breakup': 4692, 'bell': 4693, 'productivity': 4694, 'usd': 4695, 'villain': 4696, 'employment': 4697, 'rap': 4698, 'realized': 4699, 'stick': 4700, 'keys': 4701, 'expenses': 4702, 'hired': 4703, 'ink': 4704, 'continuous': 4705, 'tatkal': 4706, 'promises': 4707, 'temperament': 4708, 'journey': 4709, 'platforms': 4710, 'repeat': 4711, 'bachelors': 4712, 'j': 4713, 'temples': 4714, 'mangalore': 4715, 'tourism': 4716, 'clone': 4717, 'cocaine': 4718, 'eyesight': 4719, 'rooting': 4720, 'lucid': 4721, 'someones': 4722, 'nvidia': 4723, 'governor': 4724, 'camps': 4725, 'collect': 4726, 'enlightenment': 4727, 'telecom': 4728, 'adaptations': 4729, 'cia': 4730, 'hypothetical': 4731, 'echo': 4732, 'hollow': 4733, 'catholic': 4734, 'immigrate': 4735, 'anatomy': 4736, 'alaska': 4737, 'verbal': 4738, 'immortal': 4739, 'chandigarh': 4740, 'ssd': 4741, 'nri': 4742, 'bonus': 4743, 'manufacturers': 4744, 'bds': 4745, 'lions': 4746, 'dangal': 4747, 'unfollow': 4748, 'williams': 4749, 'prediction': 4750, 'joe': 4751, 'coincidence': 4752, 'bombs': 4753, 'graduated': 4754, 'km': 4755, 'std': 4756, 'altitude': 4757, 'thanksgiving': 4758, 'territories': 4759, 'noticed': 4760, 'turbine': 4761, 'registering': 4762, 'draft': 4763, 'supernatural': 4764, 'jazz': 4765, 'delay': 4766, 'coke': 4767, 'pornography': 4768, 'icai': 4769, 'peer': 4770, 'sender': 4771, 'disorders': 4772, 'rationale': 4773, 'subscribe': 4774, 'spark': 4775, 'rooms': 4776, 'conference': 4777, 'balanced': 4778, 'him/her': 4779, 'mate': 4780, 'amateur': 4781, 'zinc': 4782, 'vat': 4783, 'orlando': 4784, 'contribution': 4785, 'wage': 4786, 'jordan': 4787, 'vodka': 4788, 'entertainment': 4789, 'lists': 4790, 'teens': 4791, 'rough': 4792, 'bipolar': 4793, 'recruiters': 4794, 'lyft': 4795, 'clearly': 4796, 'promotion': 4797, 'discussion': 4798, 'lbs': 4799, 'duplicate': 4800, 'genuinely': 4801, 'transform': 4802, 'flowers': 4803, 'titles': 4804, 'christ': 4805, 'shoe': 4806, 'photons': 4807, 'district': 4808, 'sip': 4809, 'kilos': 4810, 'pence': 4811, 'buddhist': 4812, 'airbus': 4813, 'variables': 4814, 'discovery': 4815, 'discipline': 4816, 'conserved': 4817, 'admit': 4818, 'promising': 4819, '\\\\sqrt': 4820, 'adapter': 4821, 'employers': 4822, 'mouse': 4823, 'cracking': 4824, 'thousand': 4825, 'conscious': 4826, 'bottom': 4827, 'pardon': 4828, 'toughest': 4829, 'touching': 4830, 'honours': 4831, 'calcutta': 4832, 'irs': 4833, 'curse': 4834, 'popularity': 4835, '.net': 4836, 'duke': 4837, 'dm': 4838, 'preparations': 4839, 'genre': 4840, 'participate': 4841, 'saddest': 4842, 'reports': 4843, '4gb': 4844, 'softwares': 4845, 'mahatma': 4846, 'bharat': 4847, 'antarctica': 4848, 'salaries': 4849, 'cashless': 4850, 'hypnosis': 4851, 'outer': 4852, 'accomplish': 4853, 'pension': 4854, 'nintendo': 4855, 'allen': 4856, 'accessories': 4857, 'punch': 4858, 'fell': 4859, 'seasons': 4860, 'shakes': 4861, 'containing': 4862, 'apes': 4863, 's7': 4864, 'shops': 4865, 'aggressive': 4866, 'alkali': 4867, 'revolve': 4868, 'mint': 4869, 'invisible': 4870, 'centres': 4871, 'deepika': 4872, 'partition': 4873, 'da': 4874, 'shark': 4875, 'farm': 4876, 'although': 4877, 'intuitive': 4878, 'pets': 4879, 'inferior': 4880, 'animated': 4881, 'outlet': 4882, 'jam': 4883, '51': 4884, 'merchant': 4885, 'venice': 4886, 'recording': 4887, 'hiv': 4888, 'combustion': 4889, 'athletes': 4890, 'awakens': 4891, 'imagination': 4892, 'buyers': 4893, 'defence': 4894, 'assange': 4895, 'blank': 4896, 'lease': 4897, 'uae': 4898, 'patterns': 4899, 'iconic': 4900, 'equipment': 4901, 'helped': 4902, 'appears': 4903, 'concerned': 4904, 'cop': 4905, 'configure': 4906, 'stretch': 4907, 'diamonds': 4908, 'optimal': 4909, 'fedex': 4910, 'liquids': 4911, 'supplement': 4912, 'bitter': 4913, 'cc': 4914, 'devil': 4915, 'advertise': 4916, 'acts': 4917, 'qualify': 4918, 'nutrition': 4919, 'pixar': 4920, 'collapsed': 4921, 'printed': 4922, 'exposed': 4923, 'motorcycle': 4924, 'analog': 4925, 'derivative': 4926, 'alternate': 4927, 'niche': 4928, 'grocery': 4929, 'comparing': 4930, 'breakdown': 4931, 'claimed': 4932, 'trainer': 4933, 'planned': 4934, 'seduce': 4935, 'zodiac': 4936, 'overseas': 4937, 'adams': 4938, 'loop': 4939, 'sounds': 4940, 'russians': 4941, 'division': 4942, 'odi': 4943, 'autistic': 4944, 'noam': 4945, 'chomsky': 4946, 'bug': 4947, 'authority': 4948, 'lungs': 4949, 'apis': 4950, 'coconut': 4951, 'calm': 4952, 'singers': 4953, 'christopher': 4954, 'excuse': 4955, 'drain': 4956, 'meanings': 4957, 'privilege': 4958, 'nervous': 4959, 'combat': 4960, 'wind': 4961, 'rajasthan': 4962, 'elementary': 4963, 'directory': 4964, 'depend': 4965, 'influenced': 4966, 'instrument': 4967, 'blow': 4968, 'sword': 4969, 'volcano': 4970, 'cheque': 4971, 'caffeine': 4972, 'prior': 4973, 'hdmi': 4974, 'verified': 4975, 'mary': 4976, 'strongly': 4977, 'clutch': 4978, 'lpa': 4979, 'uploading': 4980, 'ethnic': 4981, 'squad': 4982, 'molar': 4983, 'facetime': 4984, 'solving': 4985, 'sibling': 4986, 'vanilla': 4987, 'bodies': 4988, 'aeronautical': 4989, 'lucknow': 4990, 'nuke': 4991, 'compliment': 4992, 'combinations': 4993, 'aliexpress': 4994, 'chains': 4995, 'gainer': 4996, 'dr': 4997, 'bother': 4998, 'craigslist': 4999, 'putting': 5000, 'revenge': 5001, 'trademark': 5002, 'calculator': 5003, 'lab': 5004, 'iss': 5005, 'paragraph': 5006, 'worn': 5007, 'processors': 5008, 'sizes': 5009, 'bhubaneswar': 5010, 'golf': 5011, 'enlightened': 5012, '2.0': 5013, 'responsibilities': 5014, 'vr': 5015, 'rotation': 5016, 'heads': 5017, 'edition': 5018, 'slit': 5019, 'neighbor': 5020, 'hulk': 5021, 'swollen': 5022, 'vegetable': 5023, 'normally': 5024, 'psychologist': 5025, 'emergency': 5026, 'versions': 5027, 'signature': 5028, 'kit': 5029, 'warranty': 5030, 'escort': 5031, 'ceramic': 5032, 'runs': 5033, 'liquor': 5034, 'supermarket': 5035, 'cruise': 5036, 'static': 5037, 'confess': 5038, 'propel': 5039, 'ocd': 5040, 'surprise': 5041, 'intermediate': 5042, 'optical': 5043, 'wasting': 5044, 'covalent': 5045, 'vegetables': 5046, 'collie': 5047, 'dubbed': 5048, 'asp.net': 5049, 'opposed': 5050, 'srm': 5051, 'zombie': 5052, 'luther': 5053, 'newspaper': 5054, 'territory': 5055, 'iot': 5056, 'victory': 5057, 'flour': 5058, '500/1000': 5059, 'affecting': 5060, 'hawaii': 5061, 'tight': 5062, 'waist': 5063, 'wipe': 5064, 'anti': 5065, 'coach': 5066, 'vampire': 5067, 'approaches': 5068, 'tuition': 5069, 'fellowship': 5070, 'recognize': 5071, 'cellular': 5072, 'attach': 5073, '-1': 5074, 'ex-boyfriend': 5075, 'charlie': 5076, 'efficiency': 5077, 'henry': 5078, '20th': 5079, 'noise': 5080, 'billionaires': 5081, 'toxic': 5082, 'se': 5083, 'thickness': 5084, 'pcm': 5085, 'aerospace': 5086, 'md': 5087, 'tribe': 5088, 'treaty': 5089, 'multiplayer': 5090, 'varanasi': 5091, 'em': 5092, 'loud': 5093, 'resolve': 5094, 'comparison': 5095, 'tires': 5096, 'uttar': 5097, 'bin': 5098, 'monday': 5099, 'kick': 5100, 'ccna': 5101, 'meets': 5102, 'magnitude': 5103, 'nikola': 5104, 'speeches': 5105, 'finland': 5106, 'scholarships': 5107, 'civilians': 5108, 'massive': 5109, 'hcl': 5110, 'symbols': 5111, 'shared': 5112, 'damaged': 5113, 'soviet': 5114, 'margin': 5115, 'stuffy': 5116, 'fought': 5117, 'stages': 5118, 'crude': 5119, 'ray': 5120, 'strip': 5121, 'warrior': 5122, 'grounds': 5123, 'ulcer': 5124, 'relief': 5125, 'selfish': 5126, 'ruled': 5127, 'ben': 5128, 'belief': 5129, 'scary': 5130, 'potty': 5131, 'bloomberg': 5132, 'accurately': 5133, 'diagnosed': 5134, 'ex-girlfriend': 5135, 'slave': 5136, 'talked': 5137, 'morality': 5138, 'philosophical': 5139, 'tiny': 5140, 'offshore': 5141, 'robots': 5142, 'elite': 5143, 'financially': 5144, 'nights': 5145, 'scraping': 5146, 'closest': 5147, '3.5': 5148, 'decline': 5149, 'terrorism': 5150, 'aim': 5151, 'interviewer': 5152, 'siri': 5153, 'socialism': 5154, 'kapoor': 5155, 'monopoly': 5156, 'heisenberg': 5157, 'uncertainty': 5158, 'foam': 5159, '1990': 5160, '2d': 5161, 'iphones': 5162, 'ambani': 5163, 'swim': 5164, 'ia': 5165, 'responsibility': 5166, 'cpa': 5167, 'jake': 5168, 'dumped': 5169, 'pcs': 5170, 'concentrated': 5171, 'literally': 5172, 'dialogue': 5173, 'locations': 5174, 'quad': 5175, 'climbing': 5176, 'climb': 5177, 'sachin': 5178, 'jamaica': 5179, 'represented': 5180, 'nominee': 5181, 'omegle': 5182, 'cts': 5183, 'nails': 5184, 'presently': 5185, 'introverted': 5186, 'respected': 5187, 'rat': 5188, 'officejet': 5189, '4620': 5190, 'airprint': 5191, 'cafe': 5192, 'motherboard': 5193, 'smooth': 5194, 'stands': 5195, 'airline': 5196, 'baggage': 5197, 'stealing': 5198, 'jquery': 5199, 'myanmar': 5200, 'rises': 5201, 'backend': 5202, 'rooted': 5203, 'dividing': 5204, 'publishing': 5205, 'shrink': 5206, 'cruel': 5207, 'fourth': 5208, 'storm': 5209, 'workers': 5210, 'penalties': 5211, 'mid': 5212, 'pneumonia': 5213, 'mainstream': 5214, 'formation': 5215, 'undergo': 5216, 'walker': 5217, 'enforcement': 5218, 'theoretical': 5219, 'vermont': 5220, 'wing': 5221, 'warehouse': 5222, 'irish': 5223, 'violin': 5224, 'threat': 5225, 'meter': 5226, 'purposes': 5227, 'seventh': 5228, 'elder': 5229, 'sweat': 5230, '40k': 5231, 'ants': 5232, 'json': 5233, 'occurred': 5234, 't-mobile': 5235, 'conservatives': 5236, 'modelling': 5237, 'mt': 5238, 'tip': 5239, 'semen': 5240, 'streams': 5241, 'ifs': 5242, 'netherlands': 5243, 'excuses': 5244, 'casual': 5245, 'boston': 5246, 'terrible': 5247, 'israeli': 5248, 'castle': 5249, 'pretend': 5250, 'jersey': 5251, 'cms': 5252, 'dynamic': 5253, 'judges': 5254, 'borough': 5255, 'non-fiction': 5256, 'themes': 5257, 'quitting': 5258, 'joint': 5259, 'ide': 5260, 'replies': 5261, 'hiding': 5262, 'democrat': 5263, 'nearest': 5264, 'workplace': 5265, 'respiration': 5266, 'phantom': 5267, 'baratheon': 5268, 'oriented': 5269, 'rails': 5270, 'reflection': 5271, 'toward': 5272, '10k': 5273, 'vlsi': 5274, 'ktm': 5275, 'builder': 5276, 'sunglasses': 5277, 'flute': 5278, 'dealer': 5279, 'supplements': 5280, 'difficulties': 5281, 'pop': 5282, 'craft': 5283, 'self-esteem': 5284, 'simulation': 5285, 'observe': 5286, 'pa': 5287, 'scrap': 5288, 'supporter': 5289, '50000': 5290, 'determines': 5291, 'controversial': 5292, 'ib': 5293, 'vegans': 5294, 'humping': 5295, 'learnt': 5296, 'clearing': 5297, 'drilling': 5298, 't-shirts': 5299, 'withdrawal': 5300, 'bullying': 5301, 'translated': 5302, 'fate': 5303, 'butt': 5304, 'belt': 5305, 'silk': 5306, 'authentic': 5307, 'cubic': 5308, 'vladimir': 5309, 'submitted': 5310, 'diseases': 5311, 'antibiotics': 5312, 'broker': 5313, '400': 5314, 'ebooks': 5315, 'cough': 5316, 'dentist': 5317, 'tries': 5318, 'peru': 5319, 'tweets': 5320, 'lane': 5321, 'situations': 5322, 'heroin': 5323, 'contributing': 5324, 'tape': 5325, 'photographer': 5326, 'olive': 5327, 'drops': 5328, 'alert': 5329, 'padukone': 5330, 'baseball': 5331, 'passports': 5332, 'vinci': 5333, 'labrador': 5334, 'boredom': 5335, 'strings': 5336, 'cdma': 5337, 'experts': 5338, 'pakistanis': 5339, 'polish': 5340, 'drift': 5341, 'senator': 5342, 'minerals': 5343, 'yield': 5344, 'trivia': 5345, 'rectifier': 5346, 'icon': 5347, 'healthier': 5348, 'truck': 5349, 'raj': 5350, 'imei': 5351, 'pgdm': 5352, '--': 5353, 'seek': 5354, 'receiver': 5355, 'julian': 5356, 'alike': 5357, 'rush': 5358, 'dictator': 5359, 'amplifier': 5360, 'chile': 5361, 'tobacco': 5362, 'missiles': 5363, 'invested': 5364, 'experiments': 5365, 'mario': 5366, 'initiate': 5367, 'filipinos': 5368, 'aakash': 5369, 'mask': 5370, 'kinetic': 5371, 'corporation': 5372, 'truths': 5373, 'fetish': 5374, 'explore': 5375, 'professionally': 5376, 'skilled': 5377, 'marathon': 5378, 'quran': 5379, 'indore': 5380, 'sunday': 5381, 'demonetizing': 5382, 'unbiased': 5383, 'argument': 5384, 'displacement': 5385, 'lifetime': 5386, 'leadership': 5387, 'bitcoins': 5388, 'breathing': 5389, 'bright': 5390, 'somehow': 5391, 'kitchener': 5392, 'pipe': 5393, 'arbitration': 5394, 'fool': 5395, 'freight': 5396, 'tension': 5397, 'pluto': 5398, 'papua': 5399, 'gambling': 5400, 'manufacturer': 5401, 'overwatch': 5402, 'fonts': 5403, 'recognized': 5404, 'sponsor': 5405, 't20': 5406, 'alexander': 5407, 'indicate': 5408, 'sorry': 5409, 'sworn': 5410, 'swallow': 5411, 'photographic': 5412, 'negativity': 5413, 'realty': 5414, 'ka': 5415, 'toyota': 5416, 'thousands': 5417, '8th': 5418, 'labs': 5419, 'coursera': 5420, 'pulled': 5421, 'inspiration': 5422, 'hrs': 5423, 'politically': 5424, 'breath': 5425, 'batsman': 5426, 'ate': 5427, 'circulation': 5428, 'thinks': 5429, 'castes': 5430, 'hangover': 5431, 'pitch': 5432, 'telescope': 5433, 'quantity': 5434, 'correctly': 5435, 'googling': 5436, 'curved': 5437, 'publicly': 5438, 'traveler': 5439, 'universes': 5440, 'expectations': 5441, 'household': 5442, '31': 5443, 'b2b': 5444, 'coal': 5445, 'locate': 5446, 'bear': 5447, 'bios': 5448, 'parameters': 5449, 'duties': 5450, 'matching': 5451, 'bullets': 5452, 'fed': 5453, 'aging': 5454, 'zenfone': 5455, 'roommate': 5456, 'sciences': 5457, 'synonyms': 5458, 'recorded': 5459, 'waking': 5460, 'sweating': 5461, 'adsense': 5462, 'entered': 5463, 'signed': 5464, 'technological': 5465, 'uc': 5466, 'prototype': 5467, 'retain': 5468, 'rum': 5469, 'slightly': 5470, 'welfare': 5471, 'bars': 5472, 'writes': 5473, 'medication': 5474, 'passes': 5475, 'respectively': 5476, 'utah': 5477, 'odds': 5478, 'stones': 5479, 'murdered': 5480, 'piercing': 5481, 'regardless': 5482, 'lifting': 5483, 'helping': 5484, 'trader': 5485, 'neutrons': 5486, '2007': 5487, 'catholics': 5488, 'patient': 5489, 'rocket': 5490, 'oily': 5491, 'traveled': 5492, 'django': 5493, 'dialogues': 5494, 'separately': 5495, 'las': 5496, 'budapest': 5497, 'females': 5498, 'arthritis': 5499, 'adolf': 5500, 'filipino': 5501, 'boiler': 5502, 'oxidation': 5503, 'arkansas': 5504, 'bathroom': 5505, '19th': 5506, 'neither': 5507, 'switching': 5508, 'ships': 5509, 'iiit': 5510, 'darker': 5511, 'perpetual': 5512, 'polar': 5513, 'geometry': 5514, 'complain': 5515, 'sulfate': 5516, '3s': 5517, 'kali': 5518, 'modeling': 5519, 'alibaba': 5520, 'kota': 5521, 'wrist': 5522, 'proxy': 5523, 'sexy': 5524, 'detective': 5525, 'victoria': 5526, 'lion': 5527, 'jr.': 5528, 'sand': 5529, 'plural': 5530, 'liar': 5531, 'severity': 5532, 'corner': 5533, 'thigh': 5534, 'articleship': 5535, 'warfare': 5536, 'eligibility': 5537, 'compose': 5538, 'task': 5539, 'avg': 5540, 'victims': 5541, 'applicable': 5542, 'twitching': 5543, 'studios': 5544, 'kilometers': 5545, 'annum': 5546, '2016-17': 5547, 'happiest': 5548, 'renewable': 5549, 'potus': 5550, 'helicopter': 5551, 'believing': 5552, 'carbonate': 5553, 'manhattan': 5554, 'curbing': 5555, 'preferably': 5556, 'procedures': 5557, 'ao': 5558, 'llb': 5559, 'hills': 5560, 'infrastructure': 5561, 'locked': 5562, 'edm': 5563, 'geographical': 5564, 'nehru': 5565, 'hated': 5566, 'razor': 5567, 'puberty': 5568, 'hitting': 5569, 'spider': 5570, 'buddha': 5571, 'ill': 5572, 'iranian': 5573, 'tom': 5574, 'charity': 5575, 'nevada': 5576, 'disaster': 5577, 'venus': 5578, 'establish': 5579, 'billions': 5580, 'involving': 5581, 'poverty': 5582, 'ministers': 5583, 'tweet': 5584, 'cider': 5585, '12000': 5586, 'patna': 5587, 'portfolio': 5588, 'symbiosis': 5589, 'pok': 5590, 'stopping': 5591, 'diode': 5592, 'staying': 5593, 'beaches': 5594, 'drawings': 5595, 'seller': 5596, 'hangout': 5597, 'deer': 5598, 'northern': 5599, 'lay': 5600, 'mermaids': 5601, 'wa': 5602, 'ur': 5603, 'selenium': 5604, 'horrible': 5605, 'captured': 5606, 'kumar': 5607, 'wharton': 5608, 'proved': 5609, 'automotive': 5610, 'taboo': 5611, 'occurs': 5612, 'leg': 5613, 'throws': 5614, 'suspension': 5615, 'desk': 5616, 'aside': 5617, 'yadav': 5618, 'coder': 5619, 'dose': 5620, 'brains': 5621, 'homemade': 5622, 'liver': 5623, 'soldier': 5624, 'boxing': 5625, 'pics': 5626, 'bottles': 5627, 'dissolve': 5628, 'institution': 5629, 'memorable': 5630, 'logged': 5631, 'backward': 5632, 'fm': 5633, 'busy': 5634, 'buffer': 5635, 'stance': 5636, 'exposure': 5637, 'secretly': 5638, 'earphone': 5639, 'ctc': 5640, 'demonitization': 5641, 'prevented': 5642, 'atheism': 5643, 'antenna': 5644, 'odd': 5645, 'argue': 5646, 'gems': 5647, 'ir': 5648, 'polyester': 5649, 'possibilities': 5650, 'shepherd': 5651, 'welding': 5652, 'jailbreaking': 5653, 'posters': 5654, 'kmc': 5655, 'bestfriend': 5656, 'ignoring': 5657, 'vellore': 5658, 'shahrukh': 5659, 'womb': 5660, 'organisation': 5661, 'monster': 5662, 'queens': 5663, 'andhra': 5664, 'demonetize': 5665, 'distributed': 5666, 'touches': 5667, 'garbage': 5668, 'fasting': 5669, 'imminent': 5670, '3ds': 5671, 'glucose': 5672, 'abilities': 5673, 'indicator': 5674, 'hairfall': 5675, 'ki': 5676, 'legacy': 5677, 'gateway': 5678, 'genders': 5679, 'cooked': 5680, 'bootcamp': 5681, 'rac': 5682, 'astrologer': 5683, 'behavioral': 5684, 'magnets': 5685, 'opera': 5686, 'composition': 5687, 'dizzy': 5688, 'creatures': 5689, 'elective': 5690, 'missed': 5691, 'roads': 5692, 'patients': 5693, 'lately': 5694, 'testament': 5695, 'wells': 5696, 'fargo': 5697, 'insight': 5698, 'imports': 5699, '2g': 5700, 'moocs': 5701, 'ass': 5702, 'societies': 5703, 'erp': 5704, 'theoretically': 5705, 'assume': 5706, 'mixing': 5707, 'domains': 5708, 'são': 5709, 'laos': 5710, 'basically': 5711, 'weighing': 5712, 'thighs': 5713, 'violent': 5714, 'encountered': 5715, 'peaceful': 5716, '65': 5717, 'essays': 5718, 'tendulkar': 5719, 'nicest': 5720, 'footballer': 5721, 'fetch': 5722, 'socialist': 5723, 'viewing': 5724, 'divorced': 5725, 'flavors': 5726, 'cersei': 5727, 'personalities': 5728, 'spine': 5729, 'poop': 5730, 'enthalpy': 5731, 'xperia': 5732, 'taxi': 5733, 'checked': 5734, 'thief': 5735, 'cookie': 5736, 'drawbacks': 5737, 'reactions': 5738, 'volkswagen': 5739, '2006': 5740, 'dutch': 5741, 'dairy': 5742, 'maybe': 5743, 've': 5744, 'plasma': 5745, 'indus': 5746, 'massachusetts': 5747, 'cet': 5748, 'tvf': 5749, 'earns': 5750, 'yearly': 5751, 'historians': 5752, 'churchill': 5753, 'literary': 5754, 'inspiron': 5755, 'diffusion': 5756, 'secular': 5757, 'colored': 5758, 'pickup': 5759, 'appetite': 5760, 'computational': 5761, 'peoples': 5762, 'beta': 5763, 'microbiology': 5764, 'smallest': 5765, 'appreciate': 5766, '1962': 5767, 'lanka': 5768, 'pirate': 5769, 'survival': 5770, 'harder': 5771, 'paradox': 5772, 'yesterday': 5773, 'template': 5774, 'ovulation': 5775, 'confirm': 5776, 'ross': 5777, 'trash': 5778, 'premature': 5779, 'stats': 5780, 'mathematician': 5781, 'medal': 5782, 'favor': 5783, 'served': 5784, 'az': 5785, 'purifier': 5786, 'adopted': 5787, 'hydroxide': 5788, 'regression': 5789, 'armor': 5790, 'obey': 5791, 'distilled': 5792, 'slaves': 5793, 'ballot': 5794, 'acrylic': 5795, 'is/are': 5796, 'liability': 5797, 'invitation': 5798, 'inertia': 5799, 'administrator': 5800, 'hospitality': 5801, 'existential': 5802, 'randomly': 5803, 'aspire': 5804, 'udemy': 5805, 'q': 5806, 'coupon': 5807, 'races': 5808, 'interact': 5809, 'yeast': 5810, 'tails': 5811, 'k3': 5812, 'spoiler': 5813, 'nested': 5814, 'ecosystem': 5815, 'sundar': 5816, 'inappropriate': 5817, 'knife': 5818, 'prevalent': 5819, 'readers': 5820, 'maruti': 5821, 'ali': 5822, 'karate': 5823, 'inexpensive': 5824, 'stupidest': 5825, 'st': 5826, 'lump': 5827, 'interviewing': 5828, 'suzuki': 5829, 'mr': 5830, 'sandwich': 5831, 'subway': 5832, 'teachings': 5833, 'todays': 5834, 'mongodb': 5835, 'arrays': 5836, 'recursion': 5837, 'weights': 5838, 'banana': 5839, 'buddhists': 5840, 'vary': 5841, 'misconceptions': 5842, 'tortured': 5843, 'valuation': 5844, 'viber': 5845, 'headaches': 5846, 'patience': 5847, 'ceiling': 5848, 'portals': 5849, 'fishing': 5850, 'personally': 5851, 'moons': 5852, 'extraterrestrial': 5853, 'ambassador': 5854, 'ongc': 5855, 'syntax': 5856, 'surveys': 5857, 'abolished': 5858, 'costco': 5859, 'movements': 5860, 'brazilian': 5861, 'homosexual': 5862, 'painted': 5863, 'potassium': 5864, 'ion': 5865, 'transferred': 5866, 'traction': 5867, 'prisoners': 5868, 'releasing': 5869, 'deloitte': 5870, 'reducing': 5871, 'architects': 5872, 'bi': 5873, 'detail': 5874, 'promise': 5875, 'tonight': 5876, 'hd': 5877, 'sectors': 5878, 'snooker': 5879, 'abolishing': 5880, 'superior': 5881, 'brisbane': 5882, \"d'angelo\": 5883, 'tanks': 5884, 'telegram': 5885, 'frequencies': 5886, 'flags': 5887, 'little-known': 5888, 'pieces': 5889, 'knock': 5890, 'logistics': 5891, 'hospitals': 5892, 'planes': 5893, 'pes': 5894, 'attacking': 5895, 'reveal': 5896, 'bolt': 5897, 'malayalam': 5898, 'mandatory': 5899, 'attacked': 5900, 'bearing': 5901, 'seemingly': 5902, 'connections': 5903, 'achievement': 5904, 'buddhism': 5905, 'timing': 5906, 'colonized': 5907, 'capture': 5908, 'sized': 5909, 'brings': 5910, 'pit': 5911, 'detroit': 5912, 'ourselves': 5913, 'voldemort': 5914, 'b.e': 5915, 'refused': 5916, 'admin': 5917, 'spray': 5918, 'ancestry': 5919, 'underground': 5920, 'deviation': 5921, 'encounter': 5922, 'kings': 5923, 'tab': 5924, 'customs': 5925, 'flux': 5926, 'horse': 5927, 'coin': 5928, 'suggestion': 5929, 'mortgage': 5930, 'beds': 5931, 'rocks': 5932, 'sims': 5933, 'deeper': 5934, 'passenger': 5935, 'leo': 5936, 'giant': 5937, 'pulsar': 5938, 'warcraft': 5939, 'processes': 5940, 'wavelength': 5941, 'banerjee': 5942, 'multi': 5943, 'healing': 5944, 'measuring': 5945, 'believed': 5946, 'thicker': 5947, 'monkeys': 5948, 'min': 5949, 'austria': 5950, 'president-elect': 5951, 'principal': 5952, 'holidays': 5953, 'erase': 5954, 'probable': 5955, 'fps': 5956, 'sheets': 5957, 'consumers': 5958, 'texted': 5959, '1.': 5960, 'pearl': 5961, 'slower': 5962, 'dumbledore': 5963, 'mankind': 5964, 'imagine': 5965, 'interns': 5966, 'bee': 5967, 'alarm': 5968, 'wheels': 5969, 'directions': 5970, 'shooting': 5971, 'complicated': 5972, 'merit': 5973, 'regard': 5974, 'completion': 5975, 'clinical': 5976, 'hardly': 5977, 'votes': 5978, 'lexus': 5979, 'origins': 5980, 'er': 5981, 'intense': 5982, 'salad': 5983, 'brass': 5984, 'airports': 5985, 'patents': 5986, 'turnover': 5987, 'pill': 5988, 'charles': 5989, 'bubbles': 5990, 'posture': 5991, 'behaviour': 5992, 'volcanoes': 5993, 'greenlit': 5994, 'backstory': 5995, 'nerve': 5996, 'sounding': 5997, 'opine': 5998, 'mercury': 5999, 'references': 6000, 'shanghai': 6001, 'granville': 6002, 'bare': 6003, 'labour': 6004, 'facilities': 6005, 'microphone': 6006, 'high-end': 6007, 'assumptions': 6008, 'males': 6009, 'reserved': 6010, 'benghazi': 6011, 'poland': 6012, 'franklin': 6013, 'dated': 6014, 'maker': 6015, 'ripped': 6016, 'honeywell': 6017, 'stalking': 6018, 'kidney': 6019, 'ruin': 6020, 'calorie': 6021, 'spicy': 6022, 'rib': 6023, 'silent': 6024, 'comeback': 6025, 'postal': 6026, 'ingredients': 6027, 'tube': 6028, 'vendors': 6029, '2011': 6030, 'asset': 6031, 'overdose': 6032, 'lick': 6033, 'ernest': 6034, 'nafta': 6035, 'asperger': 6036, 'sniper': 6037, 'shocking': 6038, 'zip': 6039, 'linked': 6040, 'sc/st': 6041, 'unix': 6042, 'dx': 6043, 'brake': 6044, 'hungry': 6045, 'paintings': 6046, 'staring': 6047, 'je': 6048, 'addition': 6049, 'extended': 6050, 'civic': 6051, 'converting': 6052, 'rows': 6053, 'moved': 6054, 'revit': 6055, 'composed': 6056, 'matches': 6057, 'badminton': 6058, 'bing': 6059, 'uninstall': 6060, 'continent': 6061, 'spaceships': 6062, 'bills': 6063, 'sued': 6064, 'albert': 6065, 'colombia': 6066, 'approval': 6067, 'dictionary': 6068, 'kills': 6069, 'licenses': 6070, 'compete': 6071, 'imo': 6072, 'addresses': 6073, 'hindi/urdu': 6074, 'workouts': 6075, 'harvey': 6076, 'partial': 6077, 'nyu': 6078, 'marketer': 6079, 'advertisement': 6080, 'restraining': 6081, 'hawking': 6082, 'makers': 6083, 'rs.500': 6084, 'fingerprint': 6085, 'audit': 6086, 'panthers': 6087, 'bpo': 6088, 'electors': 6089, 'anthropology': 6090, 'poorly': 6091, 'exciting': 6092, 'pays': 6093, 'midwest': 6094, 'committee': 6095, 'rides': 6096, 'fathers': 6097, 'producers': 6098, 'courts': 6099, 'long-term': 6100, 'bench': 6101, 'fingerprints': 6102, 'kansas': 6103, 'physicists': 6104, 'itchy': 6105, 'ghana': 6106, 'asthma': 6107, 'leads': 6108, 'co-founder': 6109, 'cisco': 6110, 'powered': 6111, 'revolutionary': 6112, 'singularity': 6113, 'thor': 6114, '57': 6115, 'steering': 6116, 'iso': 6117, 'performing': 6118, 'approximately': 6119, 'chloride': 6120, 'refuses': 6121, 'gluten': 6122, 'singular': 6123, 'forbidden': 6124, 'southeast': 6125, 'mileage': 6126, 'representation': 6127, 'cigarette': 6128, 'wayne': 6129, 'circular': 6130, 'burns': 6131, 'absolutely': 6132, 'laugh': 6133, 'accommodation': 6134, 'phrases': 6135, 'stares': 6136, 'honor': 6137, 'socket': 6138, 'manufactured': 6139, 'compiler': 6140, 'sight': 6141, 'arrest': 6142, 'guru': 6143, 'leak': 6144, 'oneplus': 6145, 'chuck': 6146, 'skywalker': 6147, 'strengthen': 6148, 'ego': 6149, 'battles': 6150, 'vertical': 6151, 'ordinary': 6152, 'dumps': 6153, 'reactive': 6154, 'ngos': 6155, 'referring': 6156, 'apollo': 6157, 'lesser-known': 6158, 'sights': 6159, 'crooked': 6160, 'consumed': 6161, 'alto': 6162, 'soil': 6163, 'oscar': 6164, 'obviously': 6165, 'theorem': 6166, 'expire': 6167, 'hostels': 6168, 'hilary': 6169, 'missile': 6170, 'elderly': 6171, 'wallpapers': 6172, 'front-end': 6173, 'copyrighted': 6174, 'disability': 6175, 'propaganda': 6176, 'infected': 6177, 'indigenous': 6178, 'plugged': 6179, 'edx': 6180, 'speeding': 6181, 'backed': 6182, 'showcase': 6183, 'dealing': 6184, 'therapeutics': 6185, 'dominate': 6186, 'well-known': 6187, 'selfies': 6188, 'affairs': 6189, 'blackheads': 6190, 'start-up': 6191, 'chair': 6192, 'founded': 6193, 'streak': 6194, 'cos': 6195, 'migrate': 6196, 'xii': 6197, 'feminist': 6198, 'inverse': 6199, 'mattress': 6200, 'pigs': 6201, 'mcdonalds': 6202, 'disconnect': 6203, 'guess': 6204, 'supported': 6205, 'checks': 6206, 'hot-beds': 6207, 'procedural': 6208, 'lotion': 6209, 'acca': 6210, 'whale': 6211, 'clerk': 6212, 'protecting': 6213, 'infatuation': 6214, 'beatles': 6215, 'pimple': 6216, 'relocate': 6217, 'ufc': 6218, 'alliance': 6219, 'antimatter': 6220, 'vishwanathan': 6221, 'temporary': 6222, 'negatively': 6223, 'loans': 6224, 'portable': 6225, 'invasion': 6226, 'bhagavad': 6227, 'uploaded': 6228, 'satan': 6229, 'opt': 6230, 'shuttle': 6231, 'scotland': 6232, 'beans': 6233, 'warriors': 6234, 'satisfy': 6235, 'defeated': 6236, 'michigan': 6237, 'detox': 6238, 'momentum': 6239, 'toe': 6240, 'boom': 6241, 'magnesium': 6242, 'illustrator': 6243, 'sa1': 6244, 'thin': 6245, 'taylor': 6246, 'assets': 6247, 'dylan': 6248, 'handsome': 6249, 'certifications': 6250, 'inauguration': 6251, 'invaded': 6252, 'combined': 6253, 'bots': 6254, 'isi': 6255, 'killer': 6256, 'seminar': 6257, 'magento': 6258, 'berkeley': 6259, 'india-pakistan': 6260, '100,000': 6261, 'flame': 6262, 'arc': 6263, 'raped': 6264, 'renaissance': 6265, 'plug': 6266, 'unrequited': 6267, 'packs': 6268, 'february': 6269, 'rounds': 6270, 'reduction': 6271, 'hamilton': 6272, 'ee': 6273, 'otg': 6274, 'brahmin': 6275, 'engg': 6276, 'phenol': 6277, 'muay': 6278, 'visas': 6279, 'legalized': 6280, 'ba': 6281, 'converts': 6282, 'heap': 6283, '144': 6284, 'nomination': 6285, 'shadow': 6286, 'module': 6287, 'motorola': 6288, '19-year-old': 6289, 'nd': 6290, 'iris': 6291, 'narcissistic': 6292, 'etiquette': 6293, 'kurukshetra': 6294, 'toothpaste': 6295, 'mineral': 6296, 'amounts': 6297, 'compression': 6298, '1-2': 6299, 'rabies': 6300, 'chats': 6301, 'accelerator': 6302, 'affleck': 6303, 'creed': 6304, 'listing': 6305, 'utilize': 6306, 'hana': 6307, 'generations': 6308, 'champions': 6309, 'rope': 6310, 'walter': 6311, 'ultra': 6312, 'profits': 6313, 'drowning': 6314, 'customized': 6315, 'mcdonald': 6316, 'banker': 6317, 'madras': 6318, 'programing': 6319, 'feminine': 6320, 'fleas': 6321, 'x^': 6322, 'sensors': 6323, 'evernote': 6324, 'methamphetamine': 6325, 'otherwise': 6326, 'hunter': 6327, 'greece': 6328, 'enabled': 6329, 'asteroid': 6330, 'flower': 6331, 'anemia': 6332, 'covering': 6333, 'remorse': 6334, 'severe': 6335, 'fusion': 6336, 'emulator': 6337, 'digest': 6338, 'gt': 6339, 'patel': 6340, 'hobbit': 6341, 'mooc/e-learning': 6342, 'filmmaking': 6343, 'subcontinent': 6344, 'dictatorship': 6345, 'a+': 6346, 'soup': 6347, 'raspberry': 6348, 'textile': 6349, 'northeast': 6350, 'responses': 6351, 'sofa': 6352, 'returning': 6353, 'elect': 6354, 'viswanathan': 6355, 'fitbit': 6356, 'part-time': 6357, 'vaginal': 6358, 'suited': 6359, 'ta': 6360, 'telephone': 6361, 'demat': 6362, 'unemployed': 6363, 'architectural': 6364, 'ratings': 6365, 'betrayed': 6366, 'g3': 6367, 'harsh': 6368, 'housing': 6369, 'comfortably': 6370, 'soulmate': 6371, 'exports': 6372, 'feasible': 6373, 'emi': 6374, 'methodology': 6375, 'rays': 6376, 'jpeg': 6377, 'jbl': 6378, 'infection': 6379, 'admire': 6380, 'desperate': 6381, 'napoleon': 6382, 'quite': 6383, 'pig': 6384, 'tomé': 6385, 'príncipe': 6386, 'awake': 6387, 'bananas': 6388, 'roku': 6389, 'disturbing': 6390, 'fats': 6391, 'mainly': 6392, 'bases': 6393, 'isaac': 6394, 'podcasts': 6395, 'rabbit': 6396, 'accused': 6397, 'fails': 6398, 'cortex': 6399, '410': 6400, 'pcb': 6401, 'fantastic': 6402, 'reforms': 6403, 'grader': 6404, 'deepest': 6405, 'decade': 6406, 'rumors': 6407, 'bio': 6408, 'poison': 6409, 'encourage': 6410, 'lets': 6411, 'martian': 6412, 'laserjet': 6413, 'bites': 6414, 'spite': 6415, 'puzzles': 6416, 'nissan': 6417, 'proofs': 6418, 'uberx': 6419, 'calculating': 6420, 'impacts': 6421, 'erectile': 6422, 'dysfunction': 6423, 'clever': 6424, 'steal': 6425, 'shia': 6426, 'lymph': 6427, 'nodes': 6428, 'pizzas': 6429, 'nine': 6430, 'wound': 6431, 'didnt': 6432, 'handwriting': 6433, 'suicidal': 6434, 'airliner': 6435, 'bat': 6436, 'watts': 6437, 'spiders': 6438, 'filters': 6439, 'washed': 6440, 'monoxide': 6441, 'trending': 6442, 'favour': 6443, 'jallikattu': 6444, 'raipur': 6445, 'invent': 6446, 'clan': 6447, 'corporations': 6448, 'rcc': 6449, 'musical.ly': 6450, 'dependent': 6451, 'federer': 6452, 'hanging': 6453, 'falls': 6454, 'headlights': 6455, 'orders': 6456, 'distinct': 6457, 'ethnicities': 6458, '5k': 6459, 'olds': 6460, 'wont': 6461, 'pocket': 6462, 'mobiles': 6463, 'trustworthy': 6464, 'ryan': 6465, 'horizon': 6466, 'emerging': 6467, 'nokia': 6468, 'boyfriends': 6469, 'replacing': 6470, 'alphabet': 6471, 'committing': 6472, 'bears': 6473, 'reasonably': 6474, 'prosperity': 6475, 'matters': 6476, 'workflow': 6477, 'mormon': 6478, 'pixel': 6479, 'advisors': 6480, 'highlands': 6481, 'scotch': 6482, 'honors': 6483, 'embassy': 6484, 'decreasing': 6485, 'destinations': 6486, 'debates': 6487, 'repeatedly': 6488, 'gradient': 6489, 'descent': 6490, 'timeline': 6491, 'purchases': 6492, 'hsc': 6493, 'fortune': 6494, 'entity': 6495, 'rogue': 6496, 'mma': 6497, 'compounds': 6498, 'guaranteed': 6499, 'non-profit': 6500, 'initially': 6501, 'akbar': 6502, 'repeated': 6503, 'accomplishments': 6504, 'reader': 6505, 'waterproof': 6506, 'mg': 6507, 'culturally': 6508, 'specifications': 6509, 'slip': 6510, 'poorest': 6511, 'letting': 6512, '250': 6513, '600': 6514, 'exhaust': 6515, 'melania': 6516, 'metabolism': 6517, 'shipped': 6518, 'rarely': 6519, 'amity': 6520, 'bangkok': 6521, 'remaining': 6522, 'biotic': 6523, 'abiotic': 6524, 'ranchi': 6525, 'manner': 6526, 'stamps': 6527, 'maturity': 6528, 'collectively': 6529, 'grams': 6530, 'hotspot': 6531, 'diversity': 6532, 'rapper': 6533, 'clusters': 6534, 'celsius': 6535, 'consent': 6536, 'bhutan': 6537, 'puzzle': 6538, 'viable': 6539, 'fungi': 6540, 'eight': 6541, 'distant': 6542, 'accelerate': 6543, 'ratna': 6544, 'meiosis': 6545, 'actor/actress': 6546, 'lgbt': 6547, 'whisky': 6548, 'sins': 6549, 'fallen': 6550, 'mrs.': 6551, 'sunny': 6552, 'leone': 6553, 'panels': 6554, 'furious': 6555, 'charismatic': 6556, 'operate': 6557, 'repel': 6558, 'myntra': 6559, 'deadpool': 6560, 'rainbow': 6561, 'junk': 6562, 'lamb': 6563, 'endoplasmic': 6564, 'reticulum': 6565, 'libya': 6566, 'buried': 6567, '99': 6568, 'interviewed': 6569, 'incorrect': 6570, 'honestly': 6571, 'meditate': 6572, 'demo': 6573, 'keyword': 6574, 'sunlight': 6575, 'monitors': 6576, 'sheldon': 6577, 'collide': 6578, 'mindset': 6579, 'animations': 6580, 'abused': 6581, 'tornado': 6582, 'producing': 6583, 'lip': 6584, 'orchestra': 6585, 'nearly': 6586, 'regional': 6587, 'attempting': 6588, 'newest': 6589, 'quicker': 6590, 'vikings': 6591, 'doesnt': 6592, 'lisa': 6593, '=1': 6594, 'distracted': 6595, 'blender': 6596, 'projected': 6597, 'philosophers': 6598, 'heritage': 6599, 'cosmic': 6600, 'nerd': 6601, 'bathing': 6602, 'wd': 6603, 'click': 6604, 'prostitutes': 6605, 'mountains': 6606, 'intention': 6607, 'bath': 6608, 'yamaha': 6609, 'painters': 6610, 'frequent': 6611, 'directed': 6612, 'canvas': 6613, 'aipmt': 6614, '3-4': 6615, 'formatting': 6616, 'recall': 6617, 'libra': 6618, 'racing': 6619, 'prokaryotic': 6620, 'extraction': 6621, 'desserts': 6622, '3000': 6623, '7000': 6624, 'expressing': 6625, 'insects': 6626, 'diverse': 6627, 'pulling': 6628, 'nus': 6629, 'majoring': 6630, 'predictions': 6631, 'pu': 6632, 'aa': 6633, 'toshiba': 6634, 'denied': 6635, 'destruction': 6636, 'si': 6637, 'injection': 6638, 'phobia': 6639, 'unhappy': 6640, 'spacecraft': 6641, 'rider': 6642, '30000': 6643, 'ren': 6644, 'sole': 6645, 'folders': 6646, 'co2': 6647, 'reconstruction': 6648, 'scorpion': 6649, 'frustrating': 6650, 'slack': 6651, 'enforce': 6652, 'delhi/ncr': 6653, 'appointed': 6654, 'ions': 6655, 'bully': 6656, 'prostitute': 6657, '60000': 6658, 'woah': 6659, 'recipient': 6660, 'schemes': 6661, 'bronze': 6662, 'erotic': 6663, 'tony': 6664, 'quadratic': 6665, 'baraka': 6666, 'reserves': 6667, 'grief': 6668, 'taxed': 6669, 'hons': 6670, 'hairstyle': 6671, 'clat': 6672, '40000': 6673, 'allo': 6674, 'packets': 6675, 'rapidly': 6676, 'tyres': 6677, 'dancer': 6678, 'balochistan': 6679, 'marking': 6680, 'conducted': 6681, 'worlds': 6682, 'wheat': 6683, 'intending': 6684, 'operator': 6685, 'montreal': 6686, 'ammonia': 6687, 'dancing': 6688, 'singaporeans': 6689, 'nickname': 6690, 'w/o': 6691, 'adopt': 6692, 'linguistics': 6693, 'premiere': 6694, 'suppose': 6695, 'arrogant': 6696, 'maximize': 6697, 'currencies': 6698, 'italians': 6699, 'boots': 6700, 'samples': 6701, 'continental': 6702, 'blogger': 6703, 'lovers': 6704, 'dongle': 6705, 'shri': 6706, 'istanbul': 6707, 'governance': 6708, 'march': 6709, 'shoulders': 6710, 'chapters': 6711, 'quiz': 6712, 'cnn': 6713, 'c/c++': 6714, 'reheat': 6715, 'hunger': 6716, 'anniversary': 6717, 'submarine': 6718, 'prisons': 6719, 'booked': 6720, 'pot': 6721, 'uscis': 6722, 'stairs': 6723, 'mahindra': 6724, 'oppose': 6725, 'artery': 6726, 'contest': 6727, 'coverage': 6728, 'minded': 6729, 'challenged': 6730, 'viewer': 6731, 'idle': 6732, 'arabs': 6733, 'repercussions': 6734, 'blowjob': 6735, 'prepaid': 6736, 'aspect': 6737, 'tigers': 6738, 'attorney': 6739, 'polite': 6740, 'emojis': 6741, 'insanity': 6742, 'h2o': 6743, 'dioxide': 6744, 'rifles': 6745, 'humor': 6746, 'obligations': 6747, 'jnu': 6748, 'screens': 6749, 'backlog': 6750, 'logos': 6751, 'portrayed': 6752, 'carriers': 6753, 'discrimination': 6754, 'slogan': 6755, 'wondering': 6756, 'cricketers': 6757, 'validity': 6758, '42': 6759, 'opponent': 6760, 'jumping': 6761, 'village': 6762, 'tablets': 6763, 'zones': 6764, 'frost': 6765, 'trains': 6766, 'routes': 6767, 'cow': 6768, 'achieved': 6769, 'conquer': 6770, 'docs': 6771, 'moisturizer': 6772, 'scripted': 6773, 'gene': 6774, 'lincoln': 6775, 'eukaryotic': 6776, 'plates': 6777, 'explained': 6778, 'penetration': 6779, 'span': 6780, 'yo': 6781, '95': 6782, 'blown': 6783, 'deny': 6784, 'gloves': 6785, 'cholesterol': 6786, 'blades': 6787, 'connecting': 6788, 'carolina': 6789, 'economically': 6790, 'suspense': 6791, '110': 6792, 'herbalife': 6793, 'conquered': 6794, 'creates': 6795, 'deserts': 6796, 'trips': 6797, 'i7': 6798, 'cpm': 6799, 'bhim': 6800, 'morocco': 6801, 'bros.': 6802, 'dalits': 6803, 'tourists': 6804, 'province': 6805, 'scalar': 6806, 'hub': 6807, 'bulletproof': 6808, 'gon': 6809, 'preschools': 6810, 'polls': 6811, 'royale': 6812, 'tournaments': 6813, 'ukulele': 6814, 'imply': 6815, 'isc': 6816, 'marines': 6817, 'coupons': 6818, 'preference': 6819, 'wooden': 6820, 'branches': 6821, 'fairly': 6822, 'luggage': 6823, 'ramayana': 6824, 'advisor': 6825, 'thermostat': 6826, 'cockroaches': 6827, 'correction': 6828, 'insect': 6829, 'horsepower': 6830, 'realise': 6831, 'ladies': 6832, 'abdomen': 6833, '2/3': 6834, 'constitutional': 6835, 'ordering': 6836, 'norwegian': 6837, 'freelancer': 6838, 'gd': 6839, '25000': 6840, 'requesting': 6841, 'donation': 6842, 'searches': 6843, 'hairs': 6844, 'shampoo': 6845, 'informal': 6846, 'kaine': 6847, 'mike': 6848, 'aspirants': 6849, 'choices': 6850, 'sometime': 6851, '20,000': 6852, 'illusion': 6853, 'organized': 6854, '37': 6855, 'cartoon': 6856, 'sweetest': 6857, 'sulfuric': 6858, 'azerbaijan': 6859, 'casting': 6860, 'directors': 6861, 'watermelon': 6862, 'tribunal': 6863, 'travels': 6864, 'homeopathy': 6865, '6.0': 6866, 'publication': 6867, 'i.e.': 6868, 'shirts': 6869, 'clay': 6870, 'dice': 6871, 'rolled': 6872, 'indonesian': 6873, 'day-to-day': 6874, '3.0': 6875, '220': 6876, 'paradise': 6877, 'swamp': 6878, 'pandora': 6879, 'organize': 6880, 'bleed': 6881, 'ethiopia': 6882, 'ipo': 6883, 'donations': 6884, 'mp4': 6885, 'collaborate': 6886, 'ticks': 6887, '₹2000': 6888, 'hockey': 6889, 'tick': 6890, 'catering': 6891, 'imperial': 6892, 'slap': 6893, 'walls': 6894, 'complexion': 6895, 'synchronous': 6896, 'effort': 6897, 'seals': 6898, 'kendrick': 6899, 'lamar': 6900, 'signing': 6901, 'stated': 6902, 'sunni': 6903, 'physiology': 6904, 'bca': 6905, 'adding': 6906, 'chetan': 6907, 'dtu': 6908, 'spoilers': 6909, 'guitarist': 6910, 'qa': 6911, 'emoji': 6912, 'focused': 6913, 'houston': 6914, 'turbo': 6915, 'vedic': 6916, 'prejudice': 6917, 'musicians': 6918, 'forest': 6919, 'layers': 6920, 'defining': 6921, 'boat': 6922, 'jury': 6923, 'trivandrum': 6924, 'solver': 6925, 'depicted': 6926, 'trailer': 6927, 'previously': 6928, 'guilt': 6929, 'self-confidence': 6930, 'karna': 6931, 'ally': 6932, 'denominations': 6933, 'orbits': 6934, 'survived': 6935, 'sp': 6936, 'milan': 6937, 'inorganic': 6938, 'animes': 6939, 'sushi': 6940, 'shaking': 6941, 'meals': 6942, 'cmi': 6943, 'apocalypse': 6944, 'warning': 6945, 'pins': 6946, 'nearby': 6947, 'alumni': 6948, 'thailand': 6949, 'ethernet': 6950, 'lasting': 6951, 'reminder': 6952, 'cumulative': 6953, 'facility': 6954, 'initiative': 6955, 'farmers': 6956, 'expecting': 6957, 'insult': 6958, 'kernel': 6959, 'athletic': 6960, 'cylinder': 6961, 'privileges': 6962, 'snapchats': 6963, '1080p': 6964, 'lic': 6965, 'loyal': 6966, 'afterlife': 6967, 'km/h': 6968, 'sf': 6969, 'baked': 6970, 'legislative': 6971, 'madrid': 6972, 'ey': 6973, 'travellers': 6974, 'garlic': 6975, 'socks': 6976, 'produces': 6977, 'announce': 6978, 'iit-jee': 6979, '`': 6980, 'curly': 6981, 'construct': 6982, 'bypass': 6983, 'delaware': 6984, 'strangers': 6985, 'lending': 6986, 'jurisdiction': 6987, 'irritating': 6988, 'struggling': 6989, 'amcat': 6990, 'elitmus': 6991, 'demotivated': 6992, 'xml': 6993, '-17': 6994, 'ping': 6995, 'romania': 6996, 'asexual': 6997, 'mercedes': 6998, 'databases': 6999, 'scripts': 7000, 'palo': 7001, 'excess': 7002, 'fest': 7003, 'horizontal': 7004, 'ids': 7005, 'prep': 7006, 'homework': 7007, 'returned': 7008, 'deleting': 7009, 'dakota': 7010, 'procrastinating': 7011, 'pronounced': 7012, 'dominant': 7013, 'cheats': 7014, 'fascism': 7015, 'etf': 7016, 'liberty': 7017, 'webcam': 7018, 'errors': 7019, 'jain': 7020, 'kochi': 7021, 'contracts': 7022, 'warangal': 7023, 'requires': 7024, 'holders': 7025, 'leopard': 7026, 'clinic': 7027, 'elephants': 7028, 'dye': 7029, 'potentially': 7030, 'coca-cola': 7031, 'irrational': 7032, 'magnet': 7033, 'dispute': 7034, 'duck': 7035, 'outstanding': 7036, 'elsewhere': 7037, 'myers-briggs': 7038, 'assigned': 7039, 'collar': 7040, 'follower': 7041, 'sexuality': 7042, 'everywhere': 7043, 'hebrew': 7044, 'garden': 7045, 'wanting': 7046, 'fluency': 7047, 'walt': 7048, 'discrete': 7049, 'cpl': 7050, 'nationality': 7051, 'variety': 7052, 'functioning': 7053, 'specialization': 7054, 'certificates': 7055, 'submitting': 7056, 'immigrant': 7057, 'cargo': 7058, 'begging': 7059, 'carrie': 7060, 'fisher': 7061, 'thus': 7062, 'panda': 7063, 'declaration': 7064, 'settlement': 7065, 'grapes': 7066, 'shades': 7067, 'ssl': 7068, 'spare': 7069, 'yelp': 7070, 'toppers': 7071, 'instruments': 7072, 'fog': 7073, 'helmet': 7074, 'pads': 7075, 'transistor': 7076, 'sociopaths': 7077, 'slang': 7078, 'intentionally': 7079, 'hood': 7080, '7th': 7081, 'lenses': 7082, 'controller': 7083, 'volt': 7084, 'experimental': 7085, 'plagiarism': 7086, 'photographs': 7087, 'carrying': 7088, 'lan': 7089, 'sas': 7090, 'tanning': 7091, 'deserves': 7092, 'undergrad': 7093, 'slogans': 7094, 'arguing': 7095, 'scams': 7096, 'boson': 7097, 'skateboard': 7098, 'accuracy': 7099, 'saas': 7100, 'freezing': 7101, 'pound': 7102, 'oz': 7103, 'compute': 7104, 'observer': 7105, 'cambridge': 7106, 'disc': 7107, 'crowded': 7108, 'peroxide': 7109, 'defend': 7110, 'higgs': 7111, 'bc': 7112, 'brief': 7113, 'amino': 7114, 'lifespan': 7115, 'nits': 7116, 'short-term': 7117, 'elbow': 7118, 'consideration': 7119, 'financing': 7120, 'finnish': 7121, 'swedish': 7122, 'approx': 7123, 'ahead': 7124, 'strain': 7125, 'gram': 7126, 'haiti': 7127, 'bb': 7128, 'occupation': 7129, 'professionals': 7130, 'detector': 7131, 'instances': 7132, 'hotstar': 7133, 'iocl': 7134, 'featured': 7135, 'commonwealth': 7136, 'shippuden': 7137, 'memes': 7138, 'prescribe': 7139, 'topper': 7140, 'engagement': 7141, 'outsource': 7142, 'deck': 7143, 'taco': 7144, 'chandler': 7145, 'lighting': 7146, 'vp': 7147, 'follows': 7148, 'prestigious': 7149, 'nolan': 7150, 'hey': 7151, 'apparel': 7152, 'tone': 7153, 'ibuprofen': 7154, 'akhilesh': 7155, 'globally': 7156, 'biodegradable': 7157, 'missions': 7158, 'scala': 7159, 'chips': 7160, 'kissed': 7161, 'pyramid': 7162, 'macbeth': 7163, 'pressed': 7164, 'haunted': 7165, 'manchester': 7166, 'pointer': 7167, 'hybris': 7168, 'regretted': 7169, 'ease': 7170, 'bent': 7171, 'slot': 7172, 'plain': 7173, 'marker': 7174, '30,000': 7175, 'boxes': 7176, 'rick': 7177, 'coaster': 7178, 'unread': 7179, 'acetic': 7180, 'relatives': 7181, 'deaths': 7182, 'guest': 7183, 'rama': 7184, 'maine': 7185, 'useless': 7186, 'tutor': 7187, 'toilet': 7188, 'celebrated': 7189, 'anchor': 7190, 'keen': 7191, 'mckinsey': 7192, 'mbps': 7193, 'spears': 7194, 'caucasians': 7195, 'comparable': 7196, 'ab': 7197, 'awakening': 7198, 'capgemini': 7199, 'schema': 7200, 'mice': 7201, 'tracker': 7202, 'regiment': 7203, 'portrait': 7204, 'internationally': 7205, 'approaching': 7206, 'mucus': 7207, 'alchemist': 7208, 'horny': 7209, 'adderall': 7210, 'shopkeeper': 7211, 'lied': 7212, 'jets': 7213, 'melting': 7214, 'encryption': 7215, 'parenting': 7216, 'biography': 7217, 'ankle': 7218, 'pgp': 7219, 'ears': 7220, 'closet': 7221, 'ecology': 7222, 'insist': 7223, 'voicemail': 7224, 'drank': 7225}\n",
            "Vocabulary size: 7226\n",
            "Tokens count: 623563\n",
            "Unknown tokens appeared: 35607\n",
            "Most freq words: ['?', 'the', 'what', 'is', 'how', 'i', 'a', 'to', 'in', 'do', 'of', 'are', 'and', 'can', 'for', ',', 'you', 'why', 'it', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. collect the context words\n",
        "\n",
        "First of all, we need to collect all the contexts from our corpus."
      ],
      "metadata": {
        "id": "jHEQOelqQwqP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocrsXgaynYPG"
      },
      "source": [
        "def build_contexts(tokenized_texts, window_size):\n",
        "    contexts = []\n",
        "    for tokens in tokenized_texts:\n",
        "        for i in range(len(tokens)):\n",
        "            central_word = tokens[i]\n",
        "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1)\n",
        "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
        "\n",
        "            contexts.append((central_word, context))\n",
        "\n",
        "    return contexts\n",
        "\n",
        "contexts = build_contexts(tokenized_texts, window_size=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o_ePiZ7wfpT"
      },
      "source": [
        "Check, what you got:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyQNK-9SBdb9"
      },
      "source": [
        "contexts[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbQKln_6yC4l"
      },
      "source": [
        "4. Convert to indices\n",
        "\n",
        "Let's convert words to indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOPRlKlLvUBA"
      },
      "source": [
        "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context])\n",
        "            for central_word, context in contexts]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGfhLR6x8D3r"
      },
      "source": [
        "### **1.2 Continuous Bag of Words (CBoW) Word2vec**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UuVr2IsaYhX"
      },
      "source": [
        "We have learn skip-gram model in tutorial. Now, we will explore another popular Word2Vec paradigm called Continuous Bag of Words (CBoW). *CBoW* offers faster processing and slightly better accuracy for common words compared to the *Skip-Gram*, which is more effective with rare words.\n",
        "\n",
        "**CBoW Structure**\n",
        "\n",
        "Below is the CBoW model architecture:\n",
        "\n",
        "![](https://i.ibb.co/StXTMFH/CBOW.png)\n",
        "\n",
        "In CBoW, the goal is to predict a target word from its surrounding context, represented by the sum of context vectors.\n",
        "\n",
        "We will leverage our understanding from the *Skip-Gram* model to implement *CBoW*."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Batches Generations**\n",
        "**<font color=\"red\">[Task]</font>** : Implement the batch generator.\n",
        "\n",
        "**Hint**: The generator should produce a input matrix `(batch_size, 2 * window_size)` containing context word indices and a target matrix `(batch_size)` with central word indices."
      ],
      "metadata": {
        "id": "5ENsl-sbox1m"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNaP0uaU7T2-"
      },
      "source": [
        "def make_cbow_batches_iter(contexts, window_size, batch_size):\n",
        "\n",
        "    central_words = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "    contexts = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "\n",
        "\n",
        "    batches_count = int(math.ceil(len(contexts) / batch_size))\n",
        "\n",
        "    print('Initializing batches generator with {} batches per epoch'.format(batches_count))\n",
        "\n",
        "    indices = np.arange(len(contexts))\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    for i in range(batches_count):\n",
        "      batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "      batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "      # ------------------\n",
        "      # Write your implementation here.\n",
        "\n",
        "\n",
        "      # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBF7xiik7ZaN"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IVrQl8S4L9j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5546c81-358e-4e7d-d6ba-37fe38aac30d"
      },
      "source": [
        "window_size = 2\n",
        "batch_size = 32\n",
        "\n",
        "batch = next(make_cbow_batches_iter(contexts, window_size=window_size, batch_size=batch_size))\n",
        "\n",
        "assert isinstance(batch, dict)\n",
        "assert 'labels' in batch and 'tokens' in batch\n",
        "\n",
        "assert isinstance(batch['tokens'], torch.LongTensor)\n",
        "assert isinstance(batch['labels'], torch.LongTensor)\n",
        "\n",
        "assert batch['tokens'].shape == (batch_size, 2 * window_size)\n",
        "assert batch['labels'].shape == (batch_size,)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing batches generator with 12372 batches per epoch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbKbZ_4E7T3U"
      },
      "source": [
        "2. **Model**\n",
        "**<font color=\"red\">[Task]</font>**: Build the `CBoWModel`.\n",
        "\n",
        "**Hint**: You need to implement the `forward` method based on the CBoW architecture. The context embedding is represented as the average of their context embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mkawxwe77T3V"
      },
      "source": [
        "class CBoWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # ------------------\n",
        "        # Write your implementation here.\n",
        "\n",
        "\n",
        "        # ------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHhTDxya7a3S"
      },
      "source": [
        "Check it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh_mNh__6lG2"
      },
      "source": [
        "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "outputs = model(batch['tokens'].cuda())\n",
        "\n",
        "assert isinstance(outputs, torch.cuda.FloatTensor)\n",
        "assert outputs.shape == (batch_size, len(word2index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmn56yki7T3a"
      },
      "source": [
        "3. **Training**\n",
        "**<font color=\"red\">[Task]</font>** : Train the CBoW.\n",
        "\n",
        "**Hint**: Consider referring to the training code of the previously mentioned *Skip-gram* model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzKLP9bs7T3b"
      },
      "source": [
        "# Here are the hyperparameters you can adjust\n",
        "embedding_dim = 32\n",
        "learning_rate = 0.001\n",
        "epoch_num = 4\n",
        "batch_size = 128\n",
        "\n",
        "# Initialization Model\n",
        "model = CBoWModel(len(word2index),embedding_dim)\n",
        "# Getting model to GPU\n",
        "model.cuda()\n",
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# use Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_every_nsteps = 3000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "global_step = 0\n",
        "\n",
        "for ep in range(epoch_num):\n",
        "  for step, batch in enumerate(make_cbow_batches_iter(contexts, window_size=2, batch_size=batch_size)):\n",
        "      global_step += 1\n",
        "\n",
        "      # ------------------\n",
        "      # Write your implementation here.\n",
        "\n",
        "\n",
        "      # ------------------\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      if global_step != 0 and global_step % loss_every_nsteps == 0:\n",
        "          print(\"Epoch = {}, Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(ep, step, total_loss / loss_every_nsteps,\n",
        "                                                                      time.time() - start_time))\n",
        "          total_loss = 0\n",
        "          start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Obtaining word embeddings**\n",
        "\n",
        "Word embeddings are contained within the embeddings layer of the model. We just need to move them from the GPU to the CPU and convert them to a numpy array."
      ],
      "metadata": {
        "id": "bcdVwk_gtiC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = model.embeddings.weight.data.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "YRu-WeMbtivr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing Trained Word Embeddings**"
      ],
      "metadata": {
        "id": "X-GyEW7s76-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how adequate are similarities that the model learnt."
      ],
      "metadata": {
        "id": "AqRo0WHOtrpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(embeddings, index2word, word2index, word):\n",
        "    word_emb = embeddings[word2index[word]]\n",
        "\n",
        "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
        "    top10 = np.argsort(similarities)[-10:]\n",
        "\n",
        "    return [index2word[index] for index in reversed(top10)]\n",
        "\n",
        "most_similar(embeddings, index2word, word2index, 'my')"
      ],
      "metadata": {
        "id": "mjZ4Ki8RtXZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQW4PBdF96xC"
      },
      "source": [
        "**Visualization of our embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "\n",
        "    if isinstance(color, str):\n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show:\n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=1)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "\n",
        "\n",
        "def visualize_embeddings(embeddings, index2word, word_count):\n",
        "    word_vectors = embeddings[1: word_count + 1]\n",
        "    words = index2word[1: word_count + 1]\n",
        "\n",
        "    word_tsne = get_tsne_projection(word_vectors)\n",
        "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='blue', token=words)\n",
        "\n",
        "\n",
        "visualize_embeddings(embeddings, index2word, 100)"
      ],
      "metadata": {
        "id": "5utoAUjltQZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2： Explore Word Embeddings with Word2Vec\n",
        "In this task, we shall explore the embeddings produced by word2vec. Please revisit the lecture slides or tutorials for more details on the word2vec algorithm. If you're feeling adventurous, challenge yourself and try reading the original [paper](https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf).\n",
        "\n",
        "Then run the following cells to load the word2vec vectors into memory. **Note**: This might take several minutes."
      ],
      "metadata": {
        "id": "dkidM73PdLva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_word2vec():\n",
        "    \"\"\" Load GloVe Twitter Vectors\n",
        "        Return:\n",
        "            wv_from_bin: Pre-trained embeddings with 25 dimensions for 1.2M vocabulary.\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-twitter-25\")\n",
        "    vocab = list(wv_from_bin.key_to_index.keys())  # Updated for Gensim 4.x\n",
        "    print(\"Loaded vocab size %i\" % len(vocab))\n",
        "    return wv_from_bin"
      ],
      "metadata": {
        "id": "GI3v_EcWf9nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This may take several minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_word2vec()"
      ],
      "metadata": {
        "id": "9n7coG7kgHwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Reducing dimensionality of Word2Vec Word Embeddings**\n",
        "\n",
        "Let's directly compare the word2vec embeddings to those of the co-occurrence matrix. Run the following cells to:\n",
        "\n",
        "- Put the 1.2 million word2vec vectors into a matrix M\n",
        "- Run reduce_to_k_dim (your Truncated SVD function) to reduce the vectors from 25-dimensional to 2-dimensional.\n",
        "\n"
      ],
      "metadata": {
        "id": "N3ONN2P8z9yA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']):\n",
        "    \"\"\" Put the word2vec vectors into a matrix M.\n",
        "        Param:\n",
        "            wv_from_bin: KeyedVectors object; the 1.2 million word2vec vectors loaded from file\n",
        "        Return:\n",
        "            M: numpy matrix shape (num words, 300) containing the vectors\n",
        "            word2Ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    words = list(wv_from_bin.key_to_index.keys())\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.shuffle(words)\n",
        "    words = words[:10000]\n",
        "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
        "    word2Ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2Ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    for w in required_words:\n",
        "        try:\n",
        "            M.append(wv_from_bin.word_vec(w))\n",
        "            word2Ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2Ind"
      ],
      "metadata": {
        "id": "Cmx7pKhhh1Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement reduce_to_k_dim**\n",
        "\n",
        "Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings.\n",
        "\n",
        "Note: All of numpy, scipy, and scikit-learn (sklearn) provide some implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html).\n",
        "\n",
        "**<font color=\"red\">[Task]</font>**: Complete reduce_to_k_dim function"
      ],
      "metadata": {
        "id": "NwG7p3MepE6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "\n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"\n",
        "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "\n",
        "        # ------------------\n",
        "        # Write your implementation here.\n",
        "\n",
        "\n",
        "        # ------------------\n",
        "\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ],
      "metadata": {
        "id": "ggHnG5EcidGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 25-Dimensinal Word Embeddings to k Dimensions\n",
        "# Note: This may take several minutes\n",
        "# -----------------------------------------------------------------\n",
        "M, word2Ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)"
      ],
      "metadata": {
        "id": "hDG1rO_wh2zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here is a helper function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (plt).**"
      ],
      "metadata": {
        "id": "_0fGsfm-oZG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_embeddings(M_reduced, word2Ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        Include a label next to each point.\n",
        "\n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus, k)): matrix of k-dimensional word embeddings\n",
        "            word2Ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for word in words:\n",
        "        if word in word2Ind:\n",
        "            idx = word2Ind[word]\n",
        "            x, y = M_reduced[idx, 0], M_reduced[idx, 1]\n",
        "            plt.scatter(x, y, marker='o', color='blue')\n",
        "            plt.text(x + 0.02, y + 0.02, word, fontsize=9)\n",
        "        else:\n",
        "            print(f\"Word '{word}' not found in word2Ind dictionary.\")\n",
        "\n",
        "    plt.title(\"Word Embeddings Visualization\")\n",
        "    plt.xlabel(\"Dimension 1\")\n",
        "    plt.ylabel(\"Dimension 2\")\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bMXt_-QPn0C0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1: Word2Vec Plot Analysis\n",
        "Run the cell below to plot the 2D word2vec embeddings for ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela'].\n",
        "\n",
        "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? How is the plot different from the one generated earlier from the co-occurrence matrix?"
      ],
      "metadata": {
        "id": "wBbtacQhorHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\n",
        "plot_embeddings(M_reduced, word2Ind, words)"
      ],
      "metadata": {
        "id": "usSSq_x3llW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>**:Write your answer here."
      ],
      "metadata": {
        "id": "Swps7Nsqo0Q5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Polysemous Words\n",
        "Find a [polysemous](https://en.wikipedia.org/wiki/Polysemy) word (for example, \"leaves\" or \"scoop\") such that the top-10 most similar words (according to cosine similarity) contains related words from both meanings. For example, \"leaves\" has both \"turns\" and \"ground\" in the top 10, and \"scoop\" has both \"buckets\" and \"pops\". You will probably need to try several polysemous words before you find one. Please state the polysemous word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous words you tried didn't work?\n",
        "\n",
        "Note: You should use the wv_from_bin.most_similar(word) function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance please check the GenSim [documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)."
      ],
      "metadata": {
        "id": "3mbzLnAwdOth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ------------------\n",
        "# Write your polysemous word exploration code here.\n",
        "\n",
        "wv_from_bin.most_similar(\"\")\n",
        "\n",
        "# ------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "f1pUnbtZq-ST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>**:Write your answer here."
      ],
      "metadata": {
        "id": "EIiwx3NLrHnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3: Synonyms & Antonyms\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but Cosine Distance(w1,w3) < Cosine Distance(w1,w2). For example, w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\".\n",
        "\n",
        "Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the wv_from_bin.distance(w1, w2) function here in order to compute the cosine distance between two words. Please see the GenSim documentation for further assistance.\n"
      ],
      "metadata": {
        "id": "k8vhD0k0dQ-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Write your synonym & antonym exploration code here.\n",
        "\n",
        "w1 = \"\"\n",
        "w2 = \"\"\n",
        "w3 = \"\"\n",
        "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))\n",
        "\n",
        "# ------------------"
      ],
      "metadata": {
        "id": "Wli3CumGs50-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>**:Write your answer here."
      ],
      "metadata": {
        "id": "gZV7c7_4s9Is"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4G2X-TTpzwz"
      },
      "source": [
        "## Task 3: Utilize Word Embeddings\n",
        "\n",
        "Guess, you've seen such pictures already:  \n",
        "\n",
        "![Embeddings Relations](https://www.tensorflow.org/images/linear-relationships.png)\n",
        "*Source: [Tensorflow tutorial on Vector Representations of Words](https://www.tensorflow.org/tutorials/representation/word2vec)*\n",
        "\n",
        "In the first image, we observe the intricate relationships encoded within the word embeddings space. This encompasses various dimensions like gender differences (male-female) or verb tenses.\n",
        "\n",
        "**Interactive Exploration**\n",
        "\n",
        "To delve deeper and interactively explore these relationships, check out these resources:\n",
        "- [Word Vector Demo](http://bionlp-www.utu.fi/wv_demo/)\n",
        "- [Word2Viz](https://lamyiowce.github.io/word2viz/)\n",
        "\n",
        "These tools offer a playful yet insightful experience, allowing you to grasp the nuances and capabilities of word embeddings.\n",
        "\n",
        "**Our task point**\n",
        "\n",
        "Our focus will be on utilizing [gensim](https://radimrehurek.com/gensim/), a well-regarded Python library for word embeddings. Gensim makes it effortless to work with and leverage the power of word embeddings in various applications.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Use Pretrained Embeddings**\n",
        "Base on gensim, we can easily use a well-pretrained embeddings model. There are a number of such models in <font color=\"blue\">gensim</font>, you can call `api.info()` to get the list."
      ],
      "metadata": {
        "id": "KIvBhh71WIeS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEazfh1s9eki"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('glove-twitter-25')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**use word embedidngs with gensim**\n",
        "\n",
        "Yay, we have loaded well-built word embedings models, now let's learn how to use it.\n",
        "\n",
        "1. To get word's vector, well, call `get_vector`:"
      ],
      "metadata": {
        "id": "HPQxqjIGZxt_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uF6iF6A9uGQ"
      },
      "source": [
        "model.get_vector('anything')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. To get most similar words for the given one :"
      ],
      "metadata": {
        "id": "DiXwAZTsaHCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar('bread')"
      ],
      "metadata": {
        "id": "57uH83XZaI6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Analogies with word embeddings\n",
        "\n",
        "It can do such magic (`woman` + `grandfather` - `man`) :\n"
      ],
      "metadata": {
        "id": "Rtyp__uQaVcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "model.most_similar(positive=['woman', 'grandfather'], negative=['man'])"
      ],
      "metadata": {
        "id": "9igEyCm6aqfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And this too:"
      ],
      "metadata": {
        "id": "BwCkJSNraruT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.most_similar([model.get_vector('coder') - model.get_vector('brain') + model.get_vector('money')])"
      ],
      "metadata": {
        "id": "e4t94HZXa1vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is, who is like coder, with money and without brains."
      ],
      "metadata": {
        "id": "c23cwWhKvtXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>** : Run an interesting analogy example\n",
        "\n",
        "**Hint**: Similar to (`woman` + `grandfather` - `man`)"
      ],
      "metadata": {
        "id": "4a15dwaha36x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "\n",
        "# ------------------"
      ],
      "metadata": {
        "id": "g0D04rXsa_al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.2 Finding the Most Similar Sentence**\n",
        "\n",
        "In this section, we present a method for sentence retrieval based on word embeddings.\n",
        "\n",
        "The key point is to construct *sentence embeddings*. The simplest method to obtain a sentence embedding is by averaging the embeddings of the words within the sentence.\n",
        "\n",
        "*You are probably thinking, 'What a dumb idea, why on earth the average of embedding should contain any useful information'. Well, check [this paper](https://arxiv.org/pdf/1805.09843.pdf).*\n",
        "\n"
      ],
      "metadata": {
        "id": "szS69OEObDzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Get Sentence Embedding\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : Implement a function to compute sentence embeddings.\n",
        "\n",
        "**Hint**: Tokenize and lowercase the texts. Calculate the mean embedding for words with known embeddings."
      ],
      "metadata": {
        "id": "FPJdCsjtfDxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_embedding(model, sentence):\n",
        "    \"\"\" Calcs sentence embedding as a mean of known word embeddings in the sentence.\n",
        "    If all the words are unknown, returns zero vector.\n",
        "    :param model: KeyedVectors instance\n",
        "    :param sentence: str or list of str (tokenized text)\n",
        "    \"\"\"\n",
        "    embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "\n",
        "    if isinstance(sentence, str):\n",
        "        words = word_tokenize(sentence.lower())\n",
        "    else:\n",
        "        words = sentence\n",
        "\n",
        "    sum_embedding = np.zeros([model.vector_size], dtype='float32')\n",
        "    words_in_model = 0\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "metadata": {
        "id": "0LPw1fRg1GaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check it:"
      ],
      "metadata": {
        "id": "1FbZbKPI1OZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = get_sentence_embedding(model, \"I'm very sure. This never happened to me before...\")\n",
        "assert vector.shape == (model.vector_size,)"
      ],
      "metadata": {
        "id": "u5phpDHEcdK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Building the Index**\n",
        "\n",
        "With our method ready, we can now embed all sentences in our corpus for retrieval purposes. In this case, we use data from Quora, sampling 1000 entries randomly, and converting them into sentence embeddings."
      ],
      "metadata": {
        "id": "u-8wclbCdTfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quora_data = pd.read_csv('train.csv')\n",
        "corpus = list(quora_data.sample(1000)[['question1']].question1.replace(np.nan, '', regex=True).unique())\n",
        "text_vectors = np.array([get_sentence_embedding(model, sentence) for sentence in corpus])"
      ],
      "metadata": {
        "id": "P6B2c-bJdCrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus[0]"
      ],
      "metadata": {
        "id": "NXrJ-R8leSWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Search**\n",
        "\n",
        "Now we are able perform search of the nearest neighbours to the given sentences in our base!\n",
        "\n",
        "\n",
        "We'll use cosine similarity of two vectors:\n",
        "$$\\text{cosine_similarity}(x, y) = \\frac{x^{T} y}{||x||\\cdot ||y||}$$\n",
        "\n",
        "*It's not a [distance](https://www.encyclopediaofmath.org/index.php/Metric) strictly speaking but we still can use it to search for the sentence vectors.*\n",
        "\n",
        "**<font color=\"red\">[Task]</font>** : IImplement the following function.\n",
        "\n",
        "**Hint:** Calc the similarity between `query` embedding and `text_vectors` using `cosine_similarity` function. Find `k` vectors with highest scores and return corresponding texts from `texts` list."
      ],
      "metadata": {
        "id": "QiMtj5dXebbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_nearest(model, text_vectors, texts, query, k=10):\n",
        "    query_vec = get_sentence_embedding(model, query)\n",
        "\n",
        "    # ------------------\n",
        "    # Write your implementation here.\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------"
      ],
      "metadata": {
        "id": "ynEJW6E7eg0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check it!"
      ],
      "metadata": {
        "id": "is8SoYmHkQo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "find_nearest(model, text_vectors, corpus, \"What's your biggest regret in life?\", k=10)"
      ],
      "metadata": {
        "id": "49s4zB1OjXd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bias of Word Embeddings**\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n"
      ],
      "metadata": {
        "id": "qA5CwGV8jU5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example showing word embeddings biases on gender:"
      ],
      "metadata": {
        "id": "pdIUrmnJxGvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "print(model.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ],
      "metadata": {
        "id": "gWngJZCWxduU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "661978c9-f2ee-4de1-ac10-935593d60c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('maths', 0.7983574867248535), ('basis', 0.7973601222038269), ('humør', 0.7948898673057556), ('cert', 0.7902684807777405), ('mulig', 0.7874146699905396), ('spændende', 0.7728654742240906), ('dårligt', 0.7700908184051514), ('latter', 0.7676339745521545), ('noget', 0.7676041126251221), ('vet', 0.7675378918647766)]\n",
            "\n",
            "[('representation', 0.871566116809845), ('encourages', 0.8626720309257507), ('empowering', 0.8612703084945679), ('intellectual', 0.8564386963844299), ('influences', 0.8559868931770325), ('ethical', 0.8550471663475037), ('affairs', 0.8541139960289001), ('behaviors', 0.8481355905532837), ('advocacy', 0.8439522981643677), ('critic', 0.8406822085380554)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>** Identify an example of bias.\n",
        "\n",
        "**Hint:** Consider providing an example from perspectives such as race or sexual orientation."
      ],
      "metadata": {
        "id": "zpFDY9BByDBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------\n",
        "# Write your implementation here.\n",
        "\n",
        "\n",
        "# ------------------"
      ],
      "metadata": {
        "id": "np_RQalnx0Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">[Task]</font>** Thinking About Bias.\n",
        "\n",
        "**Hint:** Briefly explain how bias can be introduced into word embeddings and suggest one method to mitigate these biases."
      ],
      "metadata": {
        "id": "902RJydFyjMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**<font color=\"red\">Write your answer here.</font>**\n",
        "\n"
      ],
      "metadata": {
        "id": "P8zWOHg3065B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqDmuu7m_PB5"
      },
      "source": [
        "## Supplementary Materials\n",
        "Source from [DeepNLP-Course of DanAnastasyev](https://colab.research.google.com/drive/1o65wrq6RYgWyyMvNP8r9ZknXBniDoXrn#forceEdit=true&offline=true&sandboxMode=true)\n",
        "\n",
        "## To read\n",
        "### Blogs\n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
        "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
        "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
        "\n",
        "### Papers\n",
        "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
        "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)  \n",
        "\n",
        "### Enhancing Embeddings\n",
        "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
        "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
        "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
        "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
        "\n",
        "### Sentence Embeddings\n",
        "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
        "\n",
        "### Backpropagation\n",
        "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
        "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
        "\n",
        "## To watch\n",
        "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
        "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Acknowledgement\n",
        "\n",
        "This assignment was developed with reference to the following course materials:\n",
        "- [DeepNLP Course by Dan Anastasyev](https://github.com/DanAnastasyev/DeepNLP-Course?tab=readme-ov-file)\n",
        "- [Exploring Word Vectors from Stanford's CS224N](https://web.stanford.edu/class/cs224n/assignments/a1_preview/exploring_word_vectors.html)\n",
        "- [Natural Language Processing course from Princeton University](https://nlp.cs.princeton.edu/cos484-sp21/)\n",
        "- [Yandex Data School NLP Course Week 1 Seminar](https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/2023/week01_embeddings/seminar.ipynb#scrollTo=9m7GZWVk-jrW)\n"
      ],
      "metadata": {
        "id": "CJ_O-O0wkool"
      }
    }
  ]
}