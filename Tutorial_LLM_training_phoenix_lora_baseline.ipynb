{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/119020/NLP_2025_Spring_Materials/blob/main/Tutorial_LLM_training_phoenix_lora_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "768b1074",
      "metadata": {
        "id": "768b1074",
        "papermill": {
          "duration": 0.005642,
          "end_time": "2023-10-19T19:46:39.562030",
          "exception": false,
          "start_time": "2023-10-19T19:46:39.556388",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Tutorial 2: Train your own LLMs\n",
        "### **Course Name:** Large Language Models **<font color=\"red\">(CSC 6203)</font>**\n",
        " *Presenter: Ke Ji*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook guide provides a comprehensive overview of using the `transformers` Python package to efficiently train a custom model. It covers the following techniques:\n",
        "\n",
        "1. Utilizing model, tokenizer, and dataset loading function from Hugging Face.\n",
        "2. Performing basic data cleaning.\n",
        "3. Training the model with basic modeling techniques, including quantization, such as qlora in this instance.\n",
        "4. Evaluating the model's performance on test set.\n",
        "5. Saving your custom model and preparing it for deployment."
      ],
      "metadata": {
        "id": "pqOBrzvvz6pB"
      },
      "id": "pqOBrzvvz6pB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Preparation\n",
        "\n",
        "Before proceeding with model training, ensure your environment is properly configured by following these steps:\n",
        "\n",
        "1. Install the necessary Python packages.\n",
        "2. Import the required libraries."
      ],
      "metadata": {
        "id": "u4AcE7nU1qQI"
      },
      "id": "u4AcE7nU1qQI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f6c6af6",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:46:39.573247Z",
          "iopub.status.busy": "2023-10-19T19:46:39.572950Z",
          "iopub.status.idle": "2023-10-19T19:47:04.277598Z",
          "shell.execute_reply": "2023-10-19T19:47:04.276185Z"
        },
        "id": "6f6c6af6",
        "papermill": {
          "duration": 24.713051,
          "end_time": "2023-10-19T19:47:04.280001",
          "exception": false,
          "start_time": "2023-10-19T19:46:39.566950",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6001139f-f7eb-4766-b729-d2f5b7cc6b53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Error parsing requirements for tokenizers: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/tokenizers-0.13.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Error parsing requirements for transformers: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/transformers-4.31.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33m    WARNING: No metadata found in /usr/local/lib/python3.10/dist-packages\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itsandbytes (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "## This code block shows how to install the necessary python packages using pip script.\n",
        "!pip install -q h5py typing-extensions wheel fschat\n",
        "# !pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 fschat\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use this command to check your GPU resources."
      ],
      "metadata": {
        "id": "Oi5LcPU_oqpu"
      },
      "id": "Oi5LcPU_oqpu"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_T66f30GsT0t",
        "outputId": "31049925-d1b5-4932-c758-d0e8b7e9e309"
      },
      "id": "_T66f30GsT0t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Mar 15 06:35:23 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a194ac33",
      "metadata": {
        "papermill": {
          "duration": 0.005056,
          "end_time": "2023-10-19T19:47:18.007366",
          "exception": false,
          "start_time": "2023-10-19T19:47:18.002310",
          "status": "completed"
        },
        "tags": [],
        "id": "a194ac33"
      },
      "source": [
        "## Load Pre-trained model and tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's load the model we are going to use - phoenix-inst-chat-7b! Note that the model itself is around 7B in full precision.\n",
        "\n",
        "Note that we are using a model with 7b parameters, so we use qlora to reduce the memory usage considering our limited gpu resources"
      ],
      "metadata": {
        "id": "uGLMe7qCXQGQ"
      },
      "id": "uGLMe7qCXQGQ"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "## Quantization type (fp4 or nf4), According to QLoRA paper, for training 4-bit base models (e.g. using LoRA adapters) one should use\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = True\n",
        "\n",
        "## If you don't have enough GPU computing resources, you can use LLMs with less parameters (like Qwen2.5-0.5b or microsoft/phi-1_5).\n",
        "model_id = \"FreedomIntelligence/phoenix-inst-chat-7b\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "## Be sure to remember to use lora or qlora (parameter efficient fine-tuning methods) if you try to use some LLMs with more parameters.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "155866f3c4e5433c90a4128d4633d12c",
            "116ed4a0802842079e9572ab90856e28",
            "6aa20bd15d8644dfb83a154efc3fbddf",
            "d1188ccc6eff4eba9f6ef09667b9b956",
            "0d6a4c1cf30b40d3b42f3f26330dda40",
            "2579d66afcaa4434884b5d558a44644b",
            "7265f845df394c1197ffdd4152a0a175",
            "8daae89f8892497db401df65b62e0080",
            "c2ddb5f4dd9f4bf3b99663cc2f4e5706",
            "6e710ccc6838451a82fc1efcf8f8c5cf",
            "882ff01a2b6743838a3205da6d9acdce"
          ]
        },
        "id": "wye4Wm9rXgFF",
        "outputId": "3df1b64c-a35f-473e-8a4e-7dbaeaf3460a"
      },
      "id": "wye4Wm9rXgFF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "155866f3c4e5433c90a4128d4633d12c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
      ],
      "metadata": {
        "id": "BFNSifwxX8Ju"
      },
      "id": "BFNSifwxX8Ju"
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "bpWUSDliX-e2"
      },
      "id": "bpWUSDliX-e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "agMER78BYbxp"
      },
      "id": "agMER78BYbxp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "## You can try differnt parameter-effient strategy for model trianing, for more info, please check https://github.com/huggingface/peft\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=8,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ],
      "metadata": {
        "id": "ZWzMDILnYecp"
      },
      "id": "ZWzMDILnYecp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631c6fd1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:48:30.004809Z",
          "iopub.status.busy": "2023-10-19T19:48:30.004561Z",
          "iopub.status.idle": "2023-10-19T19:51:43.184874Z",
          "shell.execute_reply": "2023-10-19T19:51:43.184066Z"
        },
        "papermill": {
          "duration": 193.196823,
          "end_time": "2023-10-19T19:51:43.193655",
          "exception": false,
          "start_time": "2023-10-19T19:48:29.996832",
          "status": "completed"
        },
        "tags": [],
        "id": "631c6fd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa36c772-5ac5-4db8-e20c-535c0b86e70f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
            "\n",
            "Human: <s>Are you a good AI assistant?</s>Assistant: <s>\n",
            "--------------------------------------------------------------------------------\n",
            "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
            "\n",
            "Human: Are you a good AI assistant?Assistant: As an AI language model, I am designed to assist users with a wide range of tasks and questions. I am constantly learning and improving my capabilities to provide the best possible responses to the questions and tasks that I am given. I am not a person, but rather a computer program that has been trained on a large dataset of text.\n"
          ]
        }
      ],
      "source": [
        "from fastchat.conversation import get_conv_template\n",
        "device = \"cuda\"\n",
        "model.eval()\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(prompt):\n",
        "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
        "    outputs = model.generate(input_ids, do_sample=False, max_new_tokens=1024)\n",
        "    return tokenizer.decode(*outputs, skip_special_tokens=True)\n",
        "\n",
        "conv = get_conv_template('phoenix')\n",
        "conv.append_message(conv.roles[0], \"Are you a good AI assistant?\")\n",
        "# conv.append_message(conv.roles[1], \"No, I am evil!\")\n",
        "# conv.append_message(conv.roles[0], \"Can you tell me what you can do?\")\n",
        "\n",
        "conv.append_message(conv.roles[1], None)\n",
        "prompt = conv.get_prompt()\n",
        "print(prompt)\n",
        "response = generate(prompt)\n",
        "print(\"-\"*80)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30345052",
      "metadata": {
        "papermill": {
          "duration": 0.00626,
          "end_time": "2023-10-19T19:51:43.206269",
          "exception": false,
          "start_time": "2023-10-19T19:51:43.200009",
          "status": "completed"
        },
        "tags": [],
        "id": "30345052"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a common dataset, english quotes, to fine tune our model on famous quotes."
      ],
      "metadata": {
        "id": "-zpWKTXIYtNH"
      },
      "id": "-zpWKTXIYtNH"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# data = load_dataset(\"Abirate/english_quotes\")\n",
        "dataset = load_dataset(\"FreedomIntelligence/Huatuo26M-Lite\")\n",
        "dataset = dataset['train'].map(lambda sample: {\"conversations\": [{\"role\": \"human\", \"value\": sample['question']}, {\"role\": \"gpt\", \"value\": sample['answer']}]}, batched=False)"
      ],
      "metadata": {
        "id": "VlKzEROYYuAR"
      },
      "id": "VlKzEROYYuAR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then split the dataset into train sets and test sets"
      ],
      "metadata": {
        "id": "pdK4lyKJnD17"
      },
      "id": "pdK4lyKJnD17"
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "train_dataset_size, val_dataset_size = 40, 8\n",
        "train_dataset, val_dataset, _ = random_split(dataset, [train_dataset_size, val_dataset_size, len(dataset)-train_dataset_size-val_dataset_size])"
      ],
      "metadata": {
        "id": "MYS6k8ODiRwf"
      },
      "id": "MYS6k8ODiRwf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7b91ae64",
      "metadata": {
        "papermill": {
          "duration": 0.006012,
          "end_time": "2023-10-19T19:51:43.218621",
          "exception": false,
          "start_time": "2023-10-19T19:51:43.212609",
          "status": "completed"
        },
        "tags": [],
        "id": "7b91ae64"
      },
      "source": [
        "### Customized Dataset\n",
        "Create a specialized dataset class named \"InstructionDataset\" designed to handle our custom dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37efc771",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:51:43.232713Z",
          "iopub.status.busy": "2023-10-19T19:51:43.232464Z",
          "iopub.status.idle": "2023-10-19T19:51:43.245619Z",
          "shell.execute_reply": "2023-10-19T19:51:43.245024Z"
        },
        "papermill": {
          "duration": 0.022433,
          "end_time": "2023-10-19T19:51:43.247373",
          "exception": false,
          "start_time": "2023-10-19T19:51:43.224940",
          "status": "completed"
        },
        "tags": [],
        "id": "37efc771"
      },
      "outputs": [],
      "source": [
        "import json, copy\n",
        "import transformers\n",
        "from typing import Dict, Sequence, List\n",
        "from dataclasses import dataclass\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"<pad>\"\n",
        "DEFAULT_BOS_TOKEN = \"<s>\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
        "default_conversation = get_conv_template('phoenix')\n",
        "\n",
        "class InstructDataset(Dataset):\n",
        "    def __init__(self, data: Sequence, tokenizer: transformers.PreTrainedTokenizer) -> None:\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
        "        sources = self.data[index]\n",
        "        if isinstance(index, int):\n",
        "            sources = [sources]\n",
        "        data_dict = preprocess([e['conversations'] for e in sources], self.tokenizer)\n",
        "        if isinstance(index, int):\n",
        "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0], labels=data_dict[\"labels\"][0])\n",
        "        return data_dict\n",
        "\n",
        "def preprocess(\n",
        "        sources: Sequence[str],\n",
        "        tokenizer: transformers.PreTrainedTokenizer,\n",
        "        max_length=1024\n",
        ") -> Dict:\n",
        "    ## add end signal and concatenate together\n",
        "    conversations = []\n",
        "    intermediates = []\n",
        "    for source in sources:\n",
        "        header = f\"{default_conversation.system_message}\"\n",
        "        conversation, intermediate = _add_speaker_and_signal(header, source)\n",
        "        conversations.append(conversation)\n",
        "        intermediates.append(intermediate)\n",
        "\n",
        "    ## tokenize conversations\n",
        "    conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n",
        "    input_ids = conversations_tokenized[\"input_ids\"]\n",
        "    targets = copy.deepcopy(input_ids)\n",
        "\n",
        "    ## keep only machine responses as targets\n",
        "    assert len(targets) == len(intermediates)\n",
        "    for target, inters in zip(targets, intermediates):\n",
        "        mask = torch.zeros_like(target, dtype=torch.bool)\n",
        "        for inter in inters:\n",
        "            tokenized = _tokenize_fn(inter, tokenizer)\n",
        "            start_idx = tokenized[\"input_ids\"][0].size(0) - 1\n",
        "            end_idx = tokenized[\"input_ids\"][1].size(0)\n",
        "            mask[start_idx:end_idx] = True\n",
        "        target[~mask] = IGNORE_INDEX\n",
        "\n",
        "    input_ids = input_ids[:max_length]\n",
        "    targets = targets[:max_length]\n",
        "    return dict(input_ids=input_ids, labels=targets)\n",
        "\n",
        "def _add_speaker_and_signal(header, source, get_conversation=True):\n",
        "    BEGIN_SIGNAL = DEFAULT_BOS_TOKEN\n",
        "    END_SIGNAL = DEFAULT_EOS_TOKEN\n",
        "    conversation = header\n",
        "    intermediate = []\n",
        "    for sentence in source:\n",
        "        from_str = sentence[\"role\"]\n",
        "        if from_str.lower() == \"human\":\n",
        "            from_str = default_conversation.roles[0]\n",
        "        elif from_str.lower() == \"gpt\":\n",
        "            from_str = default_conversation.roles[1]\n",
        "        else:\n",
        "            from_str = 'unknown'\n",
        "        # store the string w/o and w/ the response\n",
        "        value = (from_str + \": \" + BEGIN_SIGNAL + sentence[\"value\"] + END_SIGNAL)\n",
        "        if sentence[\"role\"].lower() == \"gpt\":\n",
        "            start = conversation + from_str + \": \" + BEGIN_SIGNAL\n",
        "            end = conversation + value\n",
        "            intermediate.append([start, end])\n",
        "        if get_conversation:\n",
        "            conversation += value\n",
        "    return conversation, intermediate\n",
        "\n",
        "##\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        ) for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
        "        for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids,\n",
        "            batch_first=True,\n",
        "            padding_value=self.tokenizer.pad_token_id)\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = InstructDataset(train_dataset, tokenizer)\n",
        "val_dataset = InstructDataset(val_dataset, tokenizer)\n",
        "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "yyFrEPnXfm5m"
      },
      "id": "yyFrEPnXfm5m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_data = train_dataset[1]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Debuging: \")\n",
        "print(sample_data)\n",
        "print(\"-\" * 80)\n",
        "print(f\"input_ids:\\n{tokenizer.decode(sample_data['input_ids'])}\")\n",
        "z = [token if token != IGNORE_INDEX else tokenizer.unk_token_id for token in sample_data['labels']]\n",
        "print(\"-\" * 80)\n",
        "print(f\"labels:\\n{tokenizer.decode(z)}\")\n",
        "print(\"=\" * 80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGeKQrgRfpcC",
        "outputId": "f5051b7a-fe9a-426c-9ed5-e5c70e0f0167"
      },
      "id": "cGeKQrgRfpcC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Debuging: \n",
            "{'input_ids': tensor([    36,  44799,   5299,    267,  99579,   7384,    530,    660,  48763,\n",
            "         64225, 103800,     17,   1387, 103800,  19502,  66799,     15,  53180,\n",
            "            15,    530, 214804,  41259,    427,    368,   7384,   1256,  11732,\n",
            "          6149, 114330,     29,    210,      1, 105430,  92968,   1954,     25,\n",
            "          2129,   8627,   7786,  37761,     42, 127696,     24,     17,     19,\n",
            "         44270,   2950,  55768,   2498,   3101,      2,   9096,  61339,     29,\n",
            "           210,      1,   7136,  59280,  27557,    355, 159110,  92968,   1954,\n",
            "            25,   2129,   8627,   7786,  37761,     42, 127696,     24,     17,\n",
            "            19,    355, 134686, 135128,  55768,    373,    420,   7436,  12142,\n",
            "         93152,  82131,    355,  22523,  50238,  12160,  20885,    355,  11333,\n",
            "         36347,    420,   5022,  59280,  92968,  20418,  90899,   2808,  28723,\n",
            "         19367,    355,   7436,  12142,  18412,   4672,   7544,  23875,  20451,\n",
            "           909,  28009,  12905,  55768,    420,      2]), 'labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,      1,   7136,  59280,  27557,    355, 159110,  92968,   1954,\n",
            "            25,   2129,   8627,   7786,  37761,     42, 127696,     24,     17,\n",
            "            19,    355, 134686, 135128,  55768,    373,    420,   7436,  12142,\n",
            "         93152,  82131,    355,  22523,  50238,  12160,  20885,    355,  11333,\n",
            "         36347,    420,   5022,  59280,  92968,  20418,  90899,   2808,  28723,\n",
            "         19367,    355,   7436,  12142,  18412,   4672,   7544,  23875,  20451,\n",
            "           909,  28009,  12905,  55768,    420,      2])}\n",
            "--------------------------------------------------------------------------------\n",
            "input_ids:\n",
            "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
            "\n",
            "Human: <s>下次月经前6天测血HCG小于5.0是不是没怀孕？想</s>Assistant: <s>根据您的描述，如果在月经前6天测血HCG小于5.0，一般来说是没有怀孕的。建议您放松心情，避免紧张劳累，加强营养。如果您的月经超过一周还未来潮，建议您再次进行试纸检查以确认是否怀孕。</s>\n",
            "--------------------------------------------------------------------------------\n",
            "labels:\n",
            "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><s>根据您的描述，如果在月经前6天测血HCG小于5.0，一般来说是没有怀孕的。建议您放松心情，避免紧张劳累，加强营养。如果您的月经超过一周还未来潮，建议您再次进行试纸检查以确认是否怀孕。</s>\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd2f2fa",
      "metadata": {
        "papermill": {
          "duration": 0.006077,
          "end_time": "2023-10-19T19:51:43.339932",
          "exception": false,
          "start_time": "2023-10-19T19:51:43.333855",
          "status": "completed"
        },
        "tags": [],
        "id": "4bd2f2fa"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "479baff4",
      "metadata": {
        "papermill": {
          "duration": 0.006366,
          "end_time": "2023-10-19T19:51:43.392890",
          "exception": false,
          "start_time": "2023-10-19T19:51:43.386524",
          "status": "completed"
        },
        "tags": [],
        "id": "479baff4"
      },
      "source": [
        "### General Training Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77035f45",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:51:43.407068Z",
          "iopub.status.busy": "2023-10-19T19:51:43.406516Z",
          "iopub.status.idle": "2023-10-19T19:51:43.441368Z",
          "shell.execute_reply": "2023-10-19T19:51:43.440478Z"
        },
        "id": "77035f45",
        "papermill": {
          "duration": 0.044046,
          "end_time": "2023-10-19T19:51:43.443234",
          "exception": false,
          "start_time": "2023-10-19T19:51:43.399188",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Set training parameters\n",
        "## Some Hyper-paramters are very important to the final training performance, such as Learning rate, per_device_train_batch_size, warmup_ratio.\n",
        "training_arguments = transformers.TrainingArguments(\n",
        "    output_dir=\"./checkpoints\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim='paged_adamw_32bit',\n",
        "    save_steps=0,\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-7,\n",
        "    weight_decay=0.001,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    gradient_checkpointing=True,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f96057",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:51:43.456893Z",
          "iopub.status.busy": "2023-10-19T19:51:43.456647Z",
          "iopub.status.idle": "2023-10-19T19:51:44.106808Z",
          "shell.execute_reply": "2023-10-19T19:51:44.106137Z"
        },
        "papermill": {
          "duration": 0.659372,
          "end_time": "2023-10-19T19:51:44.108918",
          "exception": false,
          "start_time": "2023-10-19T19:51:43.449546",
          "status": "completed"
        },
        "tags": [],
        "id": "77f96057",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07680a17-4367-4366-a2c3-3de3a2b822e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "## Set the paramerter of model to train mode\n",
        "model.train()\n",
        "## Construct the trainer for training and evaluation.\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a4f39e0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:51:44.128980Z",
          "iopub.status.busy": "2023-10-19T19:51:44.128348Z",
          "iopub.status.idle": "2023-10-19T19:51:44.135954Z",
          "shell.execute_reply": "2023-10-19T19:51:44.135072Z"
        },
        "papermill": {
          "duration": 0.018004,
          "end_time": "2023-10-19T19:51:44.137612",
          "exception": false,
          "start_time": "2023-10-19T19:51:44.119608",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a4f39e0",
        "outputId": "399fd634-defa-4dd7-8c2b-4413efb85d33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 3,932,160 || all params: 7,072,948,224 || trainable%: 0.055594355783029126\n"
          ]
        }
      ],
      "source": [
        "## We can note that there is only a very few parameters involved in training.\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe99d72e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:51:44.157925Z",
          "iopub.status.busy": "2023-10-19T19:51:44.156894Z",
          "iopub.status.idle": "2023-10-19T19:54:40.972158Z",
          "shell.execute_reply": "2023-10-19T19:54:40.971338Z"
        },
        "papermill": {
          "duration": 176.829343,
          "end_time": "2023-10-19T19:54:40.973810",
          "exception": false,
          "start_time": "2023-10-19T19:51:44.144467",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "fe99d72e",
        "outputId": "4e1b8fdf-3131-454c-b3b2-ec205f36b4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 01:38, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.919000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.633500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.996500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.835500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.503900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.901900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.653700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.249300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=2.3941301703453064, metrics={'train_runtime': 113.5069, 'train_samples_per_second': 0.352, 'train_steps_per_second': 0.088, 'total_flos': 260143231991808.0, 'train_loss': 2.3941301703453064, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "## Using model.train() function to start training!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:"
      ],
      "metadata": {
        "id": "fgrJkL6mwGbt"
      },
      "id": "fgrJkL6mwGbt"
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "_5_G78x_wF4w",
        "outputId": "1c34363a-f20e-48e8-9854-884b557bc85d"
      },
      "id": "_5_G78x_wF4w",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 00:05]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 16.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b889f01",
      "metadata": {
        "papermill": {
          "duration": 0.006442,
          "end_time": "2023-10-19T19:54:40.987304",
          "exception": false,
          "start_time": "2023-10-19T19:54:40.980862",
          "status": "completed"
        },
        "tags": [],
        "id": "3b889f01"
      },
      "source": [
        "## Save Trained LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24719a28",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:54:41.001509Z",
          "iopub.status.busy": "2023-10-19T19:54:41.000982Z",
          "iopub.status.idle": "2023-10-19T19:54:41.222220Z",
          "shell.execute_reply": "2023-10-19T19:54:41.221486Z"
        },
        "papermill": {
          "duration": 0.230395,
          "end_time": "2023-10-19T19:54:41.224101",
          "exception": false,
          "start_time": "2023-10-19T19:54:40.993706",
          "status": "completed"
        },
        "tags": [],
        "id": "24719a28"
      },
      "outputs": [],
      "source": [
        "output_path = \"lora\"\n",
        "trainer.save_model(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the trained model"
      ],
      "metadata": {
        "id": "Jgi46OdfvRkM"
      },
      "id": "Jgi46OdfvRkM"
    },
    {
      "cell_type": "code",
      "source": [
        "from fastchat.conversation import get_conv_template\n",
        "device = \"cuda\"\n",
        "model.eval()\n",
        "@torch.no_grad()\n",
        "def generate(prompt):\n",
        "    ## First, we should wrap the sentence just as what we do during the training process\n",
        "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
        "    ## Second, the model generate input_ids for each inputs.\n",
        "    outputs = trainer.model.generate(input_ids, do_sample=False, max_new_tokens=1024)\n",
        "    ## Finally, we need to decode the output_ids from id sequence to token sequence to get our final results.\n",
        "    return tokenizer.decode(*outputs, skip_special_tokens=True)\n",
        "\n",
        "conv = get_conv_template('phoenix')\n",
        "conv.append_message(conv.roles[0], \"Are you a good AI assistant?\")\n",
        "# conv.append_message(conv.roles[1], \"No, I am evil!\")\n",
        "# conv.append_message(conv.roles[0], \"Can you tell me what you can do?\")\n",
        "\n",
        "conv.append_message(conv.roles[1], None)\n",
        "prompt = conv.get_prompt()\n",
        "print(prompt)\n",
        "response = generate(prompt)\n",
        "print(\"-\"*80)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQWOFwHqvUFZ",
        "outputId": "73731208-5c66-4109-fe0f-eca36b2a3cc8"
      },
      "id": "eQWOFwHqvUFZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
            "\n",
            "Human: <s>Are you a good AI assistant?</s>Assistant: <s>\n",
            "--------------------------------------------------------------------------------\n",
            "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
            "\n",
            "Human: Are you a good AI assistant?Assistant: As an AI language model, I am designed to assist users with a wide range of tasks and questions. I am constantly learning and improving my capabilities to provide the best possible responses to the questions and tasks that I am given. I am not a person, but rather a computer program that has been trained on a large dataset of text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b5cf7d4",
      "metadata": {
        "papermill": {
          "duration": 0.006537,
          "end_time": "2023-10-19T19:54:41.237928",
          "exception": false,
          "start_time": "2023-10-19T19:54:41.231391",
          "status": "completed"
        },
        "tags": [],
        "id": "1b5cf7d4"
      },
      "source": [
        "# Clean GPU Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d346e523",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:54:41.252090Z",
          "iopub.status.busy": "2023-10-19T19:54:41.251835Z",
          "iopub.status.idle": "2023-10-19T19:54:41.753984Z",
          "shell.execute_reply": "2023-10-19T19:54:41.752976Z"
        },
        "papermill": {
          "duration": 0.511293,
          "end_time": "2023-10-19T19:54:41.755826",
          "exception": false,
          "start_time": "2023-10-19T19:54:41.244533",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d346e523",
        "outputId": "cda2a851-3af8-4869-dd5f-4a28c0ba3e0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14253"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f474353",
      "metadata": {
        "papermill": {
          "duration": 0.006941,
          "end_time": "2023-10-19T19:54:41.769884",
          "exception": false,
          "start_time": "2023-10-19T19:54:41.762943",
          "status": "completed"
        },
        "tags": [],
        "id": "9f474353"
      },
      "source": [
        "## Load the trained model back and integrate the trained LoRA within."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8502540",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:54:41.784528Z",
          "iopub.status.busy": "2023-10-19T19:54:41.784250Z",
          "iopub.status.idle": "2023-10-19T19:55:26.592347Z",
          "shell.execute_reply": "2023-10-19T19:55:26.591616Z"
        },
        "papermill": {
          "duration": 44.817681,
          "end_time": "2023-10-19T19:55:26.594276",
          "exception": false,
          "start_time": "2023-10-19T19:54:41.776595",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "4a88fe46234a4b988232a4f3d14cfbc8",
            "38802ff2b03c4129bfaa2d484a0c8dde",
            "4c896ff7b5fc426cbbe22c1610643839",
            "73eecb5eff0c42d89affab0ba5d625fc",
            "a9e3bb1d2a2c41b1a1f566739a0a8491",
            "c8bd1113023d45b1a8cb67b4a4da373a",
            "3b9357d3b9744d79bb09306879bba6e9",
            "920c8a00964040c78c414cd110891e19",
            "ce3ad5f976b248eb87649c10943e8406",
            "c1c4881502b440ae8b3e24709f40b06f",
            "e5570998041746e9a9b8081d52d88d58"
          ]
        },
        "id": "b8502540",
        "outputId": "6e8a5f99-a0d6-4890-dc7f-e8219375c365"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a88fe46234a4b988232a4f3d14cfbc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:82: UserWarning: Merge lora module to 8-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map={\"\":0})\n",
        "model = PeftModel.from_pretrained(model, output_path)\n",
        "model = model.merge_and_unload()\n",
        "model.config.max_length = 512\n",
        "model.eval()\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
        "# tokenizer.pad_token = tokenizer.unk_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a05b9b7",
      "metadata": {
        "papermill": {
          "duration": 0.006596,
          "end_time": "2023-10-19T19:55:26.608211",
          "exception": false,
          "start_time": "2023-10-19T19:55:26.601615",
          "status": "completed"
        },
        "tags": [],
        "id": "2a05b9b7"
      },
      "source": [
        "## Answer generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4c3ad5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-10-19T19:55:26.622571Z",
          "iopub.status.busy": "2023-10-19T19:55:26.622308Z",
          "iopub.status.idle": "2023-10-19T19:55:27.126229Z",
          "shell.execute_reply": "2023-10-19T19:55:27.125342Z"
        },
        "papermill": {
          "duration": 0.513352,
          "end_time": "2023-10-19T19:55:27.128158",
          "exception": false,
          "start_time": "2023-10-19T19:55:26.614806",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e4c3ad5",
        "outputId": "a834a178-6b11-432b-c79a-3e25a3f43688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1197: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            " 50%|█████     | 1/2 [00:08<00:08,  8.02s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "100%|██████████| 2/2 [00:13<00:00,  6.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
            "\n",
            "Human: What's the weather like today?Assistant: I'm sorry, but I am an AI language model and do not have the ability to access real-time weather information. You can check the weather forecast for your location on a reliable weather website or app.\n",
            "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.\n",
            "\n",
            "Human: Who are you?Assistant: I'm sorry, but I don't have any personal information. I'm just a computer program designed to help answer questions and provide information. Is there something specific you would like to know?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "@torch.no_grad()\n",
        "def generate(query_list, return_answer: bool = False):\n",
        "    def conv_format(query):\n",
        "        conv = get_conv_template('phoenix')\n",
        "        conv.append_message(conv.roles[0], query)\n",
        "        conv.append_message(conv.roles[1], None)\n",
        "        return conv.get_prompt()\n",
        "\n",
        "    query_list = [conv_format(query) for query in query_list]\n",
        "    ## First, we should wrap the sentence just as what we do during the training process\n",
        "    input_ids = tokenizer(query_list, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
        "    n_input, n_seq = input_ids.shape[0], input_ids.shape[-1]\n",
        "    output_ids = []\n",
        "    step = 1\n",
        "    ## Second, the model generate input_ids for each inputs.\n",
        "    for index in tqdm(range(0, n_input, step)):\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids[index: min(n_input, index+step)],\n",
        "            do_sample=False,\n",
        "            max_new_tokens=64,\n",
        "            # temperature=0.7,\n",
        "            repetition_penalty=1.0,\n",
        "        )\n",
        "        output_ids += outputs\n",
        "    ## finally, we need to decode the output_ids from id sequence to token sequence to get our final results.\n",
        "    responses = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    if return_answer:\n",
        "        return [response[len(query):].strip() for query, response in zip(query_list, responses)]\n",
        "    return responses\n",
        "\n",
        "# test\n",
        "print(\"\\n\".join(generate([\"What's the weather like today?\", \"Who are you?\"])))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate a trained model on a given test dataset"
      ],
      "metadata": {
        "id": "dK-NHBIVce90"
      },
      "id": "dK-NHBIVce90"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# TODO: correctly put test data files into an accessible path\n",
        "test_file = \"zh_med.json\"\n",
        "assert os.path.exists(test_file), \"Invalid test_file path\"\n",
        "\n",
        "with open(test_file, 'r', encoding='utf-8') as reader:\n",
        "    test_data = json.load(reader)\n",
        "print(test_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpU2_tmDcmT2",
        "outputId": "ed2c61b4-e677-459a-a5ab-1c5374d676a7"
      },
      "id": "UpU2_tmDcmT2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['什么是医学伦理学，它在医疗领域有何重要性？', '医学伦理学是研究医疗领域伦理问题的学科。它涉及研究医疗专业人员、患者和其他相关利益相关者之间的伦理关系，以及在医疗实践中出现的道德困境。\\n\\n医学伦理学在医疗领域具有以下重要性：\\n\\n1. 保护患者权益：医学伦理学关注患者的权益和尊严。它确保医疗决策是以患者的最大利益为出发点，并尊重患者的自主权和知情同意权。\\n\\n2. 促进医务人员职业道德：医学伦理学提供了医务人员在面对道德困境时的指导原则，帮助他们保持专业的道德标准和行为规范。\\n\\n3. 增加医疗决策的公正性：医学伦理学关注公正和公平的医疗分配原则。它确保资源在医疗领域的分配是公正和可持续的。\\n\\n4. 促进研究伦理：医学伦理学对医学研究进行伦理审查，确保研究参与者的权益和福利得到保护，并确保研究过程是符合伦理标准的。\\n\\n5. 保护医疗机构声誉：医学伦理学的遵循有助于确保医疗机构遵守伦理原则，保护其声誉和公众信任度。\\n\\n总之，医学伦理学在医疗领域的重要性在于维护患者权益、指导医务人员的职业道德行为、促进医疗决策公正和保护医学研究伦理。它为医疗行业提供了一个道德框架，确保医疗服务的质量和道德高于一切。']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_answers = generate([data[0] for data in test_data], return_answer=True)"
      ],
      "metadata": {
        "id": "_rw0jbYJdTPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7de118fe-f995-4a86-b685-a0ff241211cd"
      },
      "id": "_rw0jbYJdTPs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?it/s]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "  5%|▌         | 1/20 [00:15<04:53, 15.43s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 10%|█         | 2/20 [00:25<03:41, 12.33s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 15%|█▌        | 3/20 [00:35<03:11, 11.29s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 20%|██        | 4/20 [00:45<02:52, 10.79s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 25%|██▌       | 5/20 [00:57<02:45, 11.04s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 30%|███       | 6/20 [01:15<03:09, 13.51s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            " 35%|███▌      | 7/20 [01:29<02:56, 13.58s/it]A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data, answer in zip(test_data, model_answers):\n",
        "    data.append(answer)"
      ],
      "metadata": {
        "id": "hHPykPgIeQzy"
      },
      "id": "hHPykPgIeQzy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"saved_data.json\", 'w', encoding='utf-8') as writer:\n",
        "    json.dump(test_data, writer, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "1bZe0JS2eRK_"
      },
      "id": "1bZe0JS2eRK_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 592.642275,
      "end_time": "2023-10-19T19:56:29.207434",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-10-19T19:46:36.565159",
      "version": "2.4.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "155866f3c4e5433c90a4128d4633d12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_116ed4a0802842079e9572ab90856e28",
              "IPY_MODEL_6aa20bd15d8644dfb83a154efc3fbddf",
              "IPY_MODEL_d1188ccc6eff4eba9f6ef09667b9b956"
            ],
            "layout": "IPY_MODEL_0d6a4c1cf30b40d3b42f3f26330dda40"
          }
        },
        "116ed4a0802842079e9572ab90856e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2579d66afcaa4434884b5d558a44644b",
            "placeholder": "​",
            "style": "IPY_MODEL_7265f845df394c1197ffdd4152a0a175",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6aa20bd15d8644dfb83a154efc3fbddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8daae89f8892497db401df65b62e0080",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2ddb5f4dd9f4bf3b99663cc2f4e5706",
            "value": 5
          }
        },
        "d1188ccc6eff4eba9f6ef09667b9b956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e710ccc6838451a82fc1efcf8f8c5cf",
            "placeholder": "​",
            "style": "IPY_MODEL_882ff01a2b6743838a3205da6d9acdce",
            "value": " 5/5 [01:41&lt;00:00, 17.62s/it]"
          }
        },
        "0d6a4c1cf30b40d3b42f3f26330dda40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2579d66afcaa4434884b5d558a44644b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7265f845df394c1197ffdd4152a0a175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8daae89f8892497db401df65b62e0080": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2ddb5f4dd9f4bf3b99663cc2f4e5706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e710ccc6838451a82fc1efcf8f8c5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "882ff01a2b6743838a3205da6d9acdce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a88fe46234a4b988232a4f3d14cfbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38802ff2b03c4129bfaa2d484a0c8dde",
              "IPY_MODEL_4c896ff7b5fc426cbbe22c1610643839",
              "IPY_MODEL_73eecb5eff0c42d89affab0ba5d625fc"
            ],
            "layout": "IPY_MODEL_a9e3bb1d2a2c41b1a1f566739a0a8491"
          }
        },
        "38802ff2b03c4129bfaa2d484a0c8dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8bd1113023d45b1a8cb67b4a4da373a",
            "placeholder": "​",
            "style": "IPY_MODEL_3b9357d3b9744d79bb09306879bba6e9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "4c896ff7b5fc426cbbe22c1610643839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_920c8a00964040c78c414cd110891e19",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce3ad5f976b248eb87649c10943e8406",
            "value": 5
          }
        },
        "73eecb5eff0c42d89affab0ba5d625fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1c4881502b440ae8b3e24709f40b06f",
            "placeholder": "​",
            "style": "IPY_MODEL_e5570998041746e9a9b8081d52d88d58",
            "value": " 5/5 [01:41&lt;00:00, 17.86s/it]"
          }
        },
        "a9e3bb1d2a2c41b1a1f566739a0a8491": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8bd1113023d45b1a8cb67b4a4da373a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9357d3b9744d79bb09306879bba6e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "920c8a00964040c78c414cd110891e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce3ad5f976b248eb87649c10943e8406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1c4881502b440ae8b3e24709f40b06f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5570998041746e9a9b8081d52d88d58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}